[{"title":"centos安装nacos","date":"2020-09-19T10:00:00.000Z","path":"2020/09/19/运维/centos安装nacos/","text":"下载12https://github.com/alibaba/nacos/releaseshttps://github.com/alibaba/nacos/releases/download/1.3.2/nacos-server-1.3.2.zip","tags":[]},{"title":"centos安装rockermq","date":"2020-09-19T10:00:00.000Z","path":"2020/09/19/运维/centos安装rocketmq/","text":"一、安装jdk 1.8 jdk1.8 资源下载 上传至服务器目录，解压（以上传至root 目录为例） 1tar -zxvf jdk-8u221-linux-x64.tar.gz 将解压后的文件夹移动到/usr/local目录下 1mv jdk1.8.0_221 /usr/local/ 编辑以下文件，配置java 环境 1vim /etc/profile 具体java 环境配置: 1234export JAVA_HOME=/usr/local/jdk1.8.0_221export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib/dt.JAVA_HOME/lib/tools.jar:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125; 此处顺便配置rocketmq 环境1export NAMESRV_ADDR=127.0.0.1:9876 6.刷新文件，使配置立即生效 1source /etc/profile 查看是否安装成功 1java -version 8.配置成功,将会看到以下类似信息 123java version \"1.8.0_221\"Java(TM) SE Runtime Environment (build 1.8.0_221-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.221-b11, mixed mode) 注意：使用openjdk 安装的话在配置rocketMq时候会出现（JAVA_HOME）问题，当时使用了很多方法，都没有成功，最好还是推荐使用这种方式吧。二、安装rocketMQ 直接下载安装包（以4.5.1为例） 官网：https://www.apache.org/dyn/closer.cgi?path=rocketmq/4.5.1/rocketmq-all-4.5.1-bin-release.zip 注意：不要下载源码包，否则是没有bin目录的1wget http://mirrors.tuna.tsinghua.edu.cn/apache/rocketmq/4.5.1/rocketmq-all-4.5.1-bin-release.zip 2.解压,将会得到 rocketmq-all-4.5.1-bin-release 文件夹 1unzip rocketmq-all-4.5.1-bin-release.zip 3.进入bin 目录 修改配置(分别修改runserver.sh 以及 runbroker.sh，因为默认配置内存过大，可能导致启动失败) 1cd /root/rocketmq-all-4.5.1-bin-release/bin/ 修改 runserver.sh 文件 修改位置 123456vim runbroker.sh##使用快捷键 i 开启编辑模式##找到以下配置，将xms/xmx/xmn 分别修改成以下数值（视机器配置而定）JAVA_OPT=\"$&#123;JAVA_OPT&#125; -server -Xms512m -Xmx512m -Xmn256m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m\"##保存wq 修改 runbroker.sh 修改位置 123456vim runbroker.sh##使用快捷键 i 开启编辑模式##具体数值视机器而定JAVA_OPT=\"$&#123;JAVA_OPT&#125; -server -Xms128m -Xmx256m -Xmn256m\"##保存wq 修改配置文件 1vim broker.conf 新增如下选项 1brokerIP1=xxxxxx(你的服务器公网ip) 分别后台启动 runserver.sh 以及 runbroker.sh 1234##启动runservernohup sh mqnamesrv &amp;##以配置文件启动runbrokernohup sh mqbroker -n localhost:9876 -c /root/rocketmq-all-4.5.1-bin-release/conf/broker.conf &amp; 7.查看启动是否成功 1jps 启动成功(可以看到NamesrvStartup以及BrokerStartup) 123416065 Jps9679 NamesrvStartup7887 jar11279 BrokerStartup 10.启动成功日志 12tail -f ~/logs/rocketmqlogs/namesrv.logtail -f ~/logs/rocketmqlogs/broker.log 11.如果启动失败，请查看失败日志 1cat nohup.out 三、关于防火墙以及安全组规则配置首先，请在你的云服务器配置安全组规则通道 9876 端口 其次，centos7默认使用firewalld防火墙，而不是iptables，卸载firewalld，再安装iptables 12345678##卸载firewalldyum remove firewalld##安装iptablesyum install iptables-services##查看防火墙状态service iptables status##停止防火墙service iptables stop 四、SpringBoot整合监视台（rocketmq-externals插件）GITHUB地址 下载rocketmq-console模块即可 修改配置文件 123rocketmq.config.namesrvAddr=你的公网IP:9876##如果你版本小于3.5.8，下面应该配置为falserocketmq.config.isVIPChannel=false 启动即可 12345678910111213Description=rockermq name serviceRequires=network-online.targetAfter=network-online.target[Service]Type=simpleUser=anonymousWorkingDirectory=/opt/rocketmqExecStart=/opt/rocketmq/bin/mqnamesrvRestart=on-failure[Install]WantedBy=multi-user.target","tags":[]},{"title":"mybatis拦截器","date":"2020-09-19T08:00:00.000Z","path":"2020/09/19/mybatis/mybaits拦截器/","text":"基本知识拦截器注解的规则：具体规则如下： 12345@Intercepts(&#123; @Signature(type = StatementHandler.class, method = \"query\", args = &#123;Statement.class, ResultHandler.class&#125;), @Signature(type = StatementHandler.class, method = \"update\", args = &#123;Statement.class&#125;), @Signature(type = StatementHandler.class, method = \"batch\", args = &#123;Statement.class&#125;)&#125;) @Intercepts：标识该类是一个拦截器； @Signature：指明自定义拦截器需要拦截哪一个类型，哪一个方法； 2.1 type：对应四种类型中的一种； 2.2 method：对应接口中的哪类方法（因为可能存在重载方法）； 2.3 args：对应哪一个方法； 5. 拦截器可拦截的方法： 拦截的类 拦截的方法 Executor update, query, flushStatements, commit, rollback,getTransaction, close, isClosed ParameterHandler getParameterObject, setParameters StatementHandler prepare, parameterize, batch, update, query ResultSetHandler handleResultSets, handleOutputParameters 12345678910111213141516@Intercepts(&#123;@Signature(type = Executor.class, method = \"query\", args = &#123;MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class&#125;)&#125;)public class TestInterceptor implements Interceptor &#123; public Object intercept(Invocation invocation) throws Throwable &#123; Object target = invocation.getTarget(); //被代理对象 Method method = invocation.getMethod(); //代理方法 Object[] args = invocation.getArgs(); //方法参数 // do something ...... 方法拦截前执行代码块 Object result = invocation.proceed(); // do something .......方法拦截后执行代码块 return result; &#125; public Object plugin(Object target) &#123; return Plugin.wrap(target, this); &#125;&#125; setProperties方法如果我们的拦截器需要一些变量对象，而且这个对象是支持可配置的。 类似于Spring中的@Value(“${}”)从application.properties文件中获取。 使用方法： 1234&lt;plugin interceptor=\"com.plugin.mybatis.MyInterceptor\"&gt; &lt;property name=\"username\" value=\"xxx\"/&gt; &lt;property name=\"password\" value=\"xxx\"/&gt;&lt;/plugin&gt; plugin方法这个方法的作用是就是让mybatis判断，是否要进行拦截，然后做出决定是否生成一个代理。 1234567@Overridepublic Object plugin(Object target) &#123; if (target instanceof StatementHandler) &#123; return Plugin.wrap(target, this); &#125; return target;&#125; 需要注意的是：每经过一个拦截器对象都会调用插件的plugin方法，也就是说，该方法会调用4次。根据@Intercepts注解来决定是否进行拦截处理。 问题1：Plugin.wrap(target, this)方法的作用？ 解答：判断是否拦截这个类型对象（根据@Intercepts注解决定），然后决定是返回一个代理对象还是返回原对象。 故我们在实现plugin方法时，要判断一下目标类型，是本插件要拦截的对象时才执行Plugin.wrap方法，否则的话，直接返回目标本身。 问题2：拦截器代理对象可能经过多层代理，如何获取到真实的拦截器对象？ 123456789101112/** * &lt;p&gt; * 获得真正的处理对象,可能多层代理. * &lt;/p&gt; */@SuppressWarnings(\"unchecked\")public static &lt;T&gt; T realTarget(Object target) &#123; if (Proxy.isProxyClass(target.getClass())) &#123; MetaObject metaObject = SystemMetaObject.forObject(target); return realTarget(metaObject.getValue(\"h.target\")); &#125; return (T) target; 配置12345678910111213141516171819202122232425262728&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;properties resource=\"top/sciento/wumu/jdbc/mybatis/db.properties\"&gt;&lt;/properties&gt; &lt;plugins&gt; &lt;plugin interceptor=\"top.sciento.wumu.jdbc.mybatis.plugin.ExamplePlugin\"&gt; &lt;/plugin&gt; &lt;plugin interceptor=\"top.sciento.wumu.jdbc.mybatis.plugin.PagePlugin\"/&gt; &lt;/plugins&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;driver&#125;\"/&gt; &lt;property name=\"url\" value=\"$&#123;url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;username&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;password&#125;\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;package name=\"top.sciento.wumu.jdbc.mybatis.mapper\"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 实战1234567891011121314151617181920212223@Intercepts(&#123;@Signature( type= Executor.class, method = \"update\", args = &#123;MappedStatement.class,Object.class&#125;)&#125;)public class ExamplePlugin implements Interceptor &#123; public Object intercept(Invocation invocation) throws Throwable &#123; System.out.println(\"被拦截方法执行之前，做的辅助服务······\"); Object[] args = invocation.getArgs(); Method method = invocation.getMethod(); Object target = invocation.getTarget(); MappedStatement mappedStatement = (MappedStatement) args[0]; Object proceed = invocation.proceed(); System.out.println(\"被拦截方法执行之后，做的辅助服务······\"); return proceed; &#125; public Object plugin(Object target) &#123; return Plugin.wrap(target, this); &#125; public void setProperties(Properties properties) &#123; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111@Slf4j@Intercepts( @Signature(type = Executor.class, method = \"query\", args = &#123;MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class&#125;))public class PagePlugin implements Interceptor &#123; @Override public Object intercept(Invocation invocation) throws Throwable &#123; Object[] args = invocation.getArgs(); MappedStatement mappedStatement = (MappedStatement) args[0]; //获取参数 Object param = invocation.getArgs()[1]; BoundSql boundSql = mappedStatement.getBoundSql(param); Object parameterObject = boundSql.getParameterObject(); /** * 判断是否是继承PageVo来判断是否需要进行分页 */ if (parameterObject instanceof Page) &#123; //强转 为了拿到分页数据 Page pagevo = (Page) param; String sql = boundSql.getSql(); //获取相关配置 Configuration config = mappedStatement.getConfiguration(); Connection connection = config.getEnvironment().getDataSource().getConnection(); //拼接查询当前条件的sql的总条数 String countSql = \"select count(*) from (\" + sql + \") a\"; PreparedStatement preparedStatement = connection.prepareStatement(countSql); BoundSql countBoundSql = new BoundSql(config, countSql, boundSql.getParameterMappings(), boundSql.getParameterObject()); ParameterHandler parameterHandler = new DefaultParameterHandler(mappedStatement, parameterObject, countBoundSql); parameterHandler.setParameters(preparedStatement); //执行获得总条数 ResultSet rs = preparedStatement.executeQuery(); int count = 0; if (rs.next()) &#123; count = rs.getInt(1); &#125; //拼接分页sql String pageSql = sql + \" limit \" + pagevo.getOffset() + \" , \" + pagevo.getSize(); //重新执行新的sql doNewSql(invocation, pageSql); Object result = invocation.proceed(); connection.close(); // 这是使用了两种不同的方式返回最终的结果 pagevo.setList((List)result); pagevo.setTotal(count); //处理新的结构 PageResult&lt;?&gt; pageResult = new PageResult&lt;List&gt;((List) result,pagevo.getPage(), pagevo.getSize(), count ); return new ArrayList&lt;PageResult&gt;()&#123;&#123;add(pageResult);&#125;&#125; ; &#125; return invocation.proceed(); &#125; private void doNewSql(Invocation invocation, String sql)&#123; final Object[] args = invocation.getArgs(); MappedStatement statement = (MappedStatement) args[0]; Object parameterObject = args[1]; BoundSql boundSql = statement.getBoundSql(parameterObject); MappedStatement newStatement = newMappedStatement(statement, new BoundSqlSqlSource(boundSql)); MetaObject msObject = MetaObject.forObject(newStatement, new DefaultObjectFactory(), new DefaultObjectWrapperFactory(), new DefaultReflectorFactory()); msObject.setValue(\"sqlSource.boundSql.sql\", sql); args[0] = newStatement; &#125; private MappedStatement newMappedStatement(MappedStatement ms, SqlSource newSqlSource) &#123; MappedStatement.Builder builder = new MappedStatement.Builder(ms.getConfiguration(), ms.getId(), newSqlSource, ms.getSqlCommandType()); builder.resource(ms.getResource()); builder.fetchSize(ms.getFetchSize()); builder.statementType(ms.getStatementType()); builder.keyGenerator(ms.getKeyGenerator()); if (ms.getKeyProperties() != null &amp;&amp; ms.getKeyProperties().length != 0) &#123; StringBuilder keyProperties = new StringBuilder(); for (String keyProperty : ms.getKeyProperties()) &#123; keyProperties.append(keyProperty).append(\",\"); &#125; keyProperties.delete(keyProperties.length() - 1, keyProperties.length()); builder.keyProperty(keyProperties.toString()); &#125; builder.timeout(ms.getTimeout()); builder.parameterMap(ms.getParameterMap()); builder.resultMaps(ms.getResultMaps()); builder.resultSetType(ms.getResultSetType()); builder.cache(ms.getCache()); builder.flushCacheRequired(ms.isFlushCacheRequired()); builder.useCache(ms.isUseCache()); return builder.build(); &#125; /** * 新的SqlSource需要实现 */ class BoundSqlSqlSource implements SqlSource &#123; private BoundSql boundSql; public BoundSqlSqlSource(BoundSql boundSql) &#123; this.boundSql = boundSql; &#125; @Override public BoundSql getBoundSql(Object parameterObject) &#123; return boundSql; &#125; &#125;&#125;","tags":[]},{"title":"mybatis入门","date":"2020-09-19T06:00:00.000Z","path":"2020/09/19/mybatis/mybatis入门/","text":"添加依赖12345678910&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;x.x.x&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.44&lt;/version&gt;&lt;/dependency&gt; 创建配置文件mybatis-config.xml 配置文件的标签顺序不能打乱，不然会报错。 123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;properties resource=\"top/sciento/wumu/jdbc/mybatis/db.properties\"&gt;&lt;/properties&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;driver&#125;\"/&gt; &lt;property name=\"url\" value=\"$&#123;url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;username&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;password&#125;\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;package name=\"top.sciento.wumu.jdbc.mybatis.mapper\"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; db.properties 1234driver=com.mysql.jdbc.Driverurl=jdbc:mysql://:3306/testusername=password= 编写执行文件123456789101112131415161718192021222324252627282930313233343536package top.sciento.wumu.jdbc.mybatis;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.Configuration;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import top.sciento.wumu.jdbc.mybatis.entity.User;import top.sciento.wumu.jdbc.mybatis.mapper.UserMapper;import java.io.IOException;import java.io.InputStream;import java.io.Reader;import java.util.List;public class MybatisRunner &#123; public static void main(String[] args) throws IOException &#123; System.out.println(MybatisRunner.class.getResource(\"\")); InputStream reader = MybatisRunner.class.getResourceAsStream(\"mybatis-config.xml\"); SqlSessionFactory sessionFactory = new SqlSessionFactoryBuilder().build(reader); Configuration configuration = sessionFactory.getConfiguration(); // 默认是不会提交的，需要手动提交 SqlSession sqlSession = sessionFactory.openSession(); UserMapper userMapper = sqlSession.getMapper(UserMapper.class); List&lt;User&gt; userList = userMapper.selectList(); System.out.println(userList); User user = new User(); user.setName(\"wumu\"); user.setAge(12); int id = userMapper.insert(user); System.out.println(id); System.out.println(user); &#125;&#125; 12345678910111213public interface UserMapper &#123; List&lt;User&gt; list(); @SelectProvider(value = UserSqlBuilder.class,method = \"selectList\") List&lt;User&gt; selectList(); // 这里使用动态sql @InsertProvider(value = UserSqlBuilder.class,method = \"insert\")// @Options(useGeneratedKeys = true, keyProperty = \"id\", keyColumn = \"id\") @SelectKey(statement = \"select last_insert_id()\", keyProperty = \"id\", before = false, resultType = int.class) int insert(User user); &#125; 12345678910111213public class UserSqlBuilder &#123; public static String selectList() &#123; return new SQL().SELECT(\"id\",\"name\",\"age\").FROM(\"base_user\").toString(); &#125; public static String insert(User user)&#123;// return new SQL().INSERT_INTO(\"base_user\").INTO_COLUMNS(\"name\",\"age\")// .INTO_VALUES(user.getName(),String.valueOf(user.getAge())).toString(); return new SQL().INSERT_INTO(\"base_user\").VALUES(\"name\",\"#&#123;name&#125;\") .VALUES(\"age\",\"#&#123;age&#125;\").toString(); &#125;&#125; 123456@Datapublic class User &#123; private Integer id; private String name; private Integer age;&#125; 123456789101112131415161718192021222324252627&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;!-- namespace属性是名称空间，必须唯一 --&gt;&lt;mapper namespace=\"top.sciento.wumu.jdbc.mybatis.mapper.UserMapper\"&gt; &lt;!-- resultMap标签:映射实体与表 type属性：表示实体全路径名 id属性：为实体与表的映射取一个任意的唯一的名字 --&gt; &lt;resultMap type=\"top.sciento.wumu.jdbc.mybatis.entity.User\" id=\"UserMap\"&gt; &lt;!-- id标签:映射主键属性 result标签：映射非主键属性 property属性:实体的属性名 column属性：表的字段名 --&gt; &lt;id property=\"id\" column=\"id\"/&gt; &lt;result property=\"name\" column=\"name\"/&gt; &lt;result property=\"age\" column=\"age\"/&gt; &lt;/resultMap&gt; &lt;select id=\"list\" resultMap=\"UserMap\"&gt; select * from base_user &lt;/select&gt;&lt;/mapper&gt; 知识分析返回主键1、使用options options可以配置sql的大部分属性，对应着我们标签&lt;select&gt;上写的相关属性。 – – 描述 @Options 方法 映射语句的属性 该注解允许你指定大部分开关和配置选项，它们通常在映射语句上作为属性出现。与在注解上提供大量的属性相比，Options 注解提供了一致、清晰的方式来指定选项。属性：useCache=true、flushCache=FlushCachePolicy.DEFAULT、resultSetType=DEFAULT、statementType=PREPARED、fetchSize=-1、timeout=-1、useGeneratedKeys=false、keyProperty=&quot;&quot;、keyColumn=&quot;&quot;、resultSets=&quot;&quot;, databaseId=&quot;&quot;。注意，Java 注解无法指定 null 值。因此，一旦你使用了 Options 注解，你的语句就会被上述属性的默认值所影响。要注意避免默认值带来的非预期行为。 The databaseId(Available since 3.5.5), in case there is a configured DatabaseIdProvider, the MyBatis use the Options with no databaseId attribute or with a databaseId that matches the current one. If found with and without the databaseId the latter will be discarded. 注意：keyColumn 属性只在某些数据库中有效（如 Oracle、PostgreSQL 等）。要了解更多关于 keyColumn 和 keyProperty 可选值信息，请查看“insert, update 和 delete”一节。 2、使用SelectKey 对应着SelectKey标签 – – – - @SelectKey 方法 &lt;selectKey&gt; 这个注解的功能与 &lt;selectKey&gt; 标签完全一致。该注解只能在 @Insert 或 @InsertProvider 或 @Update 或 @UpdateProvider 标注的方法上使用，否则将会被忽略。如果标注了 @SelectKey 注解，MyBatis 将会忽略掉由 @Options 注解所设置的生成主键或设置（configuration）属性。属性：statement 以字符串数组形式指定将会被执行的 SQL 语句，keyProperty 指定作为参数传入的对象对应属性的名称，该属性将会更新成新的值，before 可以指定为 true 或 false 以指明 SQL 语句应被在插入语句的之前还是之后执行。resultType 则指定 keyProperty 的 Java 类型。statementType 则用于选择语句类型，可以选择 STATEMENT、PREPARED 或 CALLABLE 之一，它们分别对应于 Statement、PreparedStatement 和 CallableStatement。默认值是 PREPARED。 The databaseId(Available since 3.5.5), in case there is a configured DatabaseIdProvider, the MyBatis will use a statement with no databaseId attribute or with a databaseId that matches the current one. If found with and without the databaseId the latter will be discarded. 描述： @SelctKey(statement=”SQL语句”,keyProperty=”将SQL语句查询结果存放到keyProperty中去”,before=”true表示先查询再插入，false反之”,resultType=int.class)其中： statement是要运行的SQL语句，它的返回值通过resultType来指定 before表示查询语句statement运行的时机 keyProperty表示查询结果赋值给代码中的哪个对象，keyColumn表示将查询结果赋值给数据库表中哪一列 keyProperty和keyColumn都不是必需的，有没有都可以 before=true，插入之前进行查询，可以将查询结果赋给keyProperty和keyColumn，赋给keyColumn相当于更改数据库 befaore=false，先插入，再查询，这时只能将结果赋给keyProperty 赋值给keyProperty用来“读”数据库，赋值给keyColumn用来写数据库 selectKey的两大作用：1、生成主键；2、获取刚刚插入数据的主键。 使用selectKey，并且使用MySQL的last_insert_id()函数时，before必为false，也就是说必须先插入然后执行last_insert_id()才能获得刚刚插入数据的ID。 maven打包xml文件123456789101112131415161718192021222324252627282930&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;8&lt;/source&gt; &lt;target&gt;8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/build&gt;","tags":[]},{"title":"","date":"2020-09-18T08:31:36.040Z","path":"2020/09/18/运维/systemd文件模板/","text":"12345678910111213Description=My Miscellaneous ServiceRequires=network-online.targetAfter=network-online.target[Service]Type=simpleUser=anonymousWorkingDirectory=/home/anonymousExecStart=some_can_execute --option=123Restart=on-failure[Install]WantedBy=multi-user.target","tags":[]},{"title":"centos8安装mysql","date":"2020-09-17T08:55:17.000Z","path":"2020/09/17/mysql/centos8安装mysql/","text":"以root身份或具有sudo特权的用户身份使用CentOS软件包管理器安装MySQL 8.0服务器： 1sudo dnf install @mysql @mysql模块安装MySQL及其所有依赖项。 安装完成后，通过运行以下命令来启动MySQL服务并使其在启动时自动启动： 1sudo systemctl enable --now mysqld 要检查MySQL服务器是否正在运行，请输入： 1sudo systemctl status mysqld 1234● mysqld.service - MySQL 8.0 database server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2019-10-17 22:09:39 UTC; 15s ago ... 保护MySQL运行mysql_secure_installation脚本，该脚本执行一些与安全性相关的操作并设置MySQL根密码： 1sudo mysql_secure_installation 将要求您配置 VALIDATE PASSWORD PLUGIN ，该工具用于测试MySQL用户密码的强度并提高安全性。密码验证策略分为三个级别：低，中和强。如果您不想设置验证密码插件，请按ENTER。 在下一个提示符下，将要求您为MySQL根用户设置密码。完成此操作后，脚本还将要求您删除匿名用户，限制root用户对本地计算机的访问，并删除测试数据库。您应该对所有问题回答“是”。 要从命令行与MySQL服务器进行交互，请使用MySQL客户端实用程序，它作为依赖项安装。通过键入以下内容测试根访问权限： 1mysql -u root -p 出现提示时输入 root密码，将为您提供MySQL shell，如下所示： 123Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 12Server version: 8.0.17 Source distribution 就是这样！您已经在CentOS服务器上安装并保护了MySQL 8.0，并准备使用它。 身份验证方法由于CentOS 8中的某些客户端工具和库与caching_sha2_password方法不兼容，CentOS 8存储库中包含的MySQL 8.0服务器被设置为使用旧的mysql_native_password身份验证插件。上游MySQL 8.0版本。 mysql_native_password方法适用于大多数设置。但是，如果您想将默认身份验证插件更改为caching_sha2_password，这会更快并提供更好的安全性，请打开以下配置文件： 1sudo vim /etc/my.cnf.d/mysql-default-authentication-plugin.cnf 将default_authentication_plugin的值更改为caching_sha2_password： 12[mysqld]default_authentication_plugin=caching_sha2_password 关闭并保存文件，然后重新启动MySQL服务器以使更改生效： 1sudo systemctl restart mysqld","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"postgresql账号管理","date":"2020-09-16T10:37:46.000Z","path":"2020/09/16/postgresql/postgresql账号管理/","text":"注意：创建好用户（角色）之后需要连接的话，还需要修改2个权限控制的配置文件（pg_hba.conf、pg_ident.conf）。并且创建用户（user）和创建角色（role）一样，唯一的区别是用户默认可以登录，而创建的角色默认不能登录。创建用户和角色的各个参数选项是一样的。 Tip：安装PostgreSQL会自动创建一个postgres用户，需要切换到该用户下访问PostgreSQL。 创建用户/角色 1234567891011CREATE USER/ROLE name [ [ WITH ] option [ ... ] ] : 关键词 USER,ROLE； name 用户或角色名； where option can be: SUPERUSER | NOSUPERUSER :超级权限，拥有所有权限，默认nosuperuser。 | CREATEDB | NOCREATEDB :建库权限，默认nocreatedb。 | CREATEROLE | NOCREATEROLE :建角色权限，拥有创建、修改、删除角色，默认nocreaterole。 | INHERIT | NOINHERIT :继承权限，可以把除superuser权限继承给其他用户/角色，默认inherit。 | LOGIN | NOLOGIN :登录权限，作为连接的用户，默认nologin，除非是create user（默认登录）。 | REPLICATION | NOREPLICATION :复制权限，用于物理或则逻辑复制（复制和删除slots），默认是noreplication。 | BYPASSRLS | NOBYPASSRLS :安全策略RLS权限，默认nobypassrls。 12345678910| CONNECTION LIMIT connlimit :限制用户并发数，默认-1，不限制。正常连接会受限制，后台连接和prepared事务不受限制。| [ ENCRYPTED ] PASSWORD &apos;password&apos; | PASSWORD NULL :设置密码，密码仅用于有login属性的用户，不使用密码身份验证，则可以省略此选项。可以选择将空密码显式写为PASSWORD NULL。 加密方法由配置参数password_encryption确定，密码始终以加密方式存储在系统目录中。| VALID UNTIL &apos;timestamp&apos; :密码有效期时间，不设置则用不失效。| IN ROLE role_name [, ...] :新角色将立即添加为新成员。| IN GROUP role_name [, ...] :同上| ROLE role_name [, ...] :ROLE子句列出一个或多个现有角色，这些角色自动添加为新角色的成员。 （这实际上使新角色成为“组”）。| ADMIN role_name [, ...] :与ROLE类似，但命名角色将添加到新角色WITH ADMIN OPTION，使他们有权将此角色的成员资格授予其他人。| USER role_name [, ...] :同上| SYSID uid :被忽略，但是为向后兼容性而存在。 ​ 示例： 创建不需要密码登陆的用户zjy： 12postgres=# CREATE ROLE zjy LOGIN;CREATE ROLE 创建该用户后，还不能直接登录。需要修改 pg_hba.conf 文件（后面会对该文件进行说明），加入： ①：本地登陆：local all all trust②：远程登陆：host all all 192.168.163.132/32 trust 创建需要密码登陆的用户zjy1： 12postgres=# CREATE USER zjy1 WITH PASSWORD &apos;zjy1&apos;;CREATE ROLE 和ROLE的区别是：USER带LOGIN属性。也需要修改 pg_hba.conf 文件（后面会对该文件进行说明），加入：host all all 192.168.163.132/32 md5 创建有时间限制的用户zjy2： 12postgres=# CREATE ROLE zjy2 WITH LOGIN PASSWORD &apos;zjy2&apos; VALID UNTIL &apos;2019-05-30&apos;;CREATE ROLE 和2的处理方法一样，修改 pg_hba.conf 文件，该用户会的密码在给定的时间之后过期不可用。 创建有创建数据库和管理角色权限的用户admin： 12postgres=# CREATE ROLE admin WITH CREATEDB CREATEROLE;CREATE ROLE 注意：拥有创建数据库，角色的用户，也可以删除和修改这些对象。 创建具有超级权限的用户：admin 12postgres=# CREATE ROLE admin WITH SUPERUSER LOGIN PASSWORD &apos;admin&apos;;CREATE ROLE 创建复制账号：repl 12postgres=# CREATE USER repl REPLICATION LOGIN ENCRYPTED PASSWORD &apos;repl&apos;;CREATE ROLE 其他说明 授权，定义访问权限 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748GRANT &#123; &#123; SELECT | INSERT | UPDATE | DELETE | TRUNCATE | REFERENCES | TRIGGER &#125; [, ...] | ALL [ PRIVILEGES ] &#125; ON &#123; [ TABLE ] table_name [, ...] | ALL TABLES IN SCHEMA schema_name [, ...] &#125; TO role_specification [, ...] [ WITH GRANT OPTION ]##单表授权：授权zjy账号可以访问schema为zjy的zjy表grant select,insert,update,delete on zjy.zjy to zjy;##所有表授权：grant select,insert,update,delete on all tables in schema zjy to zjy;GRANT &#123; &#123; SELECT | INSERT | UPDATE | REFERENCES &#125; ( column_name [, ...] ) [, ...] | ALL [ PRIVILEGES ] ( column_name [, ...] ) &#125; ON [ TABLE ] table_name [, ...] TO role_specification [, ...] [ WITH GRANT OPTION ]##列授权，授权指定列(zjy schema下的zjy表的name列)的更新权限给zjy用户grant update (name) on zjy.zjy to zjy;##指定列授不同权限，zjy schema下的zjy表，查看更新name、age字段，插入name字段grant select (name,age),update (name,age),insert(name) on zjy.xxx to zjy;GRANT &#123; &#123; USAGE | SELECT | UPDATE &#125; [, ...] | ALL [ PRIVILEGES ] &#125; ON &#123; SEQUENCE sequence_name [, ...] | ALL SEQUENCES IN SCHEMA schema_name [, ...] &#125; TO role_specification [, ...] [ WITH GRANT OPTION ]##序列（自增键）属性授权，指定zjy schema下的seq_id_seq 给zjy用户grant select,update on sequence zjy.seq_id_seq to zjy;##序列（自增键）属性授权，给用户zjy授权zjy schema下的所有序列grant select,update on all sequences in schema zjy to zjy;GRANT &#123; &#123; CREATE | CONNECT | TEMPORARY | TEMP &#125; [, ...] | ALL [ PRIVILEGES ] &#125; ON DATABASE database_name [, ...] TO role_specification [, ...] [ WITH GRANT OPTION ]##连接数据库权限，授权cc用户连接数据库zjygrant connect on database zjy to cc;GRANT &#123; USAGE | ALL [ PRIVILEGES ] &#125; ON DOMAIN domain_name [, ...] TO role_specification [, ...] [ WITH GRANT OPTION ]## 123GRANT &#123; USAGE | ALL [ PRIVILEGES ] &#125; ON FOREIGN DATA WRAPPER fdw_name [, ...] TO role_specification [, ...] [ WITH GRANT OPTION ] \\## 123GRANT &#123; USAGE | ALL [ PRIVILEGES ] &#125; ON FOREIGN SERVER server_name [, ...] TO role_specification [, ...] [ WITH GRANT OPTION ] 1## 1234GRANT &#123; EXECUTE | ALL [ PRIVILEGES ] &#125; ON &#123; &#123; FUNCTION | PROCEDURE | ROUTINE &#125; routine_name [ ( [ [ argmode ] [ arg_name ] arg_type [, ...] ] ) ] [, ...] | ALL &#123; FUNCTIONS | PROCEDURES | ROUTINES &#125; IN SCHEMA schema_name [, ...] &#125; TO role_specification [, ...] [ WITH GRANT OPTION ] 123456##GRANT &#123; USAGE | ALL [ PRIVILEGES ] &#125; ON LANGUAGE lang_name [, ...] TO role_specification [, ...] [ WITH GRANT OPTION ] \\## 123GRANT &#123; &#123; SELECT | UPDATE &#125; [, ...] | ALL [ PRIVILEGES ] &#125; ON LARGE OBJECT loid [, ...] TO role_specification [, ...] [ WITH GRANT OPTION ] 123456789101112131415161718192021222324252627##GRANT &#123; &#123; CREATE | USAGE &#125; [, ...] | ALL [ PRIVILEGES ] &#125; ON SCHEMA schema_name [, ...] TO role_specification [, ...] [ WITH GRANT OPTION ]##连接schema权限，授权cc访问zjy schema权限grant usage on schema zjy to cc;GRANT &#123; CREATE | ALL [ PRIVILEGES ] &#125; ON TABLESPACE tablespace_name [, ...] TO role_specification [, ...] [ WITH GRANT OPTION ]GRANT &#123; USAGE | ALL [ PRIVILEGES ] &#125; ON TYPE type_name [, ...] TO role_specification [, ...] [ WITH GRANT OPTION ]where role_specification can be: [ GROUP ] role_name | PUBLIC | CURRENT_USER | SESSION_USERGRANT role_name [, ...] TO role_name [, ...] [ WITH ADMIN OPTION ]##把zjy用户的权限授予用户cc。grant zjy to cc; 权限说明： 12345678910111213141516171819202122232425SELECT：允许从指定表，视图或序列的任何列或列出的特定列进行SELECT。也允许使用COPY TO。在UPDATE或DELETE中引用现有列值也需要此权限。对于序列，此权限还允许使用currval函数。对于大对象，此权限允许读取对象。INSERT：允许将新行INSERT到指定的表中。如果列出了特定列，则只能在INSERT命令中为这些列分配（因此其他列将接收默认值）。也允许COPY FROM。UPDATE：允许更新指定表的任何列或列出的特定列，需要SELECT权限。DELETE：允许删除指定表中的行，需要SELECT权限。TRUNCATE：允许在指定的表上创建触发器。REFERENCES：允许创建引用指定表或表的指定列的外键约束。TRIGGER：允许在指定的表上创建触发器。 CREATE：对于数据库，允许在数据库中创建新的schema、table、index。CONNECT：允许用户连接到指定的数据库。在连接启动时检查此权限。TEMPORARY、TEMP：允许在使用指定数据库时创建临时表。EXECUTE：允许使用指定的函数或过程以及在函数。USAGE：对于schema，允许访问指定模式中包含的对象；对于sequence，允许使用currval和nextval函数。对于类型和域，允许在创建表，函数和其他模式对象时使用类型或域。ALL PRIVILEGES：一次授予所有可用权限。 撤销权限 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071REVOKE [ GRANT OPTION FOR ] &#123; &#123; SELECT | INSERT | UPDATE | DELETE | TRUNCATE | REFERENCES | TRIGGER &#125; [, ...] | ALL [ PRIVILEGES ] &#125; ON &#123; [ TABLE ] table_name [, ...] | ALL TABLES IN SCHEMA schema_name [, ...] &#125; FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ] ##移除用户zjy在schema zjy上所有表的select权限 revoke select on all tables in schema zjy from zjy;REVOKE [ GRANT OPTION FOR ] &#123; &#123; SELECT | INSERT | UPDATE | REFERENCES &#125; ( column_name [, ...] ) [, ...] | ALL [ PRIVILEGES ] ( column_name [, ...] ) &#125; ON [ TABLE ] table_name [, ...] FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ] ##移除用户zjy在zjy schema的zjy表的age列的查询权限 revoke select (age) on zjy.zjy from zjy;REVOKE [ GRANT OPTION FOR ] &#123; &#123; USAGE | SELECT | UPDATE &#125; [, ...] | ALL [ PRIVILEGES ] &#125; ON &#123; SEQUENCE sequence_name [, ...] | ALL SEQUENCES IN SCHEMA schema_name [, ...] &#125; FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]##序列REVOKE [ GRANT OPTION FOR ] &#123; &#123; CREATE | CONNECT | TEMPORARY | TEMP &#125; [, ...] | ALL [ PRIVILEGES ] &#125; ON DATABASE database_name [, ...] FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]##库REVOKE [ GRANT OPTION FOR ] &#123; USAGE | ALL [ PRIVILEGES ] &#125; ON DOMAIN domain_name [, ...] FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT]##REVOKE [ GRANT OPTION FOR ] &#123; USAGE | ALL [ PRIVILEGES ] &#125; ON FOREIGN DATA WRAPPER fdw_name [, ...] FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT]##REVOKE [ GRANT OPTION FOR ] &#123; USAGE | ALL [ PRIVILEGES ] &#125; ON FOREIGN SERVER server_name [, ...] FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT]##REVOKE [ GRANT OPTION FOR ] &#123; EXECUTE | ALL [ PRIVILEGES ] &#125; ON &#123; &#123; FUNCTION | PROCEDURE | ROUTINE &#125; function_name [ ( [ [ argmode ] [ arg_name ] arg_type [, ...] ] ) ] [, ...] | ALL &#123; FUNCTIONS | PROCEDURES | ROUTINES &#125; IN SCHEMA schema_name [, ...] &#125; FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]## 1234567891011121314151617181920212223242526272829303132333435363738REVOKE [ GRANT OPTION FOR ] &#123; USAGE | ALL [ PRIVILEGES ] &#125; ON LANGUAGE lang_name [, ...] FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]##REVOKE [ GRANT OPTION FOR ] &#123; &#123; SELECT | UPDATE &#125; [, ...] | ALL [ PRIVILEGES ] &#125; ON LARGE OBJECT loid [, ...] FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]##REVOKE [ GRANT OPTION FOR ] &#123; &#123; CREATE | USAGE &#125; [, ...] | ALL [ PRIVILEGES ] &#125; ON SCHEMA schema_name [, ...] FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]##schena权限REVOKE [ GRANT OPTION FOR ] &#123; CREATE | ALL [ PRIVILEGES ] &#125; ON TABLESPACE tablespace_name [, ...] FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]##REVOKE [ GRANT OPTION FOR ] &#123; USAGE | ALL [ PRIVILEGES ] &#125; ON TYPE type_name [, ...] FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]## 1234REVOKE [ ADMIN OPTION FOR ] role_name [, ...] FROM role_name [, ...] [ CASCADE | RESTRICT ]## 注意：任何用户对public的schema都有all的权限，为了安全可以禁止用户对public schema 123##移除所有用户（public），superuser除外，对指定DB下的public schema的create 权限。zjy=# revoke create on schema public from public;REVOKE 修改用户属性 123456789101112131415161718192021222324252627ALTER USER role_specification [ WITH ] option [ ... ]where option can be: SUPERUSER | NOSUPERUSER | CREATEDB | NOCREATEDB | CREATEROLE | NOCREATEROLE | INHERIT | NOINHERIT | LOGIN | NOLOGIN | REPLICATION | NOREPLICATION | BYPASSRLS | NOBYPASSRLS | CONNECTION LIMIT connlimit | [ ENCRYPTED ] PASSWORD &apos;password&apos; | PASSWORD NULL | VALID UNTIL &apos;timestamp&apos;ALTER USER name RENAME TO new_nameALTER USER &#123; role_specification | ALL &#125; [ IN DATABASE database_name ] SET configuration_parameter &#123; TO | = &#125; &#123; value | DEFAULT &#125;ALTER USER &#123; role_specification | ALL &#125; [ IN DATABASE database_name ] SET configuration_parameter FROM CURRENTALTER USER &#123; role_specification | ALL &#125; [ IN DATABASE database_name ] RESET configuration_parameterALTER USER &#123; role_specification | ALL &#125; [ IN DATABASE database_name ] RESET ALLwhere role_specification can be: role_name | CURRENT_USER | SESSION_USER 示例： 注意：option选项里的用户都可以通过alter role进行修改 修改用户为超级/非超级用户 1alter role caocao with superuser/nosuperuser; 修改用户为可/不可登陆用户 1alter role caocao with nologin/login; 修改用户名： 1alter role caocao rename to youxing; 修改用户密码，移除密码用NULL 1alter role youxing with password &apos;youxing&apos;; 修改用户参数，该用户登陆后的以该参数为准 1alter role zjy in database zjy SET geqo to 0/default; 控制访问文件 pg_hba.conf 1234567local database user auth-method [auth-options]host database user address auth-method [auth-options]hostssl database user address auth-method [auth-options]hostnossl database user address auth-method [auth-options]host database user IP-address IP-mask auth-method [auth-options]hostssl database user IP-address IP-mask auth-method [auth-options]hostnossl database user IP-address IP-mask auth-method [auth-options] local：匹配使用Unix域套接字的连接，如果没有此类型的记录，则不允许使用Unix域套接字连接。 host：匹配使用TCP/IP进行的连接，主机记录匹配SSL或非SSL连接，需要配置listen_addresses。 hostssl：匹配使用TCP/IP进行的连接，仅限于使用SSL加密进行连接，需要配置ssl参数。 hostnossl：匹配通过TCP/IP进行的连接，不使用SSL的连接。 database：匹配的数据库名称，all指定它匹配所有数据库。如果请求的数据库与请求的用户具有相同的名称则可以使用samerole值。复制（replication）不指定数据库，多个数据库可以用逗号分隔。 user：匹配的数据库用户名，值all指定它匹配所有用户。 可以通过用逗号分隔来提供多个用户名。 address：匹配的客户端计算机地址，可以包含主机名，IP地址范围。如：172.20.143.89/32、172.20.143.0/24、10.6.0.0/16、:: 1/128。 0.0.0.0/0表示所有IPv4地址，:: 0/0表示所有IPv6地址。要指定单个主机，请使用掩码长度32（对于IPv4）或128（对于IPv6）。all以匹配任何IP地址。 IP-address、IP-mask：这两个字段可用作IP地址/掩码长度，如：127.0.0.1 255.255.255.255。 auth-method：指定连接与此记录匹配时要使用的身份验证方法：trust、reject、scram-sha-256、md5、password、gss、sspi、ident、peer、ldap、radius、cert、pam、bsd。 1234567891011121314trust：允许无条件连接，允许任何PostgreSQL用户身份登录，而无需密码或任何其他身份验证。reject：拒绝任何条件连接，这对于从组中“过滤掉”某些主机非常有用。scram-sha-256：执行SCRAM-SHA-256身份验证以验证用户的密码。md5：执行SCRAM-SHA-256或MD5身份验证以验证用户的密码。password：要提供未加密的密码以进行身份验证。由于密码是通过网络以明文形式发送的，因此不应在不受信任的网络上使用。gss：使用GSSAPI对用户进行身份验证，这仅适用于TCP / IP连接。sspi：使用SSPI对用户进行身份验证，这仅适用于Windows。ident：通过联系客户端上的ident服务器获取客户端的操作系统用户名，并检查它是否与请求的数据库用户名匹配。 Ident身份验证只能用于TCP / IP连接。为本地连接指定时，将使用对等身份验证。peer：从操作系统获取客户端的操作系统用户名，并检查它是否与请求的数据库用户名匹配。这仅适用于本地连接。ldap：使用LDAP服务器进行身份验证。radius：使用RADIUS服务器进行身份验证。cert：使用SSL客户端证书进行身份验证。pam：使用操作系统提供的可插入身份验证模块（PAM）服务进行身份验证。bsd：使用操作系统提供的BSD身份验证服务进行身份验证。 auth-options：在auth-method字段之后，可以存在name = value形式的字段，用于指定认证方法的选项。 例子： 12345678910111213141516# TYPE DATABASE USER ADDRESS METHODlocal all all trust--在本地允许任何用户无密码登录local all all peer--操作系统的登录用户和pg的用户是否一致，一致则可以登录local all all ident--操作系统的登录用户和pg的用户是否一致，一致则可以登录host all all 192.168.163.0/24 md5--指定客户端IP访问通过md5身份验证进行登录host all all 192.168.163.132/32 password--指定客户端IP通过passwotd身份验证进行登录host all all 192.168.54.1/32 rejecthost all all 192.168.0.0/16 ident host all all 127.0.0.1 255.255.255.255 trust... 设置完之后可以通过查看表来查看hba： 12345678910zjy=# select * from pg_hba_file_rules; line_number | type | database | user_name | address | netmask | auth_method | options | error -------------+-------+---------------+-----------+---------------+-----------------------------------------+-------------+---------+------- 87 | host | &#123;all&#125; | &#123;all&#125; | 192.168.163.0 | 255.255.255.0 | md5 | | 92 | local | &#123;all&#125; | &#123;all&#125; | | | peer | | 94 | host | &#123;all&#125; | &#123;all&#125; | 127.0.0.1 | 255.255.255.255 | md5 | | 96 | host | &#123;all&#125; | &#123;all&#125; | ::1 | ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff | md5 | | 99 | local | &#123;replication&#125; | &#123;all&#125; | | | peer | | 100 | host | &#123;replication&#125; | &#123;all&#125; | 127.0.0.1 | 255.255.255.255 | md5 | | 101 | host | &#123;replication&#125; | &#123;all&#125; | ::1 | ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff | md5 | | 当然，修改完pg_hba.conf文件之后，需要重新加载配置，不用重启数据库： 1234postgres=# select pg_reload_conf(); pg_reload_conf ---------------- t 日常使用 用户权限管理涉及到的东西很多，本文也只是大致说明了一小部分，大部分的还得继续学习。那么现在按照一个正常项目上线的流程来创建一个应用账号为例，看看需要怎么操作。 比如一个项目zjy上线：用管理账号来操作 创建数据库： 12postgres=# create database zjy;CREATE DATABASE 创建账号：账号和数据库名字保持一致（search_path） 12postgres=# create user zjy with password &apos;zjy&apos;;CREATE ROLE 创建schema：不能用默认的public的schma 1234postgres=# \\c zjyYou are now connected to database &quot;zjy&quot; as user &quot;postgres&quot;.zjy=# create schema zjy;CREATE SCHEMA 授权： [ 复制代码](https://common.cnblogs.com/images/copycode.gif) 12345678910111213141516171819202122#访问库zjy=# grant connect on database zjy to zjy;GRANT#访问schmeazjy=# grant usage on schema zjy to zjy;GRANT#访问表zjy=# grant select,insert,update,delete on all tables in schema zjy to zjy;GRANT#如果访问自增序列，需要授权zjy=# grant select,update on all sequences in schema zjy to zjy;GRANT注意：上面的授权只对历史的一些对象授权，后期增加的对象是没有权限的，需要给个默认权限#默认表权限zjy=# ALTER DEFAULT PRIVILEGES IN SCHEMA zjy GRANT select,insert,update,delete ON TABLES TO zjy;ALTER DEFAULT PRIVILEGES#默认自增序列权限zjy=# ALTER DEFAULT PRIVILEGES IN SCHEMA zjy GRANT select,update ON sequences TO zjy;ALTER DEFAULT PRIVILEGES 常用命令 查看当前用户javascript:void(0);) 1234567891011121314151617181920zjy=# \\du List of roles Role name | Attributes | Member of -----------+------------------------------------------------------------+----------- admin | Superuser, Cannot login | &#123;&#125; postgres | Superuser, Create role, Create DB, Replication, Bypass RLS | &#123;&#125; zjy | | &#123;&#125;zjy=# select * from pg_roles; rolname | rolsuper | rolinherit | rolcreaterole | rolcreatedb | rolcanlogin | rolreplication | rolconnlimit | rolpassword | rolvaliduntil | rolbypassrls | rolconfig | oid ----------------------+----------+------------+---------------+-------------+-------------+----------------+--------------+-------------+---------------+--------------+-----------+------- pg_signal_backend | f | t | f | f | f | f | -1 | ******** | | f | | 4200 postgres | t | t | t | t | t | t | -1 | ******** | | t | | 10 admin | t | t | f | f | f | f | -1 | ******** | | f | | 16456 pg_read_all_stats | f | t | f | f | f | f | -1 | ******** | | f | | 3375 zjy | f | t | f | f | t | f | -1 | ******** | | f | | 16729 pg_monitor | f | t | f | f | f | f | -1 | ******** | | f | | 3373 pg_read_all_settings | f | t | f | f | f | f | -1 | ******** | | f | | 3374 pg_stat_scan_tables | f | t | f | f | f | f | -1 | ******** | | f | | 3377(8 rows) 查看用户权限javascript:void(0);) 12345678910111213141516171819zjy=# select * from information_schema.table_privileges where grantee=&apos;zjy&apos;; grantor | grantee | table_catalog | table_schema | table_name | privilege_type | is_grantable | with_hierarchy ----------+---------+---------------+--------------+------------+----------------+--------------+---------------- postgres | zjy | zjy | zjy | zjy | INSERT | NO | NO postgres | zjy | zjy | zjy | zjy | SELECT | NO | YES postgres | zjy | zjy | zjy | zjy | UPDATE | NO | NO postgres | zjy | zjy | zjy | zjy | DELETE | NO | NO postgres | zjy | zjy | zjy | zjy1 | INSERT | NO | NO postgres | zjy | zjy | zjy | zjy1 | SELECT | NO | YES postgres | zjy | zjy | zjy | zjy1 | UPDATE | NO | NO postgres | zjy | zjy | zjy | zjy1 | DELETE | NO | NO postgres | zjy | zjy | zjy | zjy2 | INSERT | NO | NO postgres | zjy | zjy | zjy | zjy2 | SELECT | NO | YES postgres | zjy | zjy | zjy | zjy2 | UPDATE | NO | NO postgres | zjy | zjy | zjy | zjy2 | DELETE | NO | NO postgres | zjy | zjy | zjy | zjy3 | INSERT | NO | NO postgres | zjy | zjy | zjy | zjy3 | SELECT | NO | YES postgres | zjy | zjy | zjy | zjy3 | UPDATE | NO | NO postgres | zjy | zjy | zjy | zjy3 | DELETE | NO | NO","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"centos安装vnc","date":"2020-09-15T01:32:57.000Z","path":"2020/09/15/运维/centos安装vnc/","text":"","tags":[]},{"title":"centos8安装docker","date":"2020-09-05T13:40:23.000Z","path":"2020/09/05/运维/centos8安装docker/","text":"背景简介： 现在centos已经到了8 ，一直在接触容器方面，为了尝鲜，下载了CentOS8，并尝试安装docker&amp;docker-ce，不料竟然还报了个错（缺少依赖），故及时记录一下，方便其他同学。 安装步骤： 下载docker-ce的repo 1curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo 安装依赖（这是相比centos7的关键步骤） 1yum install https://download.docker.com/linux/fedora/30/x86_64/stable/Packages/containerd.io-1.2.6-3.3.fc30.x86_64.rpm 安装docker-ce 1yum install docker-ce 启动docker 1systemctl start docker 开机启动docker 1systemctl enable docker 6.安装docker-compose 1sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose 7.添加操作权限 1sudo chmod +x /usr/local/bin/docker-compose 8.设置快捷 1sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 9.查看docker-compose 版本 1docker-compose --version","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"sudoer文件解析","date":"2020-09-05T13:40:23.000Z","path":"2020/09/05/运维/sudoer文件解析/","text":"sudo的权限控制可以在/etc/sudoers文件中查看到。 如果想要控制某个用户(或某个组用户)只能执行root权限中的一部分命令, 或者允许某些用户使用sudo时不需要输入密码,就需要对该文件有所了解。 一般来说，通过cat /etc/sudoers指令来查看该文件, 会看到如下几行代码: 123root ALL=(ALL:ALL) ALL%wheel ALL=(ALL) ALL%sudo ALL=(ALL:ALL) ALL 对/etc/sudoers文件进行编辑的代码公式可以概括为: 1授权用户/组 主机=[(切换到哪些用户或组)] [是否需要输入密码验证] 命令1,命令2,... 凡是[ ]中的内容, 都能省略; 命令和命令之间用,号分隔; 为了方便说明, 将公式的各个部分称呼为字段1 - 字段5: 12授权用户/组 主机 =[(切换到哪些用户或组)] [是否需要输入密码验证] 命令1,命令2,...字段1 字段2 =[(字段3)] [字段4] 字段5 字段3、字段4，是可以省略的。 在上面的默认例子中, “字段1”不以%号开头的表示”将要授权的用户”, 比如例子中的root；以%号开头的表示”将要授权的组”, 比如例子中的%wheel组 和 %sudo组。 “字段2”表示允许登录的主机, ALL表示所有; 如果该字段不为ALL,表示授权用户只能在某些机器上登录本服务器来执行sudo命令. 比如: 1jack mycomputer=/usr/sbin/reboot,/usr/sbin/shutdown 表示: 普通用户jack在主机(或主机组)mycomputer上, 可以通过sudo执行reboot和shutdown两个命令。”字段3”和”字段4”省略。 “字段3”如果省略, 相当于(root:root)，表示可以通过sudo提权到root; 如果为(ALL)或者(ALL:ALL), 表示能够提权到(任意用户:任意用户组)。 请注意，”字段3”如果没省略,必须使用( )双括号包含起来。这样才能区分是省略了”字段3”还是省略了”字段4”。 “字段4”的可能取值是NOPASSWD:。请注意NOPASSWD后面带有冒号:。表示执行sudo时可以不需要输入密码。比如: 1lucy ALL=(ALL) NOPASSWD: /bin/useradd 表示: 普通用户lucy可以在任何主机上, 通过sudo执行/bin/useradd命令, 并且不需要输入密码. 又比如: 1peter ALL=(ALL) NOPASSWD: ALL 表示: 普通用户peter可以在任何主机上, 通过sudo执行任何命令, 并且不需要输入密码。 “字段5”是使用逗号分开一系列命令,这些命令就是授权给用户的操作; ALL表示允许所有操作。 你可能已经注意到了, 命令都是使用绝对路径, 这是为了避免目录下有同名命令被执行，从而造成安全隐患。 如果你将授权写成如下安全性欠妥的格式: 1lucy ALL=(ALL) chown,chmod,useradd 那么用户就有可能创建一个他自己的程序, 也命名为userad, 然后放在它的本地路径中, 如此一来他就能够使用root来执行这个”名为useradd的程序”。这是相当危险的! 命令的绝对路径可通过which指令查看到: 比如which useradd可以查看到命令useradd的绝对路径: /usr/sbin/useradd 公式还要扩充例子1: 1papi ALL=(root) NOPASSWD: /bin/chown,/usr/sbin/useradd 表示: 用户papi能在所有可能出现的主机上, 提权到root下执行/bin/chown, 不必输入密码; 但运行/usr/sbin/useradd 命令时需要密码. 这是因为NOPASSWD:只影响了其后的第一个命令: 命令1. 上面给出的公式只是简化版，完整的公式如下: 1授权用户/组 主机=[(切换到哪些用户或组)] [是否需要输入密码验证] 命令1, [(字段3)] [字段4] 命令2, ... 在具有sudo操作的用户下, 执行sudo -l可以查看到该用户被允许和被禁止运行的命令. 通配符和取消命令例子2: 1papi ALL=/usr/sbin/*,/sbin/*,!/usr/sbin/fdisk 用例子2来说明通配符*的用法, 以及命令前面加上!号表示取消该命令。 该例子的意思是: 用户papi在所有可能出现的主机上, 能够运行目录/usr/sbin和/sbin下所有的程序, 但fdisk除外. 开始编辑“你讲了这么多,但是在实践中,我去编辑/etc/sudoers文件，系统提示我没权限啊，怎么办?” 这是因为/etc/sudoers的内容如此敏感，以至于该文件是只读的。所以，编辑该文件前，请确认清楚你知道自己正在做什么。 强烈建议通过visudo命令来修改该文件，通过visudo修改，如果配置出错，会有提示。 不过，系统文档推荐的做法，不是直接修改/etc/sudoers文件，而是将修改写在/etc/sudoers.d/目录下的文件中。 如果使用这种方式修改sudoers，需要在/etc/sudoers文件的最后行，加上#includedir /etc/sudoers.d一行(默认已有): 1#includedir /etc/sudoers.d 注意了，这里的指令#includedir是一个整体, 前面的#号不能丢，并非注释，也不能在#号后有空格。 任何在/etc/sudoers.d/目录下，不以~号结尾的文件和不包含.号的文件，都会被解析成/etc/sudoers的内容。 文档中是这么说的: 1234567891011# This will cause sudo to read and parse any files in the /etc/sudoers.d# directory that do not end in '~' or contain a '.' character.# Note that there must be at least one file in the sudoers.d directory (this# one will do), and all files in this directory should be mode 0440.# Note also, that because sudoers contents can vary widely, no attempt is# made to add this directive to existing sudoers files on upgrade.# Finally, please note that using the visudo command is the recommended way# to update sudoers content, since it protects against many failure modes. 其他小知识输入密码时有反馈当使用sudo后输入密码，并不会显示任何东西 —— 甚至连常规的星号都没有。有个办法可以解决该问题。 打开/etc/sudoers文件找到下述一行: 1Defaults env_reset 修改成: 1Defaults env_reset,pwfeedback 修改sudo会话时间如果你经常使用sudo 命令，你肯定注意到过当你成功输入一次密码后，可以不用再输入密码就可以运行几次sudo命令。但是一段时间后，sudo 命令会再次要求你输入密码。默认是15分钟，该时间可以调整。添加timestamp_timeout=分钟数即可。时间以分钟为单位，-1表示永不过期，但强烈不推荐。 比如我希望将时间延长到1小时，还是打开/etc/sudoers文件找到下述一行: 1Defaults env_reset 修改成: 1Defaults env_reset,pwfeedback,timestamp_timeout=60","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"centos8扩容root分区","date":"2020-09-05T13:40:23.000Z","path":"2020/09/05/运维/centos8扩容root分区/","text":"扩展磁盘最近使用虚拟机的方式弄了个centos8的虚拟机，体验最新centos系统，分配了127g的空间，由于实际需要，发现home空间有好几十g的空间，而我都是使用root用户，无需home空间，因此找到在centos8中把home空间调整到root的方法，这里跟网上找到的centos7是有差别的。 步骤： 使用usb系统进入修复 使用df-h查看空间使用情况，备份home 卸载home文件系统 删除/home所在的lv 扩展/root所在的lv 扩展/root文件系统 重新创建home lv并挂载home 查看最终调整结果 使用df-lh查看空间使用情况，备份home首先登陆ssh，使用df -lh查看空间使用情况 1df -lh root已经不够了，而vps也就自己一个人用，根本不需要用到home，home设置1个g就够了，其余的都给root，这样就可以给root多出来73个g的空间。 这因为一开始没有截图，所以看到的是后面的1g大小，一开始home是74g大小的。 备份home文件到/tmp目录 12tar cvf /tmp/home.tar /home# zip -r /tmp/home.zip /home 卸载home文件系统12fuser -km /home/umount /home 解除home目录的占用，卸载home目录 删除/home所在的lv这一步centos8有很大不同，因为centos7中目录是/dev/mapper/centos-home,而在centos8中为 /dev/mapper/cl-home，因此注意卸载设备名称 1lvremove /dev/mapper/cl-home 扩展/root所在的lv扩展root空间lv 1lvextend -L +73G /dev/mapper/cl-root 扩展/root文件系统这一步是真正增加root空间，centos7和centos8具有非常大的差别，centos7中是使用xfs_growfs /dev/mapper/centos-root，按逻辑centos8就应该是 xfs_growfs /dev/mapper/cl-root，但是结果就是 1xfs_growfs /dev/mapper/cl-root 经过摸索发现应该直接使用/就可以了 1xfs_growfs / 重新创建home lv并挂载home创建1g空间的home 1lvcreate -L 1G -n home cl 文件系统类型设置1mkfs.xfs /dev/cl/home 挂载到home目录 1mount /dev/cl/home /home 恢复home目录下文件 12345mv /tmp/home.tar /homecd /hometar xvf home.tarmv home/* .rm -rf home* 查看最终调整结果查看各分区大小 1df -lh 总结：本文主要介绍了在centos8系统下调整各分区大小，这里就是/home分区和/root分区，介绍在centos7和centos8下参数差异。熟悉linux系统下的文件系统的分区调整。对于刚装系统分区不合适需要调整centos各分区大小的用户起到指导作用，有疑问再邮件联系吧。 lvm修改根分区大小 参考： 减小lvm根分区容量: http://kwokchivu.blog.51cto.com/1128937/724128 CentOS 5 LVM逻辑卷管理: http://sunshyfangtian.blog.51cto.com/1405751/860018 目标home、根各为50GB空间，根空间不足，需缩小home至10GB、扩大根为90GB。 1234lvm&gt; lvscan ACTIVE &apos;/dev/vg_db/lv_root&apos; [50.00 GiB] inherit ACTIVE &apos;/dev/vg_db/lv_home&apos; [50.00 GiB] inherit ACTIVE &apos;/dev/vg_db/lv_swap&apos; [9.83 GiB] inherit 缩小home、增大根分区进入rescue模式1增大root分区是否可以在线完成、不用进rescue状态？找机会试试... 从Linux安装光盘启动进入rescue模式； 选择相关的语言，键盘模式，当系统提示启用网络设备时，选择“NO”； 然后在提示允许rescue模式挂载本地Linux系统到/mnt/sysimage下时选择“Skip”，文件系统必须不被挂载才可以对/分区减小容量操作。 最后系统会提示选择进入shell终端还是reboot机器，选择进入shell终端。 激活分区输入lvm命令，进入lvm界面，依次输入pvscan、vgscan、lvscan三个命令扫描pv、vg、lv相关信息。 然后输入lvchange -ay /dev/vg_db/lv_root（上文提到的/分区名称）此命令是激活/分区所在的逻辑卷，输入 quit返回到bash shell界面。 12lvchange -ay /dev/vg_db/lv_homelvchange -ay /dev/vg_db/lv_root 缩小home分区 先检查下分区: e2fsck -f /dev/vg_db/lv_home 缩小文件系统大小：resize2fs /dev/vg_db/lv_home 10G 缩小逻辑卷 输入lvm命令进入lvm模式 缩小逻辑卷：lvreduce -L 10G /dev/vg_db/lv_home 系统会询问是否缩小逻辑卷，输入 y 确定。 查看修改结果: vgdisplay，lvdisplay 12345减小LVM中的文件系统必须离线操作(处于umount装态)，要减小文件系统和LV: # Unmount相应的文件系统 # 运行磁盘检查确保卷的完整 # 减小文件系统 # 减小LV 扩大根分区 先检查下分区: e2fsck -f /dev/vg_db/lv_root 扩大逻辑卷: 输入lvm命令进入lvm模式 扩大逻辑卷：lvresize -L +40G /dev/vg_db/lv_root 更改文件系统大小 resize2fs -p /dev/vg_db/lv_root 查看修改结果: lvscan 其他操作修改swap卷大小 取消激活swap空间: swapoff 修改swap分区大小: lvresize -L 4G /dev/vg_db/lv_swap 重新格区化: mkswap -f /dev/vb_db/lv_swap 激活swap空间: swapon 新建逻辑卷lv_develop 创建逻辑卷 : lvcreate -L 2.8G -n lv_develop /dev/vb_db 创建文件系统 : mkfs.ext3 /dev/vg_db/lv_develop 增加物理盘 fdisk分区，并将分区类型为0×8e(Linux LVM) 创建物理卷PV: pvcreate /dev/hdb1 创建卷组VG: vgcreate vgtest /dev/hdb1 添加PV到VG: vgextend 创建逻辑卷LV: lvcreate -L 6000M -n mysql vgtest 创建文件系统: mkfs -t ext3 /dev/vgtest/mysql 建立新分区卷标: tune2fs –L /mysql /dev/vgtest/mysql 加载新分区: mount –t ext3 /dev/vgtest/mysql /mysql 卸载卷的顺序: umount 卸载逻辑卷:lvremove LVDEVICE 卸载卷组:vgremove VGNAME 卸载物理卷:pvremove PVDEVICE LVM分区在线扩容2011-12-19 15:24:16 http://share.blog.51cto.com/278008/745479 今天对三台服务器的LV分区进行了一次扩容。本文有点标题党嫌疑，因为只有一台服务器是在线扩容，其它两台都是先卸载再扩容的。 在线扩容的这台服务器，LV分区格式为xfs，原大小1.2TB。增加了一块硬盘，大小为1.8TB。 1`fdisk` `/dev/cciss/c0d1` `# 创建分区，并指定分区类型为LVM (8e) ``pvcreate ``/dev/cciss/c0d1p1` `# 创建pv``vgextend VolGroup00 ``/dev/cciss/c0d1p1` `# 添加新创建的pv到原有vg``lvextend -L +1.8T ``/dev/mapper/VolGroup00-LogVol05` `# 在线扩容指定lv分区``xfs_growfs ``/dev/mapper/VolGroup00-LogVol05` `# 使扩容生效。注意xfs文件系统的生效命令！ ` 其它两台服务器也是新增了一个1.8TB的硬盘，要扩容的LV分区格式为ext3。之所以没有进行在线扩容，是因为没有找到ext2online命令；后来发现，resize2fs也是支持在线扩容的！ 1`lvextend -l +100%FREE ``/dev/mapper/VolGroup00-LogVol05``umount` `-l ``/dev/mapper/VolGroup00-LogVol05``e2fsck -f ``/dev/mapper/VolGroup00-LogVol05` `# 过程比较长 ``resize2fs ``/dev/mapper/VolGroup00-LogVol05` `# 也要几分钟时间 ``mount` `/dev/mapper/VolGroup00-LogVol05` `/hdfs` 虽然resize2fs可以在线使用，但是对在线lv分区执行e2fsck有点风险！","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"centos配置网络","date":"2020-09-05T13:40:23.000Z","path":"2020/09/05/运维/centos配置网络/","text":"centos8已经发布了，下载了一个体验一下，新安装好的centos8默认网卡是没有启动的，安装好后需要先配置网络。在/etc/sysconfig/network-scripts目录下存放着网卡的配置文件，文件名称是ifcfg- 网卡名称。 一 修改配置文件设置网络时首先打开配置文件，配置文件默认如下所示，如果使用dhcp自动获取ip，只需将ONBOOT=no修改为ONBOOT=no即可。 1234567891011121314151617# 网卡配置文件按默认配置TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens33UUID=e4987998-a4ce-4cef-96f5-a3106a97f5bfDEVICE=ens33ONBOOT=no #如果使用dhcp分配ip的话，只需要将这里no改为yes，然后重启网络服务就行 如果需要配置静态ip，则按照以下修改方法修改 12345678910111213141516171819TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=static #将dhcp修改为stati表示使用静态ipDEFROUTE=yesIPADDR=192.168.128.129 #设置IP地址NETMASK=255.255.255.0 #设置子网掩码GATEWAY=192.168.128.1 #设置网关DNS1=114.114.114.114 #设置dnsIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens33UUID=e4987998-a4ce-4cef-96f5-a3106a97f5bfDEVICE=ens33ONBOOT=yes #将no改为yes 二 重启网络服务使用nmcli c reload命令重启网络服务，网络这块算是centos8改动较大的一块了，nmcli命令的参数如下所示： 123456789101112131415161718192021222324252627[hk@localhost network-scripts]$ nmcli -hUsage: nmcli [OPTIONS] OBJECT &#123; COMMAND | help &#125;OPTIONS -o[verview] overview mode (hide default values) -t[erse] terse output -p[retty] pretty output -m[ode] tabular|multiline output mode -c[olors] auto|yes|no whether to use colors in output -f[ields] &lt;field1,field2,...&gt;|all|common specify fields to output -g[et-values] &lt;field1,field2,...&gt;|all|common shortcut for -m tabular -t -f -e[scape] yes|no escape columns separators in values -a[sk] ask for missing parameters -s[how-secrets] allow displaying passwords -w[ait] &lt;seconds&gt; set timeout waiting for finishing operations -v[ersion] show program version -h[elp] print this helpOBJECT g[eneral] NetworkManager&apos;s general status and operations n[etworking] overall networking control r[adio] NetworkManager radio switches c[onnection] NetworkManager&apos;s connections # 网络管理一般使用 nmcli c d[evice] devices managed by NetworkManager a[gent] NetworkManager secret agent or polkit agent m[onitor] monitor NetworkManager changes[hk@localhost network-scripts]$ 网络管理一般使用 nmclli c，用法如下： 123456789101112131415161718192021222324252627282930[hk@localhost network-scripts]$ nmcli c -hUsage: nmcli connection &#123; COMMAND | help &#125;COMMAND := &#123; show | up | down | add | modify | clone | edit | delete | monitor | reload | load | import | export &#125; show [--active] [--order &lt;order spec&gt;] show [--active] [id | uuid | path | apath] &lt;ID&gt; ... up [[id | uuid | path] &lt;ID&gt;] [ifname &lt;ifname&gt;] [ap &lt;BSSID&gt;] [passwd-file &lt;file with passwords&gt;] down [id | uuid | path | apath] &lt;ID&gt; ... add COMMON_OPTIONS TYPE_SPECIFIC_OPTIONS SLAVE_OPTIONS IP_OPTIONS [-- ([+|-]&lt;setting&gt;.&lt;property&gt; &lt;value&gt;)+] modify [--temporary] [id | uuid | path] &lt;ID&gt; ([+|-]&lt;setting&gt;.&lt;property&gt; &lt;value&gt;)+ clone [--temporary] [id | uuid | path ] &lt;ID&gt; &lt;new name&gt; edit [id | uuid | path] &lt;ID&gt; edit [type &lt;new_con_type&gt;] [con-name &lt;new_con_name&gt;] delete [id | uuid | path] &lt;ID&gt; monitor [id | uuid | path] &lt;ID&gt; ... reload load &lt;filename&gt; [ &lt;filename&gt;... ] import [--temporary] type &lt;type&gt; file &lt;file to import&gt; export [id | uuid | path] &lt;ID&gt; [&lt;output file&gt;][hk@localhost network-scripts]$","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"centos创建用户","date":"2020-09-05T13:40:23.000Z","path":"2020/09/05/运维/centos创建用户/","text":"创建用户123456789101112131415161718192021222324252627282930useradd wumu## 给用户添加组，一定要加a(FC4: usermod -G groupA,groupB,groupC user)-a 代表 append， 也就是 将自己添加到 用户组groupA 中，而不必离开 其他用户组。#命令的所有的选项，及其含义：Options:-c, --comment COMMENT new value of the GECOS field-d, --home HOME_DIR new home directory for the user account-e, --expiredate EXPIRE_DATE set account expiration date to EXPIRE_DATE-f, --inactive INACTIVE set password inactive after expirationto INACTIVE-g, --gid GROUP force use GROUP as new primary group-G, --groups GROUPS new list of supplementary GROUPS-a, --append append the user to the supplemental GROUPSmentioned by the -G option without removinghim/her from other groups-h, --help display this help message and exit-l, --login NEW_LOGIN new value of the login name-L, --lock lock the user account-m, --move-home move contents of the home directory to the newlocation (use only with -d)-o, --non-unique allow using duplicate (non-unique) UID-p, --password PASSWORD use encrypted password for the new password-s, --shell SHELL new login shell for the user account-u, --uid UID new UID for the user account-U, --unlock unlock the user accountusermod -a -G wumugroup wumupasswd wumu 添加sudo权限1234567visudo#找到如下行数root ALL=(ALL) ALL#添加username ALL=(ALL) ALL 免密码登录123ssh-keygenssh-copy-id -i .ssh/id_rsa.pub 用户名字@192.168.x.xxxssh 用户名字@192.168.x.xxx 使用pem登录1234567891011121314151617181920#在本地生成公钥私钥ssh-keygen#输入命令后，一路回车，即可。#将本地的公钥传到服务器上ssh-copy-id -i ~/.ssh/id_rsa.pub remote-host#会提示你输入密码，成功之后，会帮助你把公钥放在服务器上，供登录使用。#把本地的私钥转为 pem 格式，供windows上的 ssh 客户端使用openssl rsa -in ~/.ssh/id_rsa -outform pem &gt; id_rsa.pemchmod 700 id_rsa.pem#这样就导出了pem格式的私钥，因为公钥已经在服务器了，所以只要服务器上的公钥不删除，用这把私钥就能登录服务器,一般来说，经过这样设置之后，可以把ssh 密码登录的方式禁用掉，使得服务器更加安全。#关闭 ssh 密码登录vi /etc/ssh/sshd_config#修改PasswordAuthentication no#重启 ssh 服务service sshd restart","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"centos使用cockpit","date":"2020-09-05T13:40:23.000Z","path":"2020/09/05/运维/centos使用cockpit/","text":"123456789101112131415161718192021222324252627282930sudo systemctl enable --now cockpit.socket[leiakun@centos8 ~]$ sudo systemctl enable --now cockpit.socket[sudo] leiakun 的密码：Created symlink /etc/systemd/system/sockets.target.wants/cockpit.socket → /usr/lib/systemd/system/cockpit.socket.[leiakun@centos8 ~]$ [leiakun@centos8 ~]$ sudo firewall-cmd --get-services |grep cockpitRH-Satellite-6 amanda-client amanda-k5-client amqp amqps apcupsd auditbacula bacula-client bb bgp bitcoin bitcoin-rpc bitcoin-testnet bitcoin-testnet-rpcbittorrent-lsd ceph ceph-mon cfengine cockpit condor-collector ctdb dhcp dhcpv6 dhcpv6-client distcc dns dns-over-tls docker-registry docker-swarm dropbox-lansync elasticsearch etcd-client etcd-server finger freeipa-4 freeipa-ldap freeipa-ldaps freeipa-replication freeipa-trust ftp ganglia-client ganglia-master git grafana gre high-availability http https imap imaps ipp ipp-client ipsec irc ircs iscsi-target isns jenkins kadmin kdeconnect kerberos kibana klogin kpasswd kprop kshell ldap ldaps libvirt libvirt-tls lightning-network llmnr managesievematrix mdns memcache minidlna mongodb mosh mountd mqtt mqtt-tls ms-wbt mssqlmurmur mysql nfs nfs3 nmea-0183 nrpe ntp nut openvpn ovirt-imageio ovirt-storageconsole ovirt-vmconsole plex pmcd pmproxy pmwebapi pmwebapis pop3 pop3s postgresql privoxyprometheus proxy-dhcp ptp pulseaudio puppetmaster quassel radius rdp redis redis-sentinel rpc-bind rsh rsyncd rtsp salt-master samba samba-client samba-dcsane sip sips slp smtp smtp-submission smtps snmp snmptrap spideroak-lansync spotify-sync squid ssdp ssh steam-streaming svdrp svn syncthing syncthing-gui synergy syslog syslog-tls telnet tentacle tftp tftp-client tile38 tinc tor-socks transmission-client upnp-client vdsm vnc-server wbem-http wbem-https wsman wsmansxdmcp xmpp-bosh xmpp-client xmpp-local xmpp-server zabbix-agent zabbix-serversudo firewall-cmd --add-service=cockpit --permanentsudo firewall-cmd --reload 多主机管理1yum install -y cockpit-dashboard","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"centos8安装kvm","date":"2020-09-05T13:40:23.000Z","path":"2020/09/05/运维/centos8安装kvm/","text":"如何在CentOS/RHEL 8上安装KVM虚拟化基于内核的虚拟机（简称KVM）是一种开源的标准虚拟化解决方案，已紧密集成到Linux中。它是一个可加载的内核模块，将Linux转换为Type-1（裸机）虚拟机管理程序，该虚拟机管理程序创建了用于运行虚拟机（VM）的虚拟操作平台。 精选回答在KVM下，每个VM是一个Linux进程，由内核调度和管理，并具有专用的虚拟化硬件（即CPU，网卡，磁盘等）。它还支持嵌套虚拟化，使您可以在另一个VM内运行一个VM。 它的一些主要功能包括支持广泛的Linux支持的硬件平台（带有虚拟化扩展的x86硬件（Intel VT或AMD-V）），它使用SELinux和安全虚拟化（sVirt）提供增强的VM安全性和隔离，它继承了内核内存管理功能，并且支持脱机和实时迁移（在物理主机之间迁移正在运行的VM）。 在本文中，您将学习如何在CentOS 8和RHEL 8 Linux中安装KVM虚拟化，创建和管理虚拟机。 准备工作： 全新安装的CentOS 8服务器 全新安装的RHEL 8服务器 在RHEL 8服务器上启用了RedHat订阅 此外，通过运行以下命令，确保您的硬件平台支持虚拟化。 12# grep -e &apos;vmx&apos; /proc/cpuinfo #Intel systems# grep -e &apos;svm&apos; /proc/cpuinfo #AMD systems 另外，请确认内核中已加载KVM模块（默认情况下应为KVM模块）。 ＃lsmod | grep kvm 这是基于英特尔的测试系统上的示例输出： img 在以前的KVM指南系列中，我们展示了如何使用KVM（基于内核的虚拟机）在Linux中创建虚拟机，并展示了如何使用virt-manager GUI工具（根据RHEL已弃用）创建和管理VM。8个文档）。对于本指南，我们将采用不同的方法，我们将使用Cockpit Web控制台。 步骤1：在CentOS 8上设置Cockpit Web控制台 1.在Cockpit是一个易于使用的集成和可扩展的基于Web的界面在网页浏览器来管理Linux服务器。它使您能够执行系统任务，例如配置网络，管理存储，创建VM和使用鼠标检查日志。它使用系统的普通用户登录名和特权，但也支持其他身份验证方法。 它是预先安装的，并已在新安装的CentOS 8和RHEL 8系统上启用，如果尚未安装，请使用以下dnf命令进行安装。应安装cockpit-machines扩展程序以管理基于Libvirt的 VM 。 # dnf install cockpit cockpit-machines 2.软件包安装完成后，启动座舱插座，使其在系统启动时自动启动，并检查其状态以确认其已启动并正在运行。 # systemctl start cockpit.socket # systemctl enable cockpit.socket # systemctl status cockpit.socket img 3.接下来，使用firewall-cmd命令将cockpit服务添加到默认启用的系统防火墙中，然后重新加载防火墙配置以应用新更改。 # firewall-cmd –add-service=cockpit –permanent # firewall-cmd –reload 4.要访问CockpitWeb控制台，请打开Web浏览器并使用以下URL进行导航。 https://FQDN:9090/或者https://SERVER_IP:9090/ 该Cockpit采用的是自签名证书启用HTTPS，只需使用该连接，当你在浏览器的警告。在登录页面上，使用您的服务器用户帐户凭据。 img img 步骤2：安装KVM虚拟化CentOS 8 5.接下来，如下安装虚拟化模块和其他虚拟化软件包。所述的virt安装包提供用于从所述命令行界面进行安装的虚拟机的工具，和一个的virt查看器用于查看虚拟机。 # dnf module install virt # dnf install virt-install virt-viewer 6.接下来，运行virt-host-validate命令以验证主机是否设置为运行libvirt系统管理程序驱动程序。 # virt-host-validate img 7.接下来，启动libvirtd守护程序（libvirtd），并使它在每次引导时自动启动。然后检查其状态以确认它已启动并正在运行。 # systemctl start libvirtd.service # systemctl enable libvirtd.service # systemctl status libvirtd.service img 步骤3：通过Cockpit设置网桥（虚拟网络交换机） 8.现在创建一个网桥（虚拟网络交换机），将虚拟机集成到与主机相同的网络中。默认情况下，一旦启动libvirtd守护程序，它将激活默认网络接口virbr0，该接口代表以NAT模式运行的虚拟网络交换机。 在本指南中，我们将以桥接模式创建名为br0的网络接口。这将使虚拟机可在主机网络上访问。 在座舱主界面中，单击“ 网络”，然后单击“ 添加网桥”，如以下屏幕截图所示。 img 9.从弹出窗口中，输入网桥名称，然后选择网桥从站或端口设备（例如，代表以太网接口的enp2s0），如以下屏幕截图所示。然后单击“ 应用”。 img 10.现在，当您查看“ 接口 ”列表时，新的网桥应显示在此处，几秒钟后，应禁用以太网接口（关闭）。 img 步骤4：通过Cockpit Web控制台创建和管理虚拟机 11.在座舱主界面中，单击“ 虚拟机”选项，如以下屏幕快照中突出显示。在“ 虚拟机”页面上，单击创建虚拟机。 img 12.将显示一个带有用于创建新VM的选项的窗口。输入连接，名称（例如ubuntu18.04），安装源类型（在测试系统上，我们已将ISO映像存储在存储池下，即/ var / lib / libvirt / images /），安装源，存储，大小，内存如下图所示。输入安装源后，应自动选择OS供应商和操作系统。 还要选中立即启动VM的选项，然后单击“ 创建”。 img 13.在上一步中单击“ 创建”后，应自动启动VM，并使用提供的ISO映像启动VM。继续安装客户机操作系统（在本例中为Ubuntu 18.04）。 img 如果你点击网络接口的的虚拟机，网络源应注明新建桥网络接口。 img 并且在安装过程中，在配置网络接口的步骤中，您应该能够注意到VM以太网接口从主机网络的DHCP服务器接收IP地址。 img 请注意，您需要安装OpenSSH软件包才能从主机网络上的任何计算机通过SSH访问来宾OS，如上一节所述。 14.客户机操作系统安装完成后，请重新引导VM，然后转到“ 磁盘”并分离/除去VM磁盘下的cdrom设备。然后单击“运行”以启动VM。 img img 15.现在，在Consoles（控制台）下，您可以使用在OS安装期间创建的用户帐户登录来宾OS。 img 步骤5：通过SSH访问虚拟机访客操作系统 16.要通过SSH从主机网络访问新安装的来宾OS，请运行以下命令（将10.42.0.197替换为来宾的IP地址）。 $ ssh tecmint@10.42.0.197 img 17.要关闭，重新启动或删除VM，请从VM列表中单击它，然后使用以下屏幕快照中突出显示的按钮。 img 在本文中，介绍了如何安装KVM虚拟化软件包以及如何通过cockpit Web控制台创建和管理VM。","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"网络配置三种模式对比（桥接模式，主机模式，网络地址转换）","date":"2020-08-27T10:29:58.000Z","path":"2020/08/27/docker/网络配置三种模式对比（桥接模式，主机模式，网络地址转换）/","text":"VMware三种网络模式简介​ VMWare提供了三种工作模式，它们是bridged(桥接模式)、NAT(网络地址转换模式)和host-only(主机模式)。安装好虚拟机以后，在网络连接里面可以看到多了两块网卡。如下图。 1 bridged(桥接模式)1.1 模式简介 在这种模式下，VMWare虚拟出来的操作系统就像是局域网中的一台独立的主机，它可以访问网内任何一台机器。 在桥接模式下，你需要手工为虚拟系统配置IP地址、子网掩码，而且还要和宿主机器处于同一网段，这样虚拟系统才能和宿主机器进行通信。同时，由 于这个虚拟系统是局域网中的一个独立的主机系统，那么就可以手工配置它的TCP/IP配置信息，以实现通过局域网的网关或路由器访问互联网。 使用桥接模式的虚拟系统和宿主机器的关系，就像连接在同一个Hub上的两台电脑。想让它们相互通讯，你就需要为虚拟系统配置IP地址和子网掩码，否则就无法通信。 1.2 工作的虚拟网卡​ bridged模式下的VMnet0虚拟网络 1.3 架构图1）使用VMnet0虚拟交换机，此时虚拟机相当与网络上的一台独立计算机与主机一样，拥有一个独立的IP地址。使用桥接方式，A，A1，A2，B可互访。 2）虚拟机就像一台真正的计算机一样，直接连接到实际的网络上，可以理解为与宿主机没有任何联系。 1.4 使用范围​ 1）如果你想利用VMWare在局域网内新建一个虚拟服务器，为局域网用户提供网络服务，就应该选择桥接模式。 2）如果你有路由器的话，那么就可以使用桥接方式上网，因为此时你申请的IP地址是写在了路由器上，而不是你的机器上，这样包括你的主机，虚拟机，也包括 连在路由器上的其他人的机器，都将可以上网，使用着由路由器分配的IP地址。 2 host-only(主机模式)2.1 模式简介​ 在某些特殊的网络调试环境中，要求将真实环境和虚拟环境隔离开，这时你就可采用host-only模式。在host-only模式中，所有的虚拟系统是可以相互通信的，但虚拟系统和真实的网络是被隔离开的。 提示：在host-only模式下，虚拟系统和宿主机器系统是可以相互通信的，相当于这两台机器通过双绞线互连。 在host-only模式下，虚拟系统的TCP/IP配置信息(如IP地址、网关地址、DNS服务器等)，都是由VMnet1(host-only)虚拟网络的DHCP服务器来动态分配的。 如果你想利用VMWare创建一个与网内其他机器相隔离的虚拟系统，进行某些特殊的网络调试工作，可以选择host-only模式。 Linux虚拟机实现Host-only方式上网 2.2 工作的虚拟网卡 host-only模式下的 VMnet1虚拟网络 2.3 架构图1）使用Vmnet1虚拟交换机，此时虚拟机只能与虚拟机、主机互访。也就是不能上Internet。使用Host方式，A，A1，A2可以互访，但A1，A2不能访问B，也不能被B访问。 2）这种方式下，虚拟机的网卡连接到宿主的 VMnet1 上，但系统并不为虚拟机提供任何路由服务，因此虚拟机只能和宿主机进行通信，而不能连接到实际网络上。 2.4 使用范围 如果你想利用VMWare创建一个与网内其他机器相隔离的虚拟系统，进行某些特殊的网络调试工作，可以选择host-only模式。 3 NAT(网络地址转换模式)3.1 模式简介​ 使用NAT模式，就是让虚拟系统借助NAT(网络地址转换)功能，通过宿主机器所在的网络来访问公网。也就是说，使用NAT模式可以实现在虚拟 系统里访问互联网。NAT模式下的虚拟系统的TCP/IP配置信息是由VMnet8(NAT)虚拟网络的DHCP服务器提供的，无法进行手工修改，因此虚拟系统也就无法和本局域网中的其他真实主机进行通讯。采用NAT模式最大的优势是虚拟系统接入互联网非常简单，你不需要进行任何其他的配置，只需要宿主机 器能访问互联网即可。 3.2 工作的虚拟网卡 NAT模式下的VMnet8虚拟网络 3.3 架构图1） 使用Vmnet8虚拟交换机，此时虚拟机可以通过主机单向网络上的其他工作站，其他工作站不能访问虚拟机。用NAT方式，A1，A2可以访问B，但B不可以访问A1，A2。但A，A1，A2可以互访。 2） 这种方式下，虚拟机的网卡连接到宿主的 VMnet8 上。此时系统的 VMWare NAT Service 服务就充当了路由器的作用，负责将虚拟机发到 VMnet8 的包进行地址转换之后发到实际的网络上，再将实际网络上返回的包进行地址转换后通过 VMnet8 发送给虚拟机。VMWare DHCP Service 负责为虚拟机提供 DHCP 服务。 3.4 使用范围​ 如果你想利用VMWare安装一个新的虚拟系统，在虚拟系统中不用进行任何手工配置就能直接访问互联网，建议你采用NAT模式。 参考https://blog.csdn.net/CleverCode/article/details/45934233","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"docker-compose文件编写","date":"2020-08-27T10:10:37.000Z","path":"2020/08/27/docker/docker-compose文件编写/","text":"","tags":[]},{"title":"SpringBoot+Quartz框架的实现","date":"2020-08-14T10:39:29.000Z","path":"2020/08/14/SpringBoot-Quartz框架的实现/","text":"定时任务 想必做程序的都或多或少的接触过,以便于我们以某个特定的 时间/频率 去执行所需要的程序,Quartz 是一个优秀的框架,可以根据我们的配置将 定时任务的执行 时间/频率 持久化至数据库, 我们通过修改数据库中的任务下次执行时间,达到不需要等到任务配置执行的原始 时间/频率,随时地运行定时任务; 并且可以看到任务的运行状态 WATING BLOCKING等 1.导入依赖 quartz自定义配置的数据源会使用C3P0创建连接,所以要引入C3P0依赖 1234567891011 &lt;!-- Quartz定时任务 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-quartz&lt;/artifactId&gt; &lt;/dependency&gt;&lt;!--C3P0 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.5.5&lt;/version&gt; &lt;/dependency&gt; 2.quartz 配置文件,yml方式 创建定时任务表的sql太长,这里就不贴了,我会将sql上传至GitHub,文末我会贴地址 12345678910111213141516171819202122232425262728293031323334353637383940414243## quartz定时任务spring: quartz: #jdbc 采用数据库方式 memory 采用内存方式 job-store-type: jdbc initialize-schema: embedded #设置自动启动，默认为 true auto-startup: true #启动时更新己存在的Job overwrite-existing-jobs: true properties: org: quartz: scheduler: instanceName: MyScheduler instanceId: AUTO jobStore: #指定使用的JobStore class: org.quartz.impl.jdbcjobstore.JobStoreTX driverDelegateClass: org.quartz.impl.jdbcjobstore.StdJDBCDelegate #数据库前缀 tablePrefix: QRTZ_ #是否为集群 isClustered: false #检测任务执行时间的间隔 毫秒 misfireThreshold: 5000 clusterCheckinInterval: 10000 #数据源名称 dataSource: myDS #线程池配置 threadPool: class: org.quartz.simpl.SimpleThreadPool threadCount: 20 threadPriority: 5 threadsInheritContextClassLoaderOfInitializingThread: true #数据源 dataSource: myDS: driver: com.mysql.cj.jdbc.Driver URL: jdbc:mysql://localhost:3306/test?characterEncoding=UTF-8&amp;useUnicode=true&amp;useSSL=false&amp;tinyInt1isBit=false&amp;serverTimezone=Asia/Shanghai user: root password: root maxConnections: 5 有同学可能会问了,配置文件是配置好了,是在哪引用的呢? 别急, 且听我娓娓道来 spring-boot-starter-quartz (为方便诉说,下文中使用 bootquartz代替) 这个包下的QuartzProperties会帮我们自动加载配置文件,且看以下部分截图 img 可以看到, QuartzProperties 使用了 @ConfigurationProperties 加载了 spring.quartz 前缀的配置,也就是上面我们的配置文件中的配置;加载之后呢, bootquartz包下有 类 QuartzAutoConfiguration, 看名字就可以知道,这个就是自动配置 quartz的类了. 所以我们不需要再去通过代码去配置 SchedulerFactoryBean 了,这是后话 QuartzAutoConfiguration 类注释 img 通过上面的截图我们发现,这里引用了 QuartzProperties 其中的 quartzScheduler()方法帮助我们创建了 SchedulerFactoryBean 并使用了 QuartzProperties 中的自定义配置,以下是quartzScheduler()部分代码 1234567891011@Bean@ConditionalOnMissingBeanpublic SchedulerFactoryBean quartzScheduler() &#123; SchedulerFactoryBean schedulerFactoryBean = new SchedulerFactoryBean(); if (!this.properties.getProperties().isEmpty()) &#123; schedulerFactoryBean.setQuartzProperties(asProperties(this.properties.getProperties())); &#125; customize(schedulerFactoryBean); return schedulerFactoryBean;&#125; 姑且一提,方法中调用了 customize(SchedulerFactoryBean schedulerFactoryBean) 方法,这个方法会寻找实现了 SchedulerFactoryBeanCustomizer 接口的配置类,在其实现方法 customize(SchedulerFactoryBean schedulerFactoryBean)中 可对 SchedulerFactoryBean 使用代码自定义配置 那么到这里结束了吗?不! 这里还有本文中最大的一个坑,作者深受其扰,扒了两天的源码才找到这个问题!!! 如果我们的项目中有其它的默认数据源,那么quartz会忽略配置文件中自定义数据源,使用默认数据源,原因看以下源码 首先是 QuartzAutoConfiguration 中的 静态内部类 JdbcStoreTypeConfiguration 12345678910111213141516171819202122232425262728 @Configuration @ConditionalOnSingleCandidate(DataSource.class) protected static class JdbcStoreTypeConfiguration &#123; @Bean @Order(0) public SchedulerFactoryBeanCustomizer dataSourceCustomizer( QuartzProperties properties, DataSource dataSource, @QuartzDataSource ObjectProvider&lt;DataSource&gt; quartzDataSource, ObjectProvider&lt;PlatformTransactionManager&gt; transactionManager) &#123; return (schedulerFactoryBean) -&gt; &#123; if (properties.getJobStoreType() == JobStoreType.JDBC) &#123; //重点在这里 begin DataSource dataSourceToUse = getDataSource(dataSource, quartzDataSource); schedulerFactoryBean.setDataSource(dataSourceToUse); //重点在这里 end PlatformTransactionManager txManager = transactionManager.getIfUnique(); if (txManager != null) &#123;schedulerFactoryBean.setTransactionManager(txManager); &#125; &#125; &#125;; &#125; private DataSource getDataSource(DataSource dataSource, ObjectProvider&lt;DataSource&gt; quartzDataSource) &#123; DataSource dataSourceIfAvailable = quartzDataSource.getIfAvailable(); return (dataSourceIfAvailable != null) ? dataSourceIfAvailable : dataSource; &#125; 其中的getDataSource 方法判断了我们项目中的 quartzDataSource是否为空,如果为空,那么就使用默认的数据源;quartzDataSource怎么才能不为空呢? 可以看到dataSourceCustomizer 方法参数中有 @QuartzDataSource 注解, 这个注解会去寻找我们项目中使用@QuartzDataSource配置的数据源,但是 我都已经在配置文件中自定义了数据源,再去手动配置一遍不是多此一举吗? 接着往下看 SchedulerFactoryBean 的初始化方法部分源码▼ 1234567private void initSchedulerFactory(StdSchedulerFactory schedulerFactory) throws SchedulerException, IOException &#123; Properties mergedProps = new Properties(); if (this.dataSource != null) &#123;mergedProps.setProperty(StdSchedulerFactory.PROP_JOB_STORE_CLASS, LocalDataSourceJobStore.class.getName()); &#125;&#125; 我们在静态内部类设置过了数据源,初始化方法只要发现数据源不为空,那么就使用会使用 LocalDataSourceJobStore 覆盖我们quartz配置文件中设置的 org.quartz.jobStore.class: org.quartz.impl.jdbcjobstore.JobStoreTX 而LocalDataSourceJobStore 中的初始化方法使用的是 SchedulerFactoryBean 中设置的数据源,所以我们quartz配置文件中的数据源才不会生效!!! 怎么解决呢? 我们上面提到了customize(SchedulerFactoryBean schedulerFactoryBean) 方法,这个方法会寻找实现了 SchedulerFactoryBeanCustomizer 接口的配置类,在其实现方法 customize(SchedulerFactoryBean schedulerFactoryBean)中 可对 SchedulerFactoryBean 使用代码自定义配置 所以 我们只要在SchedulerFactoryBean 创建后调用初始化方法之前,再将DataSource设置为null,那么SchedulerFactoryBean 初始化时,将会使用我们配置文件中的JobStoreTX去寻找我们配置的数据源了,至此,填坑完毕▼ 1234567891011121314151617181920212223242526import org.springframework.boot.autoconfigure.quartz.SchedulerFactoryBeanCustomizer;import org.springframework.context.annotation.Configuration;import org.springframework.scheduling.quartz.SchedulerFactoryBean;/** * @author Demo-Liu * @create 2020-06-12 11:20 * @description 配置定时任务 */@Configurationpublic class SchedulerConfig implements SchedulerFactoryBeanCustomizer &#123; /** * @Author Demo-Liu * @Date 20200614 12:44 * 自定义 quartz配置 * @param schedulerFactoryBean */ @Override public void customize(SchedulerFactoryBean schedulerFactoryBean) &#123; schedulerFactoryBean.setDataSource(null); &#125;&#125; 以上 在文末附上我的GitHub小demo,其中包含了quartz的数据库建表sql,并提供了一种可以更加灵活便捷的通过yml文件配置定时任务的方式 地址: GitHub-BootQuartzYml 以下是yml配置文件配置定时任务的例子 12345678910111213141516171819202122232425262728293031#通过加载此配置文件实现动态创建Job 旨在通过一种更灵活便捷的方式来控制定时任务#20200611 by Demo-Liu#jobs:# jobList:# - jobConf:# name: 测试任务 #任务名 可选# job: com.example.demo.quartz.DemoJob #任务类包路径 必须# param: #可为job类注入参数(可配置多项) 可选# jtbs: test# cron: 10 * * * * ? #任务执行频率 必须# active: true #任务激活状态 必须jobs: jobList: - jobConf: name: 测试任务 job: com.example.demo.quartz.DemoJob param: jtbs: test ss: test2 cron: 0/10 * * * * ? active: true - jobConf: name: 测试任务2 job: com.example.demo.quartz.DemoJob2 param: jtbs: test ss: test2 cron: 0/10 * * * * ? active: false","tags":[{"name":"springboot","slug":"springboot","permalink":"http://wumuwumu.github.io/tags/springboot/"},{"name":"quartz","slug":"quartz","permalink":"http://wumuwumu.github.io/tags/quartz/"}]},{"title":"框架的实现","date":"2020-08-14T10:39:21.000Z","path":"2020/08/14/框架的实现/","text":"","tags":[]},{"title":"","date":"2020-06-05T07:54:05.000Z","path":"2020/06/05/运维/frp搭建/","text":"简述frp是有个内网穿透的工具，分为客户端和服务端。客户端的程序名称是frpc，服务端的程序名称是frps。 服务器下载12// lorawan gateway 下载mipsle版本https://github.com/fatedier/frp/releases 配置文件123# frps.ini[common]bind_port = 7000 # 用于与客户端之间通信 运行程序1./frps -c ./frps.ini 客户端配置文件详细看https://github.com/fatedier/frp/blob/master/README_zh.md#dashboard 123456789101112131415161718# frpc.ini[common]server_addr = x.x.x.xserver_port = 7000# frp multi user 插件user = user1meta_token = 123[web]type = httplocal_port = 80custom_domains = www.yourdomain.com[ssh]type = tcplocal_ip = 127.0.0.1local_port = 22remote_port = 6000 运行程序1./frpc -c ./frpc.ini openwrt开机启动 配置服务 123456789101112131415161718## /etc/init.d/frpc#!/bin/sh /etc/rc.common# &quot;new&quot; style init script# Look at /lib/functions/service.sh on a running system for explanations of what other SERVICE_# options you can use, and when you might want them.START=80APP=frpcstart() &#123; service_start /usr/sbin/$APP -c /etc/frpc.ini &gt; /usr/frpc.log &amp;&#125;stop() &#123; service_stop /usr/sbin/$APP&#125; 开机启动 123chmod +x /etc/init.d/frpc/etc/init.d/frpc enable/etc/init.d/frpc start","tags":[{"name":"frp","slug":"frp","permalink":"http://wumuwumu.github.io/tags/frp/"}]},{"title":"Vue3工程搭建","date":"2020-05-09T06:23:23.000Z","path":"2020/05/09/vue/Vue3工程搭建/","text":"创建工程12npm i -g @vue/clivue create test","tags":[{"name":"vue","slug":"vue","permalink":"http://wumuwumu.github.io/tags/vue/"}]},{"title":"编写自己的Springboot-starter","date":"2020-04-18T07:53:46.000Z","path":"2020/04/18/java/编写自己的Springboot-starter/","text":"前言我们都知道可以使用SpringBoot快速的开发基于Spring框架的项目。由于围绕SpringBoot存在很多开箱即用的Starter依赖，使得我们在开发业务代码时能够非常方便的、不需要过多关注框架的配置，而只需要关注业务即可。 例如我想要在SpringBoot项目中集成Redis，那么我只需要加入spring-data-redis-starter的依赖，并简单配置一下连接信息以及Jedis连接池配置就可以。这为我们省去了之前很多的配置操作。甚至有些功能的开启只需要在启动类或配置类上增加一个注解即可完成。 那么如果我们想要自己实现自己的Starter需要做些什么呢？下面就开始介绍如何实现自己的SpringBoot-xxx-starter。 原理首先说说原理，我们知道使用一个公用的starter的时候，只需要将相应的依赖添加的Maven的配置文件当中即可，免去了自己需要引用很多依赖类，并且SpringBoot会自动进行类的自动配置。那么 SpringBoot 是如何知道要实例化哪些类，并进行自动配置的呢？ 下面简单说一下。 首先，SpringBoot 在启动时会去依赖的starter包中寻找 resources/META-INF/spring.factories文件，然后根据文件中配置的Jar包去扫描项目所依赖的Jar包，这类似于 Java 的 SPI 机制。 第二步，根据 spring.factories配置加载AutoConfigure类。 最后，根据 @Conditional注解的条件，进行自动配置并将Bean注入Spring Context 上下文当中。 我们也可以使用@ImportAutoConfiguration({MyServiceAutoConfiguration.class}) 指定自动配置哪些类。 实现终于到了代码实现的步骤，接下来就开始编码我们自己的SpringBoot-starter。 第一步创建一个SpringBoot 项目，并添加下面两个依赖到pom.xml文件当中1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 其中 spring-boot-configuration-processor的作用是编译时生成 spring-configuration-metadata.json，此文件主要给IDE使用。如当配置此jar相关配置属性在 application.yml，你可以用ctlr+鼠标左键点击属性名，IDE会跳转到你配置此属性的类中。 我们日常使用的Spring官方的Starter一般采取spring-boot-starter-{name}的命名方式，如 spring-boot-starter-web。 而非官方的Starter，官方建议 artifactId 命名应遵循{name}-spring-boot-starter的格式。 例如：ysc-spring-boot-starter 。 1234&lt;groupId&gt;com.ysc&lt;/groupId&gt;&lt;artifactId&gt;simple-spring-boot-starter&lt;/artifactId&gt;&lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; 第二步编写我们的Service类这里讲一下我们的Starter要实现的功能，很简单，提供一个Service，包含一个能够将配置文件中配置的字符串根据传入的字符进行分割的方法String[] split(String separatorChar)。 123456789101112public class StarterService &#123; private String config; public StarterService(String config) &#123; this.config = config; &#125; public String[] split(String separatorChar) &#123; return StringUtils.split(this.config, separatorChar); &#125; &#125; 第三步编写配置文件读取类123456789101112@ConfigurationProperties(\"example.service\")public class StarterServiceProperties &#123; private String config; public void setConfig(String config) &#123; this.config = config; &#125; public String getConfig() &#123; return config; &#125; &#125; 第四步，编写AutoConfigure类 ，这步是关键点12345678910111213141516@Configuration@ConditionalOnClass(StarterService.class)@EnableConfigurationProperties(StarterServiceProperties.class)public class StarterAutoConfigure &#123; @Autowired private StarterServiceProperties properties; @Bean @ConditionalOnMissingBean @ConditionalOnProperty(prefix = \"example.service\", value = \"enabled\", havingValue = \"true\") StarterService starterService ()&#123; return new StarterService(properties.getConfig()); &#125;&#125; 解释一下代码中用到的几个注解： @ConditionalOnClass，当classpath下发现该类的情况下进行自动配置。 @ConditionalOnMissingBean，当Spring Context中不存在该Bean时。 @ConditionalOnProperty(prefix = &quot;example.service&quot;,value = &quot;enabled&quot;,havingValue = &quot;true&quot;)，当配置文件中example.service.enabled=true时。 下面列举SpringBoot中的所有@Conditional注解及作用1234567891011@ConditionalOnBean:当容器中有指定的Bean的条件下 @ConditionalOnClass：当类路径下有指定的类的条件下 @ConditionalOnExpression:基于SpEL表达式作为判断条件 @ConditionalOnJava:基于JVM版本作为判断条件 @ConditionalOnJndi:在JNDI存在的条件下查找指定的位置 @ConditionalOnMissingBean:当容器中没有指定Bean的情况下 @ConditionalOnMissingClass:当类路径下没有指定的类的条件下 @ConditionalOnNotWebApplication:当前项目不是Web项目的条件下 @ConditionalOnProperty:指定的属性是否有指定的值 @ConditionalOnResource:类路径下是否有指定的资源 @ConditionalOnSingleCandidate:当指定的Bean在容器中只有一个，或者在有多个Bean的情况下，用来指定首选的Bean @ConditionalOnWebApplication:当前项目是Web项目的条件下 最后一步，在resources/META-INF/下创建spring.factories文件，并添加如下内容：1org.springframework.boot.autoconfigure.EnableAutoConfiguration=com.example.autocinfigure.StarterAutoConfigure 至此，我们的一个Starter代码部分就是完成了，下面将项目安装到本地Maven仓库中。 发布在项目根目录执行 mvn install 进行打包安装。 测试将Starter项目的依赖添加到我们自己的SpringBoot项目中 12345&lt;dependency&gt; &lt;groupId&gt;com.ysc&lt;/groupId&gt; &lt;artifactId&gt;simple-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; 在application.yml 配置文件中添加配置信息： 1234example service enabled: true config： abc-des-dde,SSS-DRS-RE,SDR-SDFR-XXX 在本地使用JUnit进行代码测试 12345678@Autowiredprivate StarterService starterService;@Testpublic void starterTest() &#123; String[] splitArray = starterService.split(\",\"); System.out.println(splitArray);&#125;","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"},{"name":"sprintboot","slug":"sprintboot","permalink":"http://wumuwumu.github.io/tags/sprintboot/"}]},{"title":"redis开启远程连接","date":"2019-12-16T09:44:28.000Z","path":"2019/12/16/redis开启远程连接/","text":"1、修改redis服务器的配置文件vi redis.conf 注释以下绑定的主机地址 # bind 127.0.0.1 或vim redis.conf bind 0.0.0.0 protected-mode no 2、修改redis服务器的参数配置修改redis的守护进程为no，不启用 127.0.0.1:6379&gt; config set daemonize “no” OK 修改redis的保护模式为no，不启用 127.0.0.1:6379&gt; config set protected-mode”no” OK 或者 config set requirepass 123 -&gt;123是密码 注意：开启 6379端口","tags":[{"name":"redis","slug":"redis","permalink":"http://wumuwumu.github.io/tags/redis/"}]},{"title":"nginx的servername配置","date":"2019-12-16T09:42:19.000Z","path":"2019/12/16/nginx/nginx的servername配置/","text":"编译自： server_names 目录： 通配符主机名 正则表达式主机名 混杂主机名 对主机名的优化 兼容性 nginx 的 server names 由 server_name 指令定义，server name 是 nginx 用于选择以哪个 server 区块处理访问请求的依据参数。可参考 《nginx 是如何处理请求的》 的描述。 server name 可以用三种方式定义： 定义准确的名字 定义通配符名字 定义正则表达式名字 例如： 1234567891011121314151617181920212223server &#123; listen 80; server_name example.org www.example.org; ...&#125;server &#123; listen 80; server_name *.example.org; ...&#125;server &#123; listen 80; server_name mail.*; ...&#125;server &#123; listen 80; server_name ~^(?&lt;user&gt;.+)\\.example\\.net$; ...&#125; 当 nginx 以请求的 server name 查找匹配的虚拟主机时，如果匹配的 server 区块不止一个，nginx 按照如下的优先顺序选择 server 区块： 准确的主机名 以 “*” 起始的最长的通配主机名 以 “*” 结尾的最长的通配主机名 第一个匹配的正则表达式（按照配置文件中的顺序） 所以，如果同时有一个通配主机名和正则表达式主机名与访问请求的 server name 匹配，nginx 会选择通配主机名的 server 区块处理请求。 通配主机名 通配主机名只能在起始和末尾使用 “*” 字符，而且必须以 “.” 分隔。形如 “www.*.example.org” 或者 “w*.example.org” 的通配主机名是无效的。要达到这个匹配效果，只有使用正则表达式： 12“www.*.example.org” -&gt; “~^www\\..+\\.example\\.org$”“w*.example.org” -&gt; “~^w.*\\.example\\.org$” “*” 号可以匹配多个名字区域，“*.example.org” 不仅可以匹配 www.example.org，也能够匹配 www.sub.example.org。 正则表达式主机名 nginx 使用的正则表达式与 Perl 语言的正则表达式（PCRE）兼容。使用正则表达式主机名，server name 必须以 “~” 字符为起始字符。 1server_name ~^www\\d+\\.example\\.net$; 如果不以 “~” 字符为起始字符，该 server name 将被视为 “准确的主机名” 或者当 server name 包含 “*” 时被视为 “通配主机名” (多数情况是非法通配主机名，因为只有当 “*” 在 server name 的起始或结尾时才合法)。 不要忘记设置 “^” 和 “$” 锚定符对主机名进行界定，这不是 nginx 的配置语法要求，而是为了使正则表达式能正确匹配。 同时也要注意，域名的分隔符 “.” 在正则表达式中应该以 “\\” 引用。如果在正则表达式中使用了 “{” 和 “}” 字符，应该将整个正则表达式引用起来，因为花括弧在 nginx 配置中也有特殊意义，引用起来以避免被 nginx 错误解析。例如： 1server_name \"~^(?&lt;name&gt;\\w\\d&#123;1,3&#125;+)\\.example\\.net$\"; 如果不引用起来，nginx 会启动失败，并显示如下错误信息： 1directive \"server_name\" is not terminated by \";\" in ... 正则表达式的 named capture （使用一个名字对匹配的字符串进行引用）可被视为一个变量，在后面的配置中使用： 1234567server &#123; server_name ~^(www\\.)?(?&lt;domain&gt;.+)$; location / &#123; root /sites/$domain; &#125;&#125; PCRE 库支持 named capture，有如下几种语法： 123?&lt;name&gt; Perl 5.10 compatible syntax, supported since PCRE-7.0?'name' Perl 5.10 compatible syntax, supported since PCRE-7.0?P&lt;name&gt; Python compatible syntax, supported since PCRE-4.0 可参考：pcre2pattern： 12345678910\\d any decimal digit\\D any character that is not a decimal digit\\h any horizontal white space character\\H any character that is not a horizontal white space character\\s any white space character\\S any character that is not a white space character\\v any vertical white space character\\V any character that is not a vertical white space character\\w any &quot;word&quot; character\\W any &quot;non-word&quot; character 如果 nginx 启动失败，并显示如下信息： 1pcre_compile() failed: unrecognized character after (?&lt; in ... 这表示 PCRE 库太老旧，可尝试使用 “?P&lt;name&gt;” 替代 “?&lt;name&gt;”。 named capture 也能以数字形式使用： 1234567server &#123; server_name ~^(www\\.)?(.+)$; location / &#123; root /sites/$2; &#125;&#125; 无论如何，数字形式的使用应尽量简单，因为数字是只是顺序标识，而不是被匹配的字符串的标识，这导致数字引用很容易被覆盖。 混杂主机名 有一些主机名是被特殊对待的。 对于未定义 “Host” 请求首部的请求，如果希望在某个 server 区块中处理这样的请求，应在 server_name 指令的参数中添加 “” 空字符串参数： 12345server &#123; listen 80; server_name example.org www.example.org \"\"; ...&#125; 在《nginx 是如何处理访问请求的》一文中曾经介绍过，如果 server 区块中没有定义 server_name 指令，便如同定义了 server_name “”。 1234Note:在 0.8.48 版以前，遇到 server 区块中没有定义 server_name 指令的情况，会将系统的主机名设置为 server 区块的 server name，而不是自动设置 \"\" 为server name。 在 0.9.4 版本，如果设置：server_name $hostname，会将系统的主机名设置为 server name。 如果某个访问使用了 IP 地址 而不是 server name，“Host” 请求首部会包含 IP 地址。对于这样的请求，可使用如下的配置： 123456789server &#123; listen 80; server_name example.org www.example.org \"\" 192.168.1.1 ; ...&#125; 下面是一个 catch-all server 区块的配置，使用了 “_” 作为 server name: 12345server &#123; listen 80 default_server; server_name _; return 444;&#125; 这个 server name 并没有什么特殊之处，它仅是一个无效的域名而已，也可以使用其他类似的名字，如 “–” and “!@#” 。 0.6.25 版以前的 nginx 曾经支持一个特殊的 server name: “*”，这个特殊主机名被错误的解释成一个 catch-all 主机名。但它从未以一个 catch-all 或者 通配主机名工作，它的功能实际上与现在的 server_name_in_redirect 指令的功能相同：server_name_in_redirect 特殊的 server name “*” 现在已经被弃用，应使用 server_name_in_redirect 指令。 要注意的是，使用 server_name 指令无法指定 defalt server 或是 catch-all name，这是 listen 指令的属性，不是 server_name 指令的属性。可参考《nginx 是如何处理访问请求的》。 我们可以定义两个 server，它们都同时监听于 :80 端口 和 :8080 端口，将其中一个设置为 :80 端口的默认 server，将另一个设置为 :8080 端口的默认 server： 12345678910111213server &#123; listen 80; listen 8080 default_server; server_name example.net; ...&#125;server &#123; listen 80 default_server; listen 8080; server_name example.org; ...&#125; 对主机名的优化 准确的主机名、以 “*” 起始的通配主机名、以 “*” 结尾的通配主机名，这三种主机名被存放在三个 hash table 中。这三个 hash table 是与监听端口绑定的。hash table 的大小在配置阶段被优化，优化的目的是努力降低这些名字在 CPU 缓存中命中失败的几率。关于设置 hash table 的详细讨论请参考：hash 在匹配主机名时，首先查找“准确主机名”的 hash table，如果没有找到，会查找以 “*” 起始的“通配主机名”的 hash table，如果没有仍未找到，会查找以 “*” 结尾的“通配主机名”的 hash table。 对于“通配主机名”的 hash table 的检索会更慢，因为是以主机名的域名部分去检索的。 注意，对于特殊的通配主机名，形如 “.example.org”，这样的主机名是存放在“通配主机名”的 hash table 中，而不是存放在“准确主机名”的 hash table 中。 如果前面都未找到，正则表达式会按写在配置文件中的顺序被测试，因此正则表达式是最慢的方法，并且没有可扩展性。 因为以上这些原因，在可能的情况下最好使用 “准确的主机名”。例如，如果对于 example.org 和 www.example.org 的请求最为频繁，对他们进行显式的定义会更有效率： 12345server &#123; listen 80; server_name example.org www.example.org *.example.org; ...&#125; 下面的定义方法不如上面的配置有效率： 12345server &#123; listen 80; server_name .example.org; ...&#125; 如果定义了大量的主机名，或者使用了很长的主机名，应在配置文件的 http context 中调整这个两个参数： server_names_hash_max_size server_names_hash_bucket_size server_names_hash_bucket_size 指令的默认值可能为 32 或 64 或 其他数字，这是根据 CPU 缓存线大小而定的。如果默认值为 32，而且定义了一个 server name 为：“too.long.server.name.example.org” 这时 nginx 就不能启动，而且显示如下的错误信息： 12could not build the server_names_hash,you should increase server_names_hash_bucket_size: 32 遇到这种情况，应将默认值设置为原来的两倍： 123http &#123; server_names_hash_bucket_size 64; ... 如果定义了大量的主机名，可能显示如下的错误信息： 123could not build the server_names_hash,you should increase either server_names_hash_max_size: 512or server_names_hash_bucket_size: 32 遇到这种情况，首先尝试调整 server_names_hash_max_size 的值，设置为大于 server name 总数的值。如果这样设置仍不能让 nginx 正常启动，或者 nginx 启动的时间变得过长，再尝试增加 server_names_hash_bucket_size 的值。 如果一个 server 是某个监听端口唯一的 server，这时 nginx 根本不会去测试 server name，同时也不会为该监听端口构建 hash table。但其中又有一个例外，如果 server name 是正则表达式，而且正则表达式中包含了 captures，这时 nginx 不得不执行该正则表达式以获取 captures。（正则表达式的 capture 是指被圆括号引用的表达式部分，它们所匹配的字符串，可通过名字或数字引用） 兼容性 从 0.9.4 开始支持特殊主机名 “$hostname” 从 0.8.48 开始，如果 server 区块中未定义 server_name 指令，nginx 默认设定空字符串为主机名，如同定义了 server_name “” 从 0.8.25 开始支持在“正则表达式主机名”中使用 named capture 特性 从 0.7.40 开始支持在“正则表达式主机名”中使用 capture 特性 从 0.7.12 开始支持 “” 空字符串主机名 从 0.6.25 开始，支持使用“正则表达式主机名”或者“通配主机名”作为第一个主机名。 从 0.6.7 开始支持“正则表达式主机名” 从 0.6.0 开始支持形如 example.* 的“通配主机名” 从 0.3.18 开始支持形如 .example.org 的特殊“通配主机名” 从 0.1.13 开始支持形如 *.example.org 的“通配主机名” 参考https://www.jianshu.com/p/1430e4046fd9","tags":[{"name":"nginx","slug":"nginx","permalink":"http://wumuwumu.github.io/tags/nginx/"}]},{"title":"odoo的dbfilter配置","date":"2019-12-16T09:37:50.000Z","path":"2019/12/16/odoo/odoo的dbfilter配置/","text":"关于 Odoo 的 dbfilter 配置项概述默认情况下首次访问odoo页面时，会要求选择要访问的数据库，db中的所有库都会被列出来供选择，这种在生产环境下通常是不希望的看到，如果在启动时指定连接的数据库名可以解决这个问题 .conf文件中指定 db_name = xxx 或者启动命令加参数-d xxx dbfilter当我们需要根据域名来匹配数据库时（比如saas环境）这样就不适用了，这个时候就可以用 dbfilter 这个配置项来实现 dbfilter 默认值为 .* eg: dbfilter = ^%h$ 表示按域名精确匹配数据库服务器中名称为域名的数据库 启动参数 --db-filter=&#39;^%d$&#39; 表示按二级域名前缀精确匹配对应名称的数据库（注意：127.0.0.1访问时会被匹配为 127 库名） 可用的匹配替代符号有 %h 和 %d %h%h 代表访问访问的域名，比如 www.abc.com %d当访问地址为 www.abc.com 时 %d 为 abc当访问地址为 shop.abc.com 时 %d 为 shop 相关源代码odoo中的相应的解析代码 123456789def db_filter(dbs, httprequest=None): httprequest = httprequest or request.httprequest h = httprequest.environ.get('HTTP_HOST', '').split(':')[0] d, _, r = h.partition('.') if d == \"www\" and r: d = r.partition('.')[0] r = openerp.tools.config['dbfilter'].replace('%h', h).replace('%d', d) dbs = [i for i in dbs if re.match(r, i)] return dbs","tags":[{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"postgresql配置文件","date":"2019-12-09T08:29:39.000Z","path":"2019/12/09/postgresql/postgresql配置文件/","text":"1、配置文件配置文件控制着一个PostgreSQL服务器实例的基本行为，主要包含postgresql.conf、pg_hba.conf、pg_ident.conf （1）postgresql.conf 该文件包含一些通用设置，比如内存分配，新建database的默认存储位置，PostgreSQL服务器的IP地址，日志的位置以及许多其他设置。9.4版引入了 一个新的postgresql.auto.conf文件，任何时候执行Altersystem SQL命令，都会创建或重写该文件。该文件中的设置会替代postgresql.conf文件中的设置。 （2）pg_hba.conf ​ 该文件用于控制访问安全性，管理客户端对Postgresql服务器的访问权限，内容包括：允许哪些用户连接到哪个数据库，允许哪些IP或者哪个网段的IP连 ​ 接到本服务器，以及指定连接时使用的身份验证模式 （3）pg_ident.conf pg_hba.conf的权限控制信息中的身份验证模式字段如果指定为ident方式，则用户连接时系统会尝试访问pg_ident文件，如果该文件存在，则系统会基于 ​ 文件内容将当前执行登录操作的操作系统用户映射为一个PostgreSQL数据库内部用户的身份来登录。 2、查看配置文件的位置：12345678postgres=# selectname,setting from pg_settings where category='File Locations'; name | setting -------------------+----------------------------------------- config_file |/var/lib/pgsql/9.6/data/postgresql.conf data_directory | /var/lib/pgsql/9.6/data external_pid_file | hba_file | /var/lib/pgsql/9.6/data/pg_hba.conf ident_file | /var/lib/pgsql/9.6/data/pg_ident.conf 3、postgresql.conf3.1、关键的设置 12345678910postgres=# selectname,context,unit,setting,boot_val,reset_val from pg_settings where namein('listen_addresses','max_connections','shared_buffers','effective_cache_size','work_mem','maintenance_work_mem')order by context,name; name | context | unit | setting |boot_val | reset_val ----------------------+------------+------+---------+-----------+----------- listen_addresses | postmaster | | * | localhost | * max_connections | postmaster | | 100 | 100 | 100 shared_buffers | postmaster | 8kB | 16384 | 1024 | 16384 effective_cache_size | user | 8kB | 524288 | 524288 | 524288 maintenance_work_mem | user | kB | 65536 | 65536 | 65536 work_mem | user | kB | 4096 | 4096 | 4096(6 rows) context 设置为postmaster，更改此形参后需要重启PostgreSQL服务才能生效； 设置为user，那么只需要执行一次重新加载即可全局生效。重启数据库服务会终止活动连接，但重新加载不会。 unit 字段表示这些设置的单位 setting是指当前设置；boot_val是指默认设置；reset_val是指重新启动服务器或重新加载设置之后的新设置 在postgresql.conf中修改了设置后，一定记得查看一下setting和reset_val并确保二者是一致，否则说明设置并未生效，需要重新启动服务器或者重新加载设置 3.2、postgresql.auto.conf与postgresql.conf区别 对于9.4版及之后的版本来说，Postgresql.auto.conf的优先级是高于postgresql.conf的，如果这两个文件中存在同名配置项，则系统会优先选择前者设定的值。 3.3、postgresql.conf以下网络设置，修改这些值是一定要重新启动数据库服务的 listen_addresses 一般设定为localhost或者local，但也有很多人会设为*，表示使用本机任一IP地址均可连接到Postgresql服务 port 默认值 为5432 max_connections 3.4、以下四个设置对系统性能有着全局性的影响，建议你在实际环境下通过实测来找到最优值 (1)share_buffers ​ 用于缓存最近访问过的数据页的内存区大小，所有用户会话均可共享此缓存区 ​ 一般来说越大越好，至少应该达到系统总内存的25%，但不宜超过8GB，因为超过后会出现“边际收益递减”效应。 ​ 需重启postgreSQL服务 （2）effective_cache_size 一个查询执行过程中可以使用的最大缓存，包括操作系统使用的部分以及PostgreSQL使用部分，系统并不会根据这个值来真实地分配这么多内存，但是规划器会根据这个值来判断系统能否提供查询执行过程中所需的内存。如果将此设置设得过小，远远小于系统真实可用内存量，那么可能会给规划器造成误导，让规划器认为系统可用内存有限，从而选择不使用索引而是走全表扫描（因为使用索引虽然速度快，但需要占用更多的中间内存）。 在一台专用于运行PostgreSQL数据库服务的服务器上，建议将effective_cache_size的值设为系统总内存的一半或者更多。 此设置可动态生效，执行重新加载即可。 （3）work_mem 此设置指定了用于执行排序，哈希关联，表扫描等操作的最大内存量。 此设置可动态生效，执行重新加载即可。 （4）mintenance_work_mem ​ 此设置指定可用于vaccum操作（即清空已标记为“被删除”状态的记录）这类系统内部维护操作的内存总量。 ​ 其值不应大于1GB 此设置可动态生效，执行重新加载即可。 3.5修改参数命令 1Alter system set work_mem=8192; 设置重新加载命令 1Select pg_reload_conf(); 3.6、遇到修改了postgresql.conf文件，结果服务器崩溃了这种情况 定位这种问题最简单的方法是查看日志文件，该文件位于postgresql数据文件夹的根目录或者pg_log子文件夹下。 4、pg_hba.confcat /var/lib/pgsql/9.6/data/pg_hba.conf 12345678910111213# TYPE DATABASE USER ADDRESS METHOD # \"local\" isfor Unix domain socket connections onlylocal all all peer# IPv4 localconnections:host all all 0.0.0.0/0 trust# IPv6 localconnections:host all all ::1/128 ident# Allow replicationconnections from localhost, by a user with the# replication privilege.#local replication postgres peer#host replication postgres 127.0.0.1/32 ident#host replication postgres ::1/128 ident (1) 身份验证模式，一般以下几种常用选项：ident、trust、md5以及password 1版本开始引入了peer身份验证模式。 Ident和peer模式公适用于Linux，Unix和Mac,不适用于windwos Reject模式，其作用是拒绝所有请求。 (2) 如果你将+0.0.0./0 reject+规则放到+127.0.0.1/32 trust+的前面，那么此时本地用户全都无法连接，即使下面有规则允许也不行。 （3）各模式 trust最不安全的身份验证模式，该模式允许用户“自证清白”，即可以不用密码就连到数据库 md5该模式最常用，要求连接发起者携带用md5算法加密的密码 password 不推荐，因为该模式使用明文密码进行身份验证，不安全 ident：该身份验证模式下，系统会将请求发起的操作系统用户映射为PostgreSQL数据库内部用户，并以该内部用户的权限登录，且此时无需提供登录密码。操作系统用户与数据库内部用户之间的映射关系会记录在pg_ident.conf文件中。 peer使用发起端的操作系统名进行身份验证 5、配置文件的重新加载123/usr/pgsql-9.6/bin/pg_ctlreload -D /var/lib/pgsql/9.6/data/ systemctlreload postgresql-9.6.service selectpg_reload_conf();","tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://wumuwumu.github.io/tags/postgresql/"}]},{"title":"postgresql安装","date":"2019-12-09T07:53:51.000Z","path":"2019/12/09/postgresql/postgresql安装/","text":"12345678910111213# Install the repository RPM:dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm# Disable the built-in PostgreSQL module:dnf -qy module disable postgresql# Install PostgreSQL:dnf install -y postgresql12-server# Optionally initialize the database and enable automatic start:/usr/pgsql-12/bin/postgresql-12-setup initdbsystemctl enable postgresql-12systemctl start postgresql-12","tags":[]},{"title":"mysql连接外网安装","date":"2019-12-09T07:24:23.000Z","path":"2019/12/09/mysql/mysql连接外网安装/","text":"添加 MySQL YUM 源根据自己的操作系统选择合适的安装源，和其他公司一样，总会让大家注册账号获取更新，注意是 Oracle 的账号，如果不想注册，下方有直接下载的地址，下载之后通过 rpm -Uvh 安装。 123456$wget 'https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm'$sudo rpm -Uvh mysql57-community-release-el7-11.noarch.rpm$yum repolist all | grep mysqlmysql-connectors-community/x86_64 MySQL Connectors Community 36mysql-tools-community/x86_64 MySQL Tools Community 47mysql57-community/x86_64 MySQL 5.7 Community Server 187 先解释下为什么下载的是 5.7 版本的，现在最新的是 5.7 版本的，当然官网默认都是最新版本的，但是下载的页面也有说明 The MySQL Yum repository includes the latest versions of: MySQL 8.0 (Development) MySQL 5.7 (GA) MySQL 5.6 (GA) MySQL 5.5 (GA - Red Hat Enterprise Linux and Oracle Linux Only) MySQL Cluster 7.5 (GA) MySQL Cluster 7.6 (Development) MySQL Workbench MySQL Fabric MySQL Router (GA) MySQL Utilities MySQL Connector / ODBC MySQL Connector / Python MySQL Shell (GA) 也就是说这个安装源包含了上面列举的这些版本，当然包括 5.6 版本的。 选择安装版本如果想安装最新版本的，直接使用 yum 命令即可 1$sudo yum install mysql-community-server 如果想要安装 5.6 版本的，有2个方法。命令行支持 yum-config-manager 命令的话，可以使用如下命令： 123456$ sudo dnf config-manager --disable mysql57-community$ sudo dnf config-manager --enable mysql56-community$ yum repolist | grep mysqlmysql-connectors-community/x86_64 MySQL Connectors Community 36mysql-tools-community/x86_64 MySQL Tools Community 47mysql56-community/x86_64 MySQL 5.6 Community Server 327 或者直接修改 /etc/yum.repos.d/mysql-community.repo 这个文件 1234567891011121314# Enable to use MySQL 5.6[mysql56-community]name=MySQL 5.6 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.6-community/el/7/$basearch/enabled=1 #表示当前版本是安装gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql57-community]name=MySQL 5.7 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/7/$basearch/enabled=0 #默认这个是 1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql 通过设置 enabled 来决定安装哪个版本。 设置好之后使用 yum 安装即可。 启动 MySQL 服务启动命令很简单 123456789101112$sudo service mysqld start $sudo systemctl start mysqld #CentOS 7$sudo systemctl status mysqld● mysqld.service - MySQL Community Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2017-05-27 12:56:26 CST; 15s ago Process: 2482 ExecStartPost=/usr/bin/mysql-systemd-start post (code=exited, status=0/SUCCESS) Process: 2421 ExecStartPre=/usr/bin/mysql-systemd-start pre (code=exited, status=0/SUCCESS) Main PID: 2481 (mysqld_safe) CGroup: /system.slice/mysqld.service ├─2481 /bin/sh /usr/bin/mysqld_safe --basedir=/usr └─2647 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/... 说明已经正在运行中了。 对于 MySQL 5.7 版本，启动的时候如果数据为空的，则会出现如下提示 The server is initialized. An SSL certificate and key files are generated in the data directory. The validate_password plugin is installed and enabled. A superuser account ‘root‘@’localhost’ is created. A password for the superuser is set and stored in the error log file.To reveal it, use the following command: sudo grep &#39;temporary password&#39; /var/log/mysqld.log 简单的说就是服务安装好了，SSL 认证的文件会在 data 目录中生存，密码不要设置的太简单了，初始密码通过下面的命令查看，赶紧去改密码吧。 安装提示，查看密码，登录数据库，然后修改密码： 12$ mysql -uroot -p #输入查看到的密码mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass4!'; MySQL 5.6 的安全设置由于 5.7 版本在安装的时候就设置好了，不需要额外设置，但是 5.6 版本建议从安全角度完善下，运行官方脚本即可 1$ mysql_secure_installation 会提示设置5个关键位置 设置 root 密码 禁止 root 账号远程登录 禁止匿名账号（anonymous）登录 删除测试库 是否确认修改 安装第三方组件查看 yum 源中有哪些默认的组件： 1$ yum --disablerepo=\\* --enablerepo='mysql*-community*' list available 需要安装直接通过 yum 命令安装即可。 修改编码在 /etc/my.cnf 中设置默认的编码 123456789[client]default-character-set = utf8[mysqld]default-storage-engine = INNODBcharacter-set-server = utf8collation-server = utf8_general_ci #不区分大小写collation-server = utf8_bin #区分大小写collation-server = utf8_unicode_ci #比 utf8_general_ci 更准确 创建数据库和用户创建数据库 123456CREATE DATABASE &lt;datebasename&gt; CHARACTER SET utf8;CREATE USER 'username'@'host' IDENTIFIED BY 'password';GRANT privileges ON databasename.tablename TO 'username'@'host';SHOW GRANTS FOR 'username'@'host';REVOKE privilege ON databasename.tablename FROM 'username'@'host';DROP USER 'username'@'host'; 其中 username：你将创建的用户名 host：指定该用户在哪个主机上可以登陆，如果是本地用户可用 localhost，如果想让该用户可以从任意远程主机登陆，可以使用通配符 % password：该用户的登陆密码，密码可以为空，如果为空则该用户可以不需要密码登陆服务器 privileges：用户的操作权限，如 SELECT，INSERT，UPDATE 等，如果要授予所的权限则使用ALL databasename：数据库名 tablename：表名，如果要授予该用户对所有数据库和表的相应操作权限则可用 表示，如 .* 参考https://www.jianshu.com/p/7cccdaa2d177","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"redux-saga分成多个文件","date":"2019-11-26T07:26:49.000Z","path":"2019/11/26/react/redux-saga分成多个文件/","text":"1)12345678// single entry point to start all Sagas at onceexport default function* rootSaga() &#123; yield [ saga1(), saga2(), saga3(), ]&#125; Here the 3 sagas will be run in parallel. The root saga will block until the 3 sagas complete. If one of the 3 fail, the error will be propagated to the root saga which will be killed, which will also kill the other 2 saga 2)1234567export default function* root() &#123; yield [ fork(saga1), fork(saga2), fork(saga3) ]&#125; The only difference I see here is that this time the yield effect will not block because forking is non-blocking, thus the root saga will reach the end but the 3 childs will remain alive. Error behavior is the same as 1) 3)12345export default function* root() &#123; yield fork(saga1) yield fork(saga2) yield fork(saga3)&#125; I don’t see any difference in behavior from 2) better examplesThe problem with forking is that if any of the root saga fails, then the root saga will be killed, and the other sub sagas will also be killed because their parent got killed. In practice this means that your whole app may become unusable (if it relies heavily on sagas) just because of a minor saga error so it’s not really good. 4)12345export default function* root() &#123; yield spawn(saga1) yield spawn(saga2) yield spawn(saga3)&#125; This time, if an error occur in saga1, it will not make root, saga2 and saga3 get killed so only a part of your app stops working in case of error. Somehow this can also be very problematic because the saga1 might be killed due to an error like a failing http request that you didn’t catch properly, making the whole feature covered by saga1 unavailable for the app lifetime. 5)@granmoe has suggested the following way to start sagas in: #570 123456789101112131415161718192021function* rootSaga () &#123; const sagas = [ saga1, saga2, saga3, ]; yield sagas.map(saga =&gt; spawn(function* () &#123; while (true) &#123; try &#123; yield call(saga) &#125; catch (e) &#123; console.log(e) &#125; &#125; &#125;) )&#125; This time, if any of the 3 sagas had an error, it would be automatically restarted. This may, or not, be the desired behavior according to your app. 6)Here’s how I handle sagas in my own app: 12345678910111213141516171819202122232425const makeRestartable = (saga) =&gt; &#123; return function* () &#123; yield spawn(function* () &#123; while (true) &#123; try &#123; yield call(saga); console.error(&quot;unexpected root saga termination. The root sagas are supposed to be sagas that live during the whole app lifetime!&quot;,saga); &#125; catch (e) &#123; console.error(&quot;Saga error, the saga will be restarted&quot;,e); &#125; yield delay(1000); // Avoid infinite failures blocking app TODO use backoff retry policy... &#125; &#125;) &#125;;&#125;;const rootSagas = [ domain1saga, domain2saga, domain3saga,].map(makeRestartable);export default function* root() &#123; yield rootSagas.map(saga =&gt; call(saga));&#125; I’m using a saga HOC to add error handling to the root sagas. In my app, all root sagas are never supposed to terminate but should block, and if there are errors they should be automatically restarted. Restarting synchronously can, in my experience, lead to infinite loops (if the saga fails everytime you try to restart it) so I added a hacky delay for now to prevent this issue. You mentionned different domains in your app so this pattern seems appropriate to your usecase where each domain should somehow have its own root saga.","tags":[{"name":"react","slug":"react","permalink":"http://wumuwumu.github.io/tags/react/"}]},{"title":"odoo源码解析4-wsgi_server","date":"2019-11-16T07:35:05.000Z","path":"2019/11/16/odoo/odoo源码解析4-wsgi-server/","text":"application12345678910def application(environ, start_response): ## 是否启动代理 # FIXME: is checking for the presence of HTTP_X_FORWARDED_HOST really useful? # we're ignoring the user configuration, and that means we won't # support the standardised Forwarded header once werkzeug supports # it if config['proxy_mode'] and 'HTTP_X_FORWARDED_HOST' in environ: return ProxyFix(application_unproxied)(environ, start_response) else: return application_unproxied(environ, start_response) application_unproxied清除数据库和用户的追踪清除动作在application方法的结尾不能完成，因为werkzeu在后面还会生成有关的日志。 123456789101112131415161718192021def application_unproxied(environ, start_response): \"\"\" WSGI entry point.\"\"\" # cleanup db/uid trackers - they're set at HTTP dispatch in # web.session.OpenERPSession.send() and at RPC dispatch in # odoo.service.web_services.objects_proxy.dispatch(). # /!\\ The cleanup cannot be done at the end of this `application` # method because werkzeug still produces relevant logging afterwards if hasattr(threading.current_thread(), 'uid'): del threading.current_thread().uid if hasattr(threading.current_thread(), 'dbname'): del threading.current_thread().dbname if hasattr(threading.current_thread(), 'url'): del threading.current_thread().url with odoo.api.Environment.manage(): result = odoo.http.root(environ, start_response) if result is not None: return result # We never returned from the loop. return werkzeug.exceptions.NotFound(\"No handler found.\\n\")(environ, start_response) 参考 https://blog.csdn.net/weixin_35737303/article/details/79038982","tags":[{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"odoo源码解析-启动web服务器","date":"2019-11-16T06:41:42.000Z","path":"2019/11/16/odoo/odoo源码解析3-启动web服务器/","text":"启动12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def start(preload=None, stop=False): \"\"\" Start the odoo http server and cron processor. \"\"\" global server ## 这里加载两个模块web和web_kan，在这里加载模块才能够在用户没有登录的时候才能够访问路由 load_server_wide_modules() odoo.service.wsgi_server._patch_xmlrpc_marshaller() \"\"\" ·GeventServer ·PreforkServer ·ThreadedServer(默认) CommonServer是后面三个类的父类 Odoo服务器通过ThreadedServer.run()运行 \"\"\" if odoo.evented: server = GeventServer(odoo.service.wsgi_server.application) elif config['workers']: if config['test_enable'] or config['test_file']: _logger.warning(\"Unit testing in workers mode could fail; use --workers 0.\") server = PreforkServer(odoo.service.wsgi_server.application) # Workaround for Python issue24291, fixed in 3.6 (see Python issue26721) if sys.version_info[:2] == (3,5): # turn on buffering also for wfile, to avoid partial writes (Default buffer = 8k) werkzeug.serving.WSGIRequestHandler.wbufsize = -1 else: server = ThreadedServer(odoo.service.wsgi_server.application) watcher = None if 'reload' in config['dev_mode'] and not odoo.evented: if inotify: watcher = FSWatcherInotify() watcher.start() elif watchdog: watcher = FSWatcherWatchdog() watcher.start() else: if os.name == 'posix' and platform.system() != 'Darwin': module = 'inotify' else: module = 'watchdog' _logger.warning(\"'%s' module not installed. Code autoreload feature is disabled\", module) if 'werkzeug' in config['dev_mode']: server.app = DebuggedApplication(server.app, evalex=True) ## 启动web服务器 rc = server.run(preload, stop) if watcher: watcher.stop() # like the legend of the phoenix, all ends with beginnings if getattr(odoo, 'phoenix', False): _reexec() return rc if rc else 0 ThreadedServer(CommandServer)Run1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950\"\"\" Start the http server and the cron thread then wait for a signal. The first SIGINT or SIGTERM signal will initiate a graceful shutdown while a second one if any will force an immediate exit. \"\"\"## 启动一个系统命令监测。。。self.start(stop=stop)## 安装、更新、加载模块rc = preload_registries(preload)if stop: self.stop() return rc## 加载定时任务self.cron_spawn()# Wait for a first signal to be handled. (time.sleep will be interrupted# by the signal handler)try: while self.quit_signals_received == 0: self.process_limit() if self.limit_reached_time: has_other_valid_requests = any( not t.daemon and t not in self.limits_reached_threads for t in threading.enumerate() if getattr(t, 'type', None) == 'http') if (not has_other_valid_requests or (time.time() - self.limit_reached_time) &gt; SLEEP_INTERVAL): # We wait there is no processing requests # other than the ones exceeding the limits, up to 1 min, # before asking for a reload. _logger.info('Dumping stacktrace of limit exceeding threads before reloading') dumpstacks(thread_idents=[thread.ident for thread in self.limits_reached_threads]) self.reload() # `reload` increments `self.quit_signals_received` # and the loop will end after this iteration, # therefore leading to the server stop. # `reload` also sets the `phoenix` flag # to tell the server to restart the server after shutting down. else: time.sleep(1) else: time.sleep(SLEEP_INTERVAL) except KeyboardInterrupt: pass self.stop() start12345678910111213141516171819def start(self, stop=False): _logger.debug(\"Setting signal handlers\") set_limit_memory_hard() if os.name == 'posix': signal.signal(signal.SIGINT, self.signal_handler) signal.signal(signal.SIGTERM, self.signal_handler) signal.signal(signal.SIGCHLD, self.signal_handler) signal.signal(signal.SIGHUP, self.signal_handler) signal.signal(signal.SIGXCPU, self.signal_handler) signal.signal(signal.SIGQUIT, dumpstacks) signal.signal(signal.SIGUSR1, log_ormcache_stats) elif os.name == 'nt': import win32api win32api.SetConsoleCtrlHandler(lambda sig: self.signal_handler(sig, None), 1) test_mode = config['test_enable'] or config['test_file'] if test_mode or (config['http_enable'] and not stop): # some tests need the http deamon to be available... self.http_spawn() ThreadedWSGIServerReloadable这个服务可以不启动也能够运行程序。他的作用是debug保持端口是开启的。 Werkzeug是Python的WSGI规范的实现函数库。基于BSD协议。WSGI(Web Server Gateway Interface)WSGI服务允许重用环境提供的监听套接字，它通过自动重加载使用，用于保持当有重加载的时候监听套接字是打开状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class ThreadedWSGIServerReloadable(LoggingBaseWSGIServerMixIn, werkzeug.serving.ThreadedWSGIServer): \"\"\" werkzeug Threaded WSGI Server patched to allow reusing a listen socket given by the environement, this is used by autoreload to keep the listen socket open when a reload happens. \"\"\" def __init__(self, host, port, app): super(ThreadedWSGIServerReloadable, self).__init__(host, port, app, handler=RequestHandler) # See https://github.com/pallets/werkzeug/pull/770 # This allow the request threads to not be set as daemon # so the server waits for them when shutting down gracefully. self.daemon_threads = False def server_bind(self): SD_LISTEN_FDS_START = 3 if os.environ.get('LISTEN_FDS') == '1' and os.environ.get('LISTEN_PID') == str(os.getpid()): self.reload_socket = True self.socket = socket.fromfd(SD_LISTEN_FDS_START, socket.AF_INET, socket.SOCK_STREAM) _logger.info('HTTP service (werkzeug) running through socket activation') else: self.reload_socket = False super(ThreadedWSGIServerReloadable, self).server_bind() _logger.info('HTTP service (werkzeug) running on %s:%s', self.server_name, self.server_port) def server_activate(self): if not self.reload_socket: super(ThreadedWSGIServerReloadable, self).server_activate() def process_request(self, request, client_address): \"\"\" Start a new thread to process the request. Override the default method of class socketserver.ThreadingMixIn to be able to get the thread object which is instantiated and set its start time as an attribute \"\"\" t = threading.Thread(target = self.process_request_thread, args = (request, client_address)) t.daemon = self.daemon_threads t.type = 'http' t.start_time = time.time() t.start() # TODO: Remove this method as soon as either of the revision # - python/cpython@8b1f52b5a93403acd7d112cd1c1bc716b31a418a for Python 3.6, # - python/cpython@908082451382b8b3ba09ebba638db660edbf5d8e for Python 3.7, # is included in all Python 3 releases installed on all operating systems supported by Odoo. # These revisions are included in Python from releases 3.6.8 and Python 3.7.2 respectively. def _handle_request_noblock(self): \"\"\" In the python module `socketserver` `process_request` loop, the __shutdown_request flag is not checked between select and accept. Thus when we set it to `True` thanks to the call `httpd.shutdown`, a last request is accepted before exiting the loop. We override this function to add an additional check before the accept(). \"\"\" if self._BaseServer__shutdown_request: return super(ThreadedWSGIServerReloadable, self)._handle_request_noblock() 参考 https://blog.csdn.net/weixin_35737303/article/details/79038879","tags":[{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"hibernate使用hbm2ddl.auto=在生产环境更新.md","date":"2019-11-16T06:22:56.000Z","path":"2019/11/16/java/hibernate使用hbm2ddl.auto=在生产环境更新/","text":"https://www.codenong.com/221379/","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"odoo源码解析2-server命令","date":"2019-11-16T03:40:07.000Z","path":"2019/11/16/odoo/odoo源码解析2-server命令/","text":"默认情况下的启动命令的server，这个是将odoo运行起来的命令。核心代码如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556## 判断是否为root用户，如果为root用户就发送警告check_root_user() ## 解析命令行参数odoo.tools.config.parse_config(args)## 如果为postgres用户就停止运行check_postgres_user()report_configuration()config = odoo.tools.config# the default limit for CSV fields in the module is 128KiB, which is not# quite sufficient to import images to store in attachment. 500MiB is a# bit overkill, but better safe than sorry I guesscsv.field_size_limit(500 * 1024 * 1024)## 创建加载的数据库preload = []if config['db_name']: preload = config['db_name'].split(',') for db_name in preload: try: odoo.service.db._create_empty_database(db_name) config['init']['base'] = True except ProgrammingError as err: if err.pgcode == errorcodes.INSUFFICIENT_PRIVILEGE: # We use an INFO loglevel on purpose in order to avoid # reporting unnecessary warnings on build environment # using restricted database access. _logger.info(\"Could not determine if database %s exists, \" \"skipping auto-creation: %s\", db_name, err) else: raise err except odoo.service.db.DatabaseExists: passif config[\"translate_out\"]: export_translation() sys.exit(0)if config[\"translate_in\"]: import_translation() sys.exit(0)# This needs to be done now to ensure the use of the multiprocessing# signaling mecanism for registries loaded with -dif config['workers']: odoo.multi_process = True## 是否在启动服务后停止，用户创建更新数据库stop = config[\"stop_after_init\"]## 设置pid文件setup_pid_file()## 启动serverrc = odoo.service.server.start(preload=preload, stop=stop)sys.exit(rc) 参考 https://blog.csdn.net/weixin_35737303/article/details/79038671","tags":[{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"odoo模块加载机制","date":"2019-11-08T12:36:34.000Z","path":"2019/11/08/odoo/odoo模块加载机制/","text":"Odoo的启动通过openerp-server脚本完成，它是系统的入口。 然后加载配置文件openerp-server.conf 或者 openerp_serverrc； openerp-server.conf的主要内容： 这个文件缺省是没有的，Odoo系统会有一个默认值，但是一般情况我们都需配置这个文件。 启动http服务器，监听端口。 模块加载： 模块加载外层就是封装一个Registry(Mapping)对象:实际是一个字典，它包含对应的db，model等映射关系，一个DB对应一个Registry。后续的操作都会围绕这个Registry进行，将相关的数据赋值给相应的属性项。 初始化数据库（初次运行)1)加载base模块下的base.sql文件并执行。此时数据库表为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174CREATE TABLE ir_actions ( id serial, primary key(id));CREATE TABLE ir_act_window (primary key(id)) INHERITS (ir_actions);CREATE TABLE ir_act_report_xml (primary key(id)) INHERITS (ir_actions);CREATE TABLE ir_act_url (primary key(id)) INHERITS (ir_actions);CREATE TABLE ir_act_server (primary key(id)) INHERITS (ir_actions);CREATE TABLE ir_act_client (primary key(id)) INHERITS (ir_actions);CREATE TABLE ir_model ( id serial, model varchar NOT NULL, name varchar, state varchar, info text, primary key(id));CREATE TABLE ir_model_fields ( id serial, model varchar NOT NULL, model_id integer references ir_model on delete cascade, name varchar NOT NULL, relation varchar, select_level varchar, field_description varchar, ttype varchar, state varchar default 'base', relation_field varchar, translate boolean default False, serialization_field_id integer references ir_model_fields on delete cascade, primary key(id));CREATE TABLE res_lang ( id serial, name VARCHAR(64) NOT NULL UNIQUE, code VARCHAR(16) NOT NULL UNIQUE, primary key(id));CREATE TABLE res_users ( id serial NOT NULL, active boolean default True, login varchar(64) NOT NULL UNIQUE, password varchar(64) default null, -- No FK references below, will be added later by ORM -- (when the destination rows exist) company_id integer, -- references res_company, partner_id integer, -- references res_partner, primary key(id));create table wkf ( id serial, name varchar(64), osv varchar(64), on_create bool default false, primary key(id));CREATE TABLE ir_module_category ( id serial NOT NULL, create_uid integer, -- references res_users on delete set null, create_date timestamp without time zone, write_date timestamp without time zone, write_uid integer, -- references res_users on delete set null, parent_id integer REFERENCES ir_module_category ON DELETE SET NULL, name character varying(128) NOT NULL, primary key(id));CREATE TABLE ir_module_module ( id serial NOT NULL, create_uid integer, -- references res_users on delete set null, create_date timestamp without time zone, write_date timestamp without time zone, write_uid integer, -- references res_users on delete set null, website character varying(256), summary character varying(256), name character varying(128) NOT NULL, author character varying(128), icon varchar, state character varying(16), latest_version character varying(64), shortdesc character varying(256), category_id integer REFERENCES ir_module_category ON DELETE SET NULL, description text, application boolean default False, demo boolean default False, web boolean DEFAULT FALSE, license character varying(32), sequence integer DEFAULT 100, auto_install boolean default False, primary key(id));ALTER TABLE ir_module_module add constraint name_uniq unique (name);CREATE TABLE ir_module_module_dependency ( id serial NOT NULL, create_uid integer, -- references res_users on delete set null, create_date timestamp without time zone, write_date timestamp without time zone, write_uid integer, -- references res_users on delete set null, name character varying(128), module_id integer REFERENCES ir_module_module ON DELETE cascade, primary key(id));CREATE TABLE ir_model_data ( id serial NOT NULL, create_uid integer, create_date timestamp without time zone, write_date timestamp without time zone, write_uid integer, noupdate boolean, name varchar NOT NULL, date_init timestamp without time zone, date_update timestamp without time zone, module varchar NOT NULL, model varchar NOT NULL, res_id integer, primary key(id));-- Records foreign keys and constraints installed by a module (so they can be-- removed when the module is uninstalled):-- - for a foreign key: type is 'f',-- - for a constraint: type is 'u' (this is the convention PostgreSQL uses).CREATE TABLE ir_model_constraint ( id serial NOT NULL, date_init timestamp without time zone, date_update timestamp without time zone, module integer NOT NULL references ir_module_module on delete restrict, model integer NOT NULL references ir_model on delete restrict, type character varying(1) NOT NULL, name varchar NOT NULL, primary key(id));-- Records relation tables (i.e. implementing many2many) installed by a module-- (so they can be removed when the module is uninstalled).CREATE TABLE ir_model_relation ( id serial NOT NULL, date_init timestamp without time zone, date_update timestamp without time zone, module integer NOT NULL references ir_module_module on delete restrict, model integer NOT NULL references ir_model on delete restrict, name varchar NOT NULL, primary key(id)); CREATE TABLE res_currency ( id serial, name varchar NOT NULL, primary key(id));CREATE TABLE res_company ( id serial, name varchar NOT NULL, partner_id integer, currency_id integer, primary key(id));CREATE TABLE res_partner ( id serial, name varchar, company_id integer, primary key(id)); 这20张表是odoo系统级的，它是模块加载及系统运行的基础。后续模块生成的表及相关数据都可以在这20张中找到蛛丝马迹。 数据库表初始化后，就可以加载模块数据（addons）到数据库了，这个也是odoo作为平台灵活的原因，所有的数据都在数据库。找到addons-path下所有的模块,然后一个一个的加载到数据库中。Info就是load模块的openerp.py文件，它是一个dict。 根据openerp.py中定义的category创建分类信息：将模块信息写入ir_module_module表：将module信息写入ir_model_data表：一个module要写两次ir_model_data表，写module的dependency表： 根据依赖关系进行判断，递归更新那些需要auto_install的模块状态为“to install”。 到目前为止，模块的加载都是在数据库级别，只是将“模块文件”信息存入数据库表，但是还没有真正加载到程序中。Odoo运行时查找object是通过Registry.get()获取的，而不是通过python自己的机制来找到相应的object，所以odoo在加载模块时会把模块下包含的model全部注册到models.py的module_to_models字典中。 **下面的步骤就是加载模块到内存： 加载base模块创建一个包含model层级的节点图，第二行代码将从数据库更新数据到graph中。然后调用load_module_graph方法加载模块，最终执行加载的方法： 这个方法是odoo加载model的核心，通过 import方法加载模块，这个是python的机制，当import到某个继承了BaseModel类的class时，它的实例化将有别于python自身的实例化操作，后者说它根本不会通过python自身的new方法创建实例，所有的实例创建都是通过 _build_model 方法及元类创建，并注册到module_to_models中。通过这种方式实例化model就可以解决我们在xml中配置model时指定的继承，字段，约束等各种属性。 标记需要加载或者更新的模块（db）加载被标记的模块（加载过程与加载base模块一致）完成及清理安装清理菜单删除卸载的模块核实model的view运行post-install测试","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"},{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"odoo源码解析1-启动命令","date":"2019-11-06T12:01:48.000Z","path":"2019/11/06/odoo/odoo源码解析1-启动命令/","text":"启动命令12345678#!/usr/bin/env python3# set server timezone in UTC before time module imported__import__('os').environ['TZ'] = 'UTC'import odooif __name__ == \"__main__\": odoo.cli.main() main函数主要是进行一些初始化和启动相关的命令 解析启动命令的参数 1234567891011121314151617181920212223242526272829def main(): args = sys.argv[1:] # The only shared option is '--addons-path=' needed to discover additional # commands from modules if len(args) &gt; 1 and args[0].startswith('--addons-path=') and not args[1].startswith(\"-\"): # parse only the addons-path, do not setup the logger... odoo.tools.config._parse_config([args[0]]) args = args[1:] # Default legacy command command = \"server\" # TODO: find a way to properly discover addons subcommands without importing the world # Subcommand discovery if len(args) and not args[0].startswith(\"-\"): logging.disable(logging.CRITICAL) for module in get_modules(): if isdir(joinpath(get_module_path(module), 'cli')): __import__('odoo.addons.' + module) logging.disable(logging.NOTSET) command = args[0] args = args[1:] if command in commands: o = commands[command]() o.run(args) else: sys.exit('Unknow command %r' % (command,))","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"},{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"python原类实践","date":"2019-11-03T02:24:27.000Z","path":"2019/11/03/python/python原类实践/","text":"使用原来创建ORM的实例我们通过创建一个类似Django中的ORM来熟悉一下元类的使用，通常元类用来创建API是非常好的选择，使用元类的编写很复杂但使用者可以非常简洁的调用API。 1234567#我们想创建一个类似Django的ORM，只要定义字段就可以实现对数据库表和字段的操作。class User(Model): # 定义类的属性到列的映射： id = IntegerField('id') name = StringField('username') email = StringField('email') password = StringField('password') 例如： 1234# 创建一个实例：u = User(id=12345, name='Michael', email='test@orm.org', password='my-pwd')# 保存到数据库：u.save() 接下来我么来实现这么个功能： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#coding:utf-8#一、首先来定义Field类，它负责保存数据库表的字段名和字段类型：class Field(object): def __init__(self, name, column_type): self.name = name self.column_type = column_type def __str__(self): return '&lt;%s:%s&gt;' % (self.__class__.__name__, self.name)class StringField(Field): def __init__(self, name): super(StringField, self).__init__(name, 'varchar(100)')class IntegerField(Field): def __init__(self, name): super(IntegerField, self).__init__(name, 'bigint')#二、定义元类，控制Model对象的创建class ModelMetaclass(type): '''定义元类''' def __new__(cls, name, bases, attrs): if name=='Model': return super(ModelMetaclass,cls).__new__(cls, name, bases, attrs) mappings = dict() for k, v in attrs.iteritems(): # 保存类属性和列的映射关系到mappings字典 if isinstance(v, Field): print('Found mapping: %s==&gt;%s' % (k, v)) mappings[k] = v for k in mappings.iterkeys(): #将类属性移除，使定义的类字段不污染User类属性，只在实例中可以访问这些key attrs.pop(k) attrs['__table__'] = name.lower() # 假设表名和为类名的小写,创建类时添加一个__table__类属性 attrs['__mappings__'] = mappings # 保存属性和列的映射关系，创建类时添加一个__mappings__类属性 return super(ModelMetaclass,cls).__new__(cls, name, bases, attrs)#三、编写Model基类class Model(dict): __metaclass__ = ModelMetaclass def __init__(self, **kw): super(Model, self).__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r\"'Model' object has no attribute '%s'\" % key) def __setattr__(self, key, value): self[key] = value def save(self): fields = [] params = [] args = [] for k, v in self.__mappings__.iteritems(): fields.append(v.name) params.append('?') args.append(getattr(self, k, None)) sql = 'insert into %s (%s) values (%s)' % (self.__table__, ','.join(fields), ','.join(params)) print('SQL: %s' % sql) print('ARGS: %s' % str(args))#最后，我们使用定义好的ORM接口，使用起来非常的简单。class User(Model): # 定义类的属性到列的映射： id = IntegerField('id') name = StringField('username') email = StringField('email') password = StringField('password')# 创建一个实例：u = User(id=12345, name='Michael', email='test@orm.org', password='my-pwd')# 保存到数据库：u.save()#输出# Found mapping: email==&gt;&lt;StringField:email&gt;# Found mapping: password==&gt;&lt;StringField:password&gt;# Found mapping: id==&gt;&lt;IntegerField:id&gt;# Found mapping: name==&gt;&lt;StringField:username&gt;# SQL: insert into User (password,email,username,id) values (?,?,?,?)# ARGS: ['my-pwd', 'test@orm.org', 'Michael', 12345] 使用new方法和元类方式分别实现单例模式1、new、init、call的介绍在讲到使用元类创建单例模式之前，比需了解new这个内置方法的作用，在上面讲元类的时候我们用到了new方法来实现类的创建。然而我在那之前还是对new这个方法和init方法有一定的疑惑。因此这里花点时间对其概念做一次了解和区分。 new方法负责创建一个实例对象，在对象被创建的时候调用该方法它是一个类方法。new方法在返回一个实例之后，会自动的调用init方法，对实例进行初始化。如果new方法不返回值，或者返回的不是实例，那么它就不会自动的去调用init方法。 init 方法负责将该实例对象进行初始化，在对象被创建之后调用该方法，在new方法创建出一个实例后对实例属性进行初始化。init方法可以没有返回值。 call方法其实和类的创建过程和实例化没有多大关系了，定义了call方法才能被使用函数的方式执行。 123456789例如：class A(object): def __call__(self): print \"__call__ be called\"a = A()a()#输出#__call__ be called 打个比方帮助理解：如果将创建实例的过程比作建一个房子。 那么class就是一个房屋的设计图，他规定了这个房子有几个房间，每个人房间的大小朝向等。这个设计图就是累的结构 new就是一个房屋的框架，每个具体的房屋都需要先搭好框架后才能进行专修，当然现有了房屋设计才能有具体的房屋框架出来。这个就是从类到类实例的创建。 init就是装修房子的过程，对房屋的墙面和地板等颜色材质的丰富就是它该做的事情，当然先有具体的房子框架出来才能进行装饰了。这个就是实例属性的初始化，它是在new出一个实例后才能初始化。 call就是房子的电话，有了固定电话，才能被打电话嘛（就是通过括号的方式像函数一样执行）。 123456789101112131415161718192021222324252627#coding:utf-8class Foo(object): def __new__(cls, *args, **kwargs): #__new__是一个类方法，在对象创建的时候调用 print \"excute __new__\" return super(Foo,cls).__new__(cls,*args,**kwargs) def __init__(self,value): #__init__是一个实例方法，在对象创建后调用，对实例属性做初始化 print \"excute __init\" self.value = valuef1 = Foo(1)print f1.valuef2 = Foo(2)print f2.value#输出===：excute __new__excute __init1excute __new__excute __init2#====可以看出new方法在init方法之前执行 子类如果重写new方法，一般依然要调用父类的new方法。 123class Child(Foo): def __new__(cls, *args, **kwargs): return suyper(Child, cls).__new__(cls, *args, **kwargs) 必须注意的是，类的new方法之后，必须生成本类的实例才能自动调用本类的init方法进行初始化，否则不会自动调用init. 12345678910111213141516class Foo(object): def __init__(self, *args, **kwargs): print \"Foo __init__\" def __new__(cls, *args, **kwargs): return object.__new__(Stranger, *args, **kwargs)class Stranger(object): def __init__(self,name): print \"class Stranger's __init__ be called\" self.name = namefoo = Foo(\"test\")print type(foo) #&lt;class '__main__.Stranger'&gt;print foo.name #AttributeError: 'Stranger' object has no attribute 'name'#说明：如果new方法返回的不是本类的实例，那么本类（Foo）的init和生成的类(Stranger)的init都不会被调用 2.实现单例模式依照Python官方文档的说法，new方法主要是当你继承一些不可变的class时(比如int, str, tuple)， 提供给你一个自定义这些类的实例化过程的途径。还有就是实现自定义的metaclass。接下来我们分别通过这两种方式来实现单例模式。当初在看到cookbook中的元类来实现单例模式的时候对其相当疑惑，因此才有了上面这些对元类的总结。 简单来说，单例模式的原理就是通过在类属性中添加一个单例判定位ins_flag，通过这个flag判断是否已经被实例化过了,如果被实例化过了就返回该实例。 new方法实现单例：1234567891011class Singleton(object): def __new__(cls, *args, **kwargs): if not hasattr(cls,\"_instance\"): cls._instance = super(Singleton, cls).__new__(cls, *args, **kwargs) return cls._instances1 = Singleton()s2 = Singleton()print s1 is s2 因为重写new方法，所以继承至Singleton的类，在不重写new的情况下都将是单例模式。 元类实现单例当初我也很疑惑为什么我们是从写使用元类的init方法，而不是使用new方法来初为元类增加一个属性。其实我只是上面那一段关于元类中new方法迷惑了，它主要用于我们需要对类的结构进行改变的时候我们才要重写这个方法。 1234567891011121314151617181920212223242526272829class Singleton(type): def __init__(self, *args, **kwargs): print \"__init__\" self.__instance = None super(Singleton,self).__init__(*args, **kwargs) def __call__(self, *args, **kwargs): print \"__call__\" if self.__instance is None: self.__instance = super(Singleton,self).__call__(*args, **kwargs) return self.__instanceclass Foo(object): __metaclass__ = Singleton #在代码执行到这里的时候，元类中的__new__方法和__init__方法其实已经被执行了，而不是在Foo实例化的时候执行。且仅会执行一次。foo1 = Foo()foo2 = Foo()print Foo.__dict__ #_Singleton__instance': &lt;__main__.Foo object at 0x100c52f10&gt; 存在一个私有属性来保存属性，而不会污染Foo类（其实还是会污染，只是无法直接通过__instance属性访问）print foo1 is foo2 # True# 输出# __init__# __call__# __call__# &#123;'__module__': '__main__', '__metaclass__': &lt;class '__main__.Singleton'&gt;, '_Singleton__instance': &lt;__main__.Foo object at 0x100c52f10&gt;, '__dict__': &lt;attribute '__dict__' of 'Foo' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Foo' objects&gt;, '__doc__': None&#125;# True 基于这个例子： 我们知道元类(Singleton)生成的实例是一个类(Foo),而这里我们仅仅需要对这个实例(Foo)增加一个属性(instance)来判断和保存生成的单例。想想也知道为一个类添加一个属性当然是在init__中实现了。 关于call方法的调用，因为Foo是Singleton的一个实例。所以Foo()这样的方式就调用了Singleton的call方法。不明白就回头看看上一节中的call方法介绍。 假如我们通过元类的new方法来也可以实现，但显然没有通过init来实现优雅，因为我们不会为了为实例增加一个属性而重写new方法。所以这个形式不推荐。 1234567891011121314151617181920212223242526class Singleton(type): def __new__(cls, name,bases,attrs): print \"__new__\" attrs[\"_instance\"] = None return super(Singleton,cls).__new__(cls,name,bases,attrs) def __call__(self, *args, **kwargs): print \"__call__\" if self._instance is None: self._instance = super(Singleton,self).__call__(*args, **kwargs) return self._instanceclass Foo(object): __metaclass__ = Singletonfoo1 = Foo()foo2 = Foo()print Foo.__dict__ print foo1 is foo2 # True# 输出# __new__# __call__# __call__# &#123;'__module__': '__main__', '__metaclass__': &lt;class '__main__.Singleton'&gt;, '_instance': &lt;__main__.Foo object at 0x103e07ed0&gt;, '__dict__': &lt;attribute '__dict__' of 'Foo' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Foo' objects&gt;, '__doc__': None&#125;# True","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"python原类","date":"2019-11-03T02:05:32.000Z","path":"2019/11/03/python/python原类/","text":"Python中一切皆对象，类也是对象​ 之前我们说Python中一切都是对象。对象从哪里来，对象是类的实例。如下，使用type()函数查看对象所属的类型。我们可以看到Python中所以实例都是类的对象。那么类呢，既然一切都是对象，那么类也应该是对象。如下代码中发现我们创建的Person类原来也是对象，是type的对象。 123456789101112131415161718192021222324a =10; b = 12.12; c=\"hello\" ;d =[1,2,3,\"rr\"];e = &#123;\"aa\":1,\"bb\":\"cc\"&#125;type(a);type(b);type(c);type(d);type(e)&lt;class 'int'&gt; #a = 10;a也是对象，即10是对象，是int类型的对象&lt;class 'float'&gt; #float也是类，注意python很多类的写法是小写，有的则是大写&lt;class 'str'&gt;&lt;class 'list'&gt;&lt;class 'dict'&gt;class Person(object): print(\"不调用类，也会执行我\") def __init__(self,name): self.name = name def p(self): print(\"this is a methond\") print(Person) tom = Person(\"tom\")print(\"tom实例的类型是：%s\"%type(tom)) # 实例tom是Person类的对象。print(\"Peron类的类型：%s\"%type(Person)) #结果看出我们创建的类属于type类,也就是说Person是type类的对象print(\"type的类型是：%s\"%type(type)) #type是type自己的对象不调用类，也会执行我&lt;class '__main__.Person'&gt;tom实例的类型是：&lt;class '__main__.Person'&gt;Peron类的类型：&lt;class 'type'&gt;type的类型是：&lt;class 'type'&gt; 动态创建类通过class动态的构建需要的类因为类也是对象，你可以在运行时动态的创建它们，就像其他任何对象一样。首先，你可以在函数中创建类，使用class关键字即可。 12345678910111213141516def choose_class(name): if name == 'foo': class Foo(object): pass return Foo # 返回的是类，不是类的实例 else: class Bar(object): pass return BarMyClass = choose_class('foo')print MyClass # 函数返回的是类，不是类的实例#输出：&lt;class '__main__.Foo'&gt;print MyClass() # 你可以通过这个类创建类实例，也就是对象#输出：&lt;__main__.Foo object at 0x1085ed950 通过type函数构造类但这还不够动态，因为你仍然需要自己编写整个类的代码。由于类也是对象，所以它们必须是通过什么东西来生成的才对。当你使用class关键字时，Python解释器自动创建这个对象。但就和Python中的大多数事情一样，Python仍然提供给你手动处理的方法。还记得内建函数type吗？这个古老但强大的函数能够让你知道一个对象的类型是什么，就像这样： 12345678print type(1)#输出：&lt;type 'int'&gt;print type(\"1\")#输出：&lt;type 'str'&gt;print type(ObjectCreator)#输出：&lt;type 'type'&gt;print type(ObjectCreator())#输出：&lt;class '__main__.ObjectCreator'&gt; 这里，type有一种完全不同的能力，它也能动态的创建类。type可以接受一个类的描述作为参数，然后返回一个类。（我知道，根据传入参数的不同，同一个函数拥有两种完全不同的用法是一件很傻的事情，但这在Python中是为了保持向后兼容性） type的语法： 1type(类名, 父类的元组（针对继承的情况，可以为空），包含属性的字典（名称和值）) 比如下面的代码： 12class MyShinyClass(object): pass 可以手动通过type创建，其实 12345MyShinyClass = type('MyShinyClass', (), &#123;&#125;) # 返回一个类对象print MyShinyClass#输出：&lt;class '__main__.MyShinyClass'&gt;print MyShinyClass() # 创建一个该类的实例#输出：&lt;__main__.MyShinyClass object at 0x1085cd810&gt; 你会发现我们使用“MyShinyClass”作为类名，并且也可以把它当做一个变量来作为类的引用。 接下来我们通过一个具体的例子看看type是如何创建类的，范例：javascript:void(0);) 123456789101112131415161718192021222324252627282930311、构建Foo类#构建目标代码class Foo(object): bar = True#使用type构建Foo = type('Foo', (), &#123;'bar':True&#125;)2.继承Foo类#构建目标代码：class FooChild(Foo): pass#使用type构建FooChild = type('FooChild', (Foo,),&#123;&#125;)print FooChild#输出：&lt;class '__main__.FooChild'&gt;print FooChild.bar # bar属性是由Foo继承而来#输出：True3.为Foochild类增加方法def echo_bar(self): print self.barFooChild = type('FooChild', (Foo,), &#123;'echo_bar': echo_bar&#125;)hasattr(Foo, 'echo_bar')#输出：Falsehasattr(FooChild, 'echo_bar')#输出：Truemy_foo = FooChild()my_foo.echo_bar()#输出：True 可以看到，在Python中，类也是对象，你可以动态的创建类。这就是当我们使用关键字class时Python在幕后做的事情，而这就是通过元类来实现的。 type创建类与class的比较使用type创建带属性和方法的类1234567891011121314151617181920212223242526272829303132333435363738391.使用type创建带有属性的类,添加的属性是类属性，并不是实例属性Girl = type(\"Girl\",(),&#123;\"country\":\"china\",\"sex\":\"male\"&#125;)girl = Girl()print(girl.country,girl.sex) #使用type创建的类，调用属性时IDE不会自动提示补全print(type(girl),type(Girl))'''china male&lt;class '__main__.Girl'&gt; &lt;class 'type'&gt;''' 2.使用type创建带有方法的类#python中方法有普通方法，类方法，静态方法。def speak(self): #要带有参数self,因为类中方法默认带self参数。 print(\"这是给类添加的普通方法\") @classmethoddef c_run(cls): print(\"这是给类添加的类方法\") @staticmethoddef s_eat(): print(\"这是给类添加的静态方法\") #创建类，给类添加静态方法，类方法，普通方法。跟添加类属性差不多.Boy = type(\"Boy\",(),&#123;\"speak\":speak,\"c_run\":c_run,\"s_eat\":s_eat,\"sex\":\"female\"&#125;)boy = Boy()boy.speak()boy.s_eat() #调用类中的静态方法boy.c_run() #调用类中类方法print(\"boy.sex:\",boy.sex)print(type(boy),type(Boy))'''这是给类添加的普通方法这是给类添加的静态方法这是给类添加的类方法boy.sex: female&lt;class '__main__.Boy'&gt; &lt;class 'type'&gt;''' 使用type定义带继承，属性和方法的类1234567891011121314151617181920class Person(object): def __init__(self,name): self.name = name def p(self): print(\"这是Person的方法\")class Animal(object): def run(self): print(\"animal can run \")#定义一个拥有继承的类，继承的效果和性质和class一样。Worker = type(\"Worker\",(Person,Animal),&#123;\"job\":\"程序员\"&#125;)w1 = Worker(\"tom\")w1.p()w1.run()print(type(w1),type(Worker))'''这是Person的方法animal can run &lt;class '__main__.Worker'&gt; &lt;class 'type'&gt;&lt;class '__main__.Person'&gt;''' 总结： 通过type添加的属性是类属性，并不是实例属性通过type可以给类添加普通方法，静态方法，类方法，效果跟class一样type创建类的效果，包括继承等的使用性质和class创建的类一样。本质class创建类的本质就是用type创建。所以可以说python中所有类都是type创建的。 自定义元类元类的主要目的就是为了当创建类时能够自动地改变类。通常，你会为API做这样的事情，你希望可以创建符合当前上下文的类。假想一个很傻的例子，你决定在你的模块里所有的类的属性都应该是大写形式。有好几种方法可以办到，但其中一种就是通过设定metaclass。采用这种方法，这个模块中的所有类都会通过这个元类来创建，我们只需要告诉元类把所有的属性都改成大写形式就万事大吉了。 metaclass实际上可以被任意调用，它并不需要是一个正式的类。所以，我们这里就先以一个简单的函数作为例子开始。 1、使用函数当做元类12345678910111213# 元类会自动将你通常传给‘type’的参数作为自己的参数传入def upper_attr(future_class_name, future_class_parents, future_class_attr): '''返回一个类对象，将属性都转为大写形式''' #选择所有不以'__'开头的属性 attrs = ((name, value) for name, value in future_class_attr.items() if not name.startswith('__')) # 将它们转为大写形式 uppercase_attr = dict((name.upper(), value) for name, value in attrs) #通过'type'来做类对象的创建 return type(future_class_name, future_class_parents, uppercase_attr)#返回一个类class Foo(object): __metaclass__ = upper_attr bar = 'bip' 12345678print hasattr(Foo, 'bar')# 输出: Falseprint hasattr(Foo, 'BAR')# 输出:True f = Foo()print f.BAR# 输出:'bip' 2、使用class来当做元类由于metaclass必须返回一个类。 1234567891011# 请记住，'type'实际上是一个类，就像'str'和'int'一样。所以，你可以从type继承# __new__ 是在__init__之前被调用的特殊方法，__new__是用来创建对象并返回之的方法，__new_()是一个类方法# 而__init__只是用来将传入的参数初始化给对象，它是在对象创建之后执行的方法。# 你很少用到__new__，除非你希望能够控制对象的创建。这里，创建的对象是类，我们希望能够自定义它，所以我们这里改写__new__# 如果你希望的话，你也可以在__init__中做些事情。还有一些高级的用法会涉及到改写__call__特殊方法，但是我们这里不用，下面我们可以单独的讨论这个使用class UpperAttrMetaClass(type): def __new__(upperattr_metaclass, future_class_name, future_class_parents, future_class_attr): attrs = ((name, value) for name, value in future_class_attr.items() if not name.startswith('__')) uppercase_attr = dict((name.upper(), value) for name, value in attrs) return type(future_class_name, future_class_parents, uppercase_attr)#返回一个对象，但同时这个对象是一个类 但是，这种方式其实不是OOP。我们直接调用了type，而且我们没有改写父类的new方法。现在让我们这样去处理: 12345678class UpperAttrMetaclass(type): def __new__(upperattr_metaclass, future_class_name, future_class_parents, future_class_attr): attrs = ((name, value) for name, value in future_class_attr.items() if not name.startswith('__')) uppercase_attr = dict((name.upper(), value) for name, value in attrs) # 复用type.__new__方法 # 这就是基本的OOP编程，没什么魔法。由于type是元类也就是类，因此它本身也是通过__new__方法生成其实例，只不过这个实例是一个类. return type.__new__(upperattr_metaclass, future_class_name, future_class_parents, uppercase_attr) 你可能已经注意到了有个额外的参数upperattr_metaclass，这并没有什么特别的。类方法的第一个参数总是表示当前的实例，就像在普通的类方法中的self参数一样。当然了，为了清晰起见，这里的名字我起的比较长。但是就像self一样，所有的参数都有它们的传统名称。因此，在真实的产品代码中一个元类应该是像这样的： 12345class UpperAttrMetaclass(type): def __new__(cls, name, bases, dct): attrs = ((name, value) for name, value in dct.items() if not name.startswith('__') uppercase_attr = dict((name.upper(), value) for name, value in attrs) return type.__new__(cls, name, bases, uppercase_attr) 如果使用super方法的话，我们还可以使它变得更清晰一些。 12345class UpperAttrMetaclass(type): def __new__(cls, name, bases, dct): attrs = ((name, value) for name, value in dct.items() if not name.startswith('__')) uppercase_attr = dict((name.upper(), value) for name, value in attrs) return super(UpperAttrMetaclass, cls).__new__(cls, name, bases, uppercase_attr) 参考https://www.cnblogs.com/tkqasn/p/6524879.html https://blog.csdn.net/qq_26442553/article/details/82459234","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"pm2学习","date":"2019-11-02T02:10:05.000Z","path":"2019/11/02/pm2学习/","text":"pm2基本命令123456789101112131415161718192021222324252627# 启动程序pm2 start app.jspm2 start npm --name pro -- run dev# 查看程序pm2 start listpm2 monitpm2 logs# 重启pm2 restart allpm2 reload allpm2 restartt 0# 停止pm2 stop allpm2 stop 0# 杀死pm2 delete allpm2 delete 0# 集群pm2 start app.js -i max # 根据cpu数目启动线程pm2 start app.js -i 3 # 启动3个进程pm2 start app.js -x # 使用fork模式启动pm2 start app.json 日志问题日志系统对于任意应用而言，通常都是必不可少的一个辅助功能。pm2的相关文件默认存放于$HOME/.pm2/目录下，其日志主要有两类： a. pm2自身的日志，存放于$HOME/.pm2/pm2.log； b. pm2所管理的应用的日志，存放于$HOME/.pm2/logs/目录下，标准谁出日志存放于${APP_NAME}_out.log，标准错误日志存放于${APP_NAME}_error.log； 这里之所以把日志单独说明一下是因为，如果程序开发不严谨，为了调试程序，导致应用产生大量标准输出，使服务器本身记录大量的日志，导致服务磁盘满载问题。一般而言，pm2管理的应用本身都有自己日志系统，所以对于这种不必要的输出内容需禁用日志，重定向到/dev/null。 与crontab比较，也有类似情况，crontab自身日志，与其管理的应用本身的输出。应用脚本输出一定需要重定向到/dev/null，因为该输出内容会以邮件的形式发送给用户，内容存储在邮件文件，会产生意向不到的结果，或会导致脚本压根不被执行； 开机启动12pm2 startupsystemctl enable pm2-root 参考https://pm2.keymetrics.io/docs/usage/monitoring/","tags":[{"name":"node","slug":"node","permalink":"http://wumuwumu.github.io/tags/node/"}]},{"title":"Dockerfile中的CMD与ENTRYPOINT","date":"2019-10-29T15:49:07.000Z","path":"2019/10/29/Dockerfile中的CMD与ENTRYPOINT/","text":"https://www.cnblogs.com/sparkdev/p/8461576.html","tags":[{"name":"docker","slug":"docker","permalink":"http://wumuwumu.github.io/tags/docker/"}]},{"title":"docker网络模式","date":"2019-10-29T06:26:03.000Z","path":"2019/10/29/docker/docker网络模式/","text":"Docker的网络模式详解1、Docker的四种网络模式 img （1）docker四种网络模式如下： Bridge contauner 桥接式网络模式 Host(open) container 开放式网络模式 Container(join) container 联合挂载式网络模式，是host网络模式的延伸 None(Close) container 封闭式网络模式 （2）可以通过docker network命令查看 12345[root@along ~]# docker network lsNETWORK ID NAME DRIVER SCOPEf23b4899add1 bridge bridge local65520497f693 host host locala0c5f18e0f04 none null local复制代码 （3）docker run –network 命令可以指定使用网络模式 2、Bridge 网络模式2.1 介绍 当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上，所以有默认地址172.17.0.0/16的地址。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。 从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中。可以通过brctl show命令查看。 123[root@along ~]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.024241c45d6e no复制代码 bridge模式是docker的默认网络模式，不写–net参数，就是bridge模式。使用docker run -p时，docker实际是在iptables做了DNAT规则，实现端口转发功能。可以使用iptables -t nat -vnL查看。 1234[root@along ~]# iptables -t nat -vnLChain POSTROUTING (policy ACCEPT 20 packets, 1238 bytes) pkts bytes target prot opt in out source destination 0 0 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0复制代码 2.2 bridge模式示意图 2.3 演示bridge 网络模式；–network不指定，默认也是bridge模式 12345678910111213141516171819202122232425262728[root@along ~]# docker run --name b1 -it --network bridge --rm busybox:latest / # ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:12 errors:0 dropped:0 overruns:0 frame:0 TX packets:6 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1016 (1016.0 B) TX bytes:508 (508.0 B)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)/ # route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0/ # ping 10.11.55.5 正常访问宿主机PING 10.11.55.5 (10.11.55.5): 56 data bytes64 bytes from 10.11.55.5: seq=0 ttl=64 time=0.292 ms/ # exit复制代码 3、Host 网络模式3.1 介绍 如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。 3.2 Host模式示意图 3.3 演示12345678910111213141516171819202122232425262728[root@along ~]# docker run --name b2 -it --network host --rm busybox:latest/ # ifconfig -a 和宿主机一样docker0 Link encap:Ethernet HWaddr 02:42:41:C4:5D:6E inet addr:172.17.0.1 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:41ff:fec4:5d6e/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:90 errors:0 dropped:0 overruns:0 frame:0 TX packets:26 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:5903 (5.7 KiB) TX bytes:2381 (2.3 KiB)eth0 Link encap:Ethernet HWaddr 00:0C:29:AB:D2:DA inet addr:10.11.55.5 Bcast:10.11.55.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:feab:d2da/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:3913 errors:0 dropped:0 overruns:0 frame:0 TX packets:3327 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:354314 (346.0 KiB) TX bytes:919096 (897.5 KiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)复制代码 4、Container 网络模式4.1 介绍 这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。 4.2 Container模式示意图 4.3 演示（1）在一个终端，使用bridge网络模式启动容器b1 12345678910111213141516171819202122232425[root@along ~]# docker run --name b1 -it --rm busybox:latest / # ifconfig b1的ip为172.17.0.2eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:6 errors:0 dropped:0 overruns:0 frame:0 TX packets:6 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:508 (508.0 B) TX bytes:508 (508.0 B)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)/ # echo &quot;hello world b1&quot; &gt; /tmp/index.html/ # httpd -h /tmp/ 在b1上启动httpd服务/ # netstat -nutlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 :::80 :::* LISTEN 复制代码 （2）在另一个终端使用Container 网络模式创建容器b2 12345678910111213141516171819202122[root@along ~]# docker run --name b2 -it --network container:b1 --rm busybox:latest/ # ifconfig -a b2的ip和b1一样eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:648 (648.0 B)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)/ # wget -O - -q 127.0.0.1 b1启动的httpd服务，在b2上直接访问hello world b1/ # ls /tmp/ 但是文件系统并不共享，只共享网络复制代码 5、None 网络模式5.1 介绍 使用none模式，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息，只有lo 网络接口。需要我们自己为Docker容器添加网卡、配置IP等。 不参与网络通信，运行于此类容器中的进程仅能访问本地回环接口；仅适用于进程无须网络通信的场景中，例如：备份、进程诊断及各种离线任务等。 5.2 Node模式示意图 5.3 演示12345678910111213[root@along ~]# docker run --name b1 -it --network none --rm busybox:latest / # ifconfig lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)/ # route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface","tags":[{"name":"docker","slug":"docker","permalink":"http://wumuwumu.github.io/tags/docker/"}]},{"title":"npm版本管理","date":"2019-10-29T06:08:30.000Z","path":"2019/10/29/npm版本管理/","text":"在打包项目的时候，我们都要更新package.json的版本号，接着给给代码添加tag，最后push代码，这样的流程泰国麻烦有什么方法简化。 12345671. package.json`中修改递增`version2. git add -A3. git commit -m &quot;update version&quot;4. git push5. git tag &lt;tag version&gt;6. git push --tag7. npm publish 解决方法我们可以使用npm version命令，从文档上我们可以看到其依据semver支持了大部分alias： 1npm version [&lt;newversion&gt; | major | minor | patch | premajor | preminor | prepatch | prerelease | from-git] 例：初始版本为1.0.0 npm version prepatch //预备补丁版本号 v1.0.1-0 npm version prerelease //预发布版本号 v1.0.1-1 npm version patch //补丁版本号 v1.0.2 npm version preminor //预备次版本号 v1.1.0-0 npm version minor //次版本号 v1.1.0 npm version premajor //预备主版本号 v2.0.0-0 npm version major //主版本号 v2.0.0 当在仓库中执行npm version时，会自动提交git commit并打上git tag。 当使用-m参数时，就可以自定义发布版本的信息，其中%s可以用来代替当前版本号 123&gt; npm version patch -m &quot;upgrade to %s for reasons&quot;&gt; 复制代码&gt; 这样以后版本迭代只需要以下步骤 npm version patch | minor | major | ...etc git push git push --tag npm publish npm version会同时创建时 v版本号 形式的tag，将tag push上去就可以自动触发构建了。 也可以简化这步操作，在npm version操作后自动 push 在 package.json中加入下面的代码，即可实现npm version操作后，自动push代码及tag，也就自动触发了 npm 发布操作。 123\"scripts\": &#123; \"postversion\": \"git push --follow-tags\"&#125; 衍生问题 如何发布beta，rc，alpha版本呢？如果发布了，应该如何安装？ 解决方案首先我们要理解这些版本的含义 alpha：内部测试版本 beta： 公开测试版本 rc： 候选版本（Release Candidate） 然后将package.json的version改成x.x.x-beta 配合npm publish --tag &lt;tag&gt;，我们可以发布对应的dist-tag 举个例子： 使用npm publish --tag beta发布后，然后就可以使用npm install &lt;pkg&gt;@beta安装对应版本的包。 我们可以通过npm dist-tag ls &lt;pkg&gt;来查看包的dist-tag 1234&#123; latest: 1.0.1, // 这就是npm publish默认发布的tag beta: 1.0.1-beta&#125; 当我们的beta版本稳定后，可以使用npm dist-tag add x.x.x-beta latest设置为稳定版本。 npm version与npm dist-tag关于npm version prerelease的作用我这里不再赘述，你可以查看这个文章。我只是记录一下关于npm version与npm dist-tag的使用： 第一步：发布第一个稳定版本 1npm publish//1.0.0 第二步：修改文件继续发布第二个版本 123git add -A &amp;&amp; git commit -m &quot;c&quot;npm version patchnpm publish//1.0.1 第三步：继续修改文件发布一个prerelease版本 123git add -A &amp;&amp; git commit -m &quot;c&quot;npm version prereleasenpm publish --tag -beta//版本n-n-n-n@1.0.2-0 第四步：继续修改发布第二个prerelease版本 123git add -A &amp;&amp; git commit -m &quot;c&quot;npm version prereleasenpm publish --tag -beta//版本n-n-n-n@1.0.2-1 第五步：npm info查看我们的版本信息 1234567891011121314151617181920212223242526&#123; name: 'n-n-n-n', 'dist-tags': &#123; latest: '1.0.1', '-beta': '1.0.2-1' &#125;, versions: [ '1.0.0', '1.0.1', '1.0.2-0', '1.0.2-1' ], maintainers: [ 'liangklfang &lt;liangklfang@163.com&gt;' ], time: &#123; modified: '2017-04-01T12:17:56.755Z', created: '2017-04-01T12:15:23.605Z', '1.0.0': '2017-04-01T12:15:23.605Z', '1.0.1': '2017-04-01T12:16:24.916Z', '1.0.2-0': '2017-04-01T12:17:23.354Z', '1.0.2-1': '2017-04-01T12:17:56.755Z' &#125;, homepage: 'https://github.com/liangklfang/n#readme', repository: &#123; type: 'git', url: 'git+https://github.com/liangklfang/n.git' &#125;, bugs: &#123; url: 'https://github.com/liangklfang/n/issues' &#125;, license: 'ISC', readmeFilename: 'README.md', version: '1.0.1', description: '', main: 'index.js', scripts: &#123; test: 'echo \"Error: no test specified\" &amp;&amp; exit 1' &#125;, author: '', gitHead: '8123b8addf6fed83c4c5edead1dc2614241a4479', dist: &#123; shasum: 'a60d8b02222e4cae74e91b69b316a5b173d2ac9d', tarball: 'https://registry.npmjs.org/n-n-n-n/-/n-n-n-n-1.0.1.tgz' &#125;, directories: &#123;&#125; &#125; 我们只要注意下面者两个部分： 12&apos;dist-tags&apos;: &#123; latest: &apos;1.0.1&apos;, &apos;-beta&apos;: &apos;1.0.2-1&apos; &#125;, versions: [ &apos;1.0.0&apos;, &apos;1.0.1&apos;, &apos;1.0.2-0&apos;, &apos;1.0.2-1&apos; ], 其中最新的稳定版本和最新的beta版本可以在dist-tags中看到，而versions数组中存储的是所有的版本。 第六步：npm dist-tag命令 1npm dist-tag ls n-n-n-n 即npm dist-tag获取到所有的最新的版本，包括prerelease与稳定版本，得到下面结果： 12-beta: 1.0.2-1latest: 1.0.1 第七步：当我们的prerelease版本已经稳定了，重新设置为稳定版本 1npm dist-tag add n-n-n-n@1.0.2-1 latest 此时你通过npm info查看可以知道： 1234567891011121314151617181920212223242526&#123; name: 'n-n-n-n', 'dist-tags': &#123; latest: '1.0.2-1', '-beta': '1.0.2-1' &#125;, versions: [ '1.0.0', '1.0.1', '1.0.2-0', '1.0.2-1' ], maintainers: [ 'liangklfang &lt;liangklfang@163.com&gt;' ], time: &#123; modified: '2017-04-01T12:24:55.800Z', created: '2017-04-01T12:15:23.605Z', '1.0.0': '2017-04-01T12:15:23.605Z', '1.0.1': '2017-04-01T12:16:24.916Z', '1.0.2-0': '2017-04-01T12:17:23.354Z', '1.0.2-1': '2017-04-01T12:17:56.755Z' &#125;, homepage: 'https://github.com/liangklfang/n#readme', repository: &#123; type: 'git', url: 'git+https://github.com/liangklfang/n.git' &#125;, bugs: &#123; url: 'https://github.com/liangklfang/n/issues' &#125;, license: 'ISC', readmeFilename: 'README.md', version: '1.0.2-1', description: '', main: 'index.js', scripts: &#123; test: 'echo \"Error: no test specified\" &amp;&amp; exit 1' &#125;, author: '', gitHead: '03189d2cc61604aa05f4b93e429d3caa3b637f8c', dist: &#123; shasum: '41ea170a6b155c8d61658cd4c309f0d5d1b12ced', tarball: 'https://registry.npmjs.org/n-n-n-n/-/n-n-n-n-1.0.2-1.tgz' &#125;, directories: &#123;&#125; &#125; 主要关注如下: 12&apos;dist-tags&apos;: &#123; latest: &apos;1.0.2-1&apos;, &apos;-beta&apos;: &apos;1.0.2-1&apos; &#125;, versions: [ &apos;1.0.0&apos;, &apos;1.0.1&apos;, &apos;1.0.2-0&apos;, &apos;1.0.2-1&apos; ] 此时latest版本已经是prerelease版本”1.0.2-1”了！此时用户如果直接运行npm install就会安装我们的prerelease版本了，因为版本已经更新了！ 当然，我们的npm publish可以有很多tag的，比如上面是beta，也可以是stable, dev, canary等，比如下面你继续运行： 123git add -A &amp;&amp; git commit -m &quot;c&quot;npm version prereleasenpm publish --tag -dev 此时你运行npm info就会得到下面的信息： 123456789101112131415161718192021222324252627&#123; name: &apos;n-n-n-n&apos;, &apos;dist-tags&apos;: &#123; latest: &apos;1.0.2-1&apos;, &apos;-beta&apos;: &apos;1.0.2-1&apos;, &apos;-dev&apos;: &apos;1.0.2-2&apos; &#125;, versions: [ &apos;1.0.0&apos;, &apos;1.0.1&apos;, &apos;1.0.2-0&apos;, &apos;1.0.2-1&apos;, &apos;1.0.2-2&apos; ], maintainers: [ &apos;liangklfang &lt;liangklfang@163.com&gt;&apos; ], time: &#123; modified: &apos;2017-04-01T13:01:17.106Z&apos;, created: &apos;2017-04-01T12:15:23.605Z&apos;, &apos;1.0.0&apos;: &apos;2017-04-01T12:15:23.605Z&apos;, &apos;1.0.1&apos;: &apos;2017-04-01T12:16:24.916Z&apos;, &apos;1.0.2-0&apos;: &apos;2017-04-01T12:17:23.354Z&apos;, &apos;1.0.2-1&apos;: &apos;2017-04-01T12:17:56.755Z&apos;, &apos;1.0.2-2&apos;: &apos;2017-04-01T13:01:17.106Z&apos; &#125;, homepage: &apos;https://github.com/liangklfang/n#readme&apos;, repository: &#123; type: &apos;git&apos;, url: &apos;git+https://github.com/liangklfang/n.git&apos; &#125;, bugs: &#123; url: &apos;https://github.com/liangklfang/n/issues&apos; &#125;, license: &apos;ISC&apos;, readmeFilename: &apos;README.md&apos;, version: &apos;1.0.2-1&apos;, description: &apos;&apos;, main: &apos;index.js&apos;, scripts: &#123; test: &apos;echo &quot;Error: no test specified&quot; &amp;&amp; exit 1&apos; &#125;, author: &apos;&apos;, gitHead: &apos;03189d2cc61604aa05f4b93e429d3caa3b637f8c&apos;, dist: &#123; shasum: &apos;41ea170a6b155c8d61658cd4c309f0d5d1b12ced&apos;, tarball: &apos;https://registry.npmjs.org/n-n-n-n/-/n-n-n-n-1.0.2-1.tgz&apos; &#125;, directories: &#123;&#125; &#125; 重点关注如下内容 12&apos;dist-tags&apos;: &#123; latest: &apos;1.0.2-1&apos;, &apos;-beta&apos;: &apos;1.0.2-1&apos;, &apos;-dev&apos;: &apos;1.0.2-2&apos; &#125;, versions: [ &apos;1.0.0&apos;, &apos;1.0.1&apos;, &apos;1.0.2-0&apos;, &apos;1.0.2-1&apos;, &apos;1.0.2-2&apos; ], 此时你会看到-beta版本最新是1.0.2-1，而-dev版本最新是1.0.2-2 参考 https://github.com/liangklfangl/npm-dist-tag https://juejin.im/post/5b624d42f265da0fa1223ffa https://docs.npmjs.com/cli/version","tags":[{"name":"node","slug":"node","permalink":"http://wumuwumu.github.io/tags/node/"}]},{"title":"python中and和or用法","date":"2019-10-25T07:41:30.000Z","path":"2019/10/25/python/python中and和or用法/","text":"在Python 中，and 和 or 执行布尔逻辑演算，如你所期待的一样。但是它们并不返回布尔值，而是返回它们实际进行比较的值之一。 （类似C++里面的&amp;&amp;和||的短路求值） （ 在布尔环境中，0、”、[]、()、{}、None为假；其它任何东西都为真。但是可以在类中定义特定的方法使得类实例的演算值为假。） and实例：123456&gt;&gt;&gt; 'a' and 'b''b'&gt;&gt;&gt; '' and 'b'''&gt;&gt;&gt; 'a' and 'b' and 'c''c'12345 从左到右扫描，返回第一个为假的表达式值，无假值则返回最后一个表达式值。 or实例：123456&gt;&gt;&gt; 'a' or 'b''a'&gt;&gt;&gt; '' or 'b''b'&gt;&gt;&gt; '' or [] or&#123;&#125;&#123;&#125;12345 从左到右扫描，返回第一个为真的表达式值，无真值则返回最后一个表达式值。 and-or搭配：123456&gt;&gt;&gt; a = \"betabin\"&gt;&gt;&gt; b = \"python\"&gt;&gt;&gt; 1 and a or b'betabin'&gt;&gt;&gt; 0 and a or b'python'12345 看起来类似于于我们Ｃ＋＋中的条件运算符（bool？a：b），是的，当a为true的时候是一样的。但是，当a为false的时候，就明显不同了。 如果坚持要用and-or技巧来实现条件运算符的话，可以用种安全的方法： 1234&gt;&gt;&gt; a = \"\"&gt;&gt;&gt; b = \"betabin\"&gt;&gt;&gt; (1 and [a] or [b])[0]''123 就是万能的[]，把a为假的可能性给抹杀掉，然后通过[0]再获得（因为要通过[0]获得元素，所以b也得加上[]）。 这个and-or技巧主要在lambda中使用。","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"使用jenv对java多版本管理","date":"2019-10-25T02:42:43.000Z","path":"2019/10/25/java/使用jenv对java多版本管理/","text":"配置JDK环境变量 打开 vim ~/.bash_profile 文件 进行添加 1234export JAVA_8_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_112.jdk/Contents/Homeexport JAVA_7_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home# 默认激活 jdk8export JAVA_HOME=$JAVA_8_HOME 编辑完成，重新加载 .bash_profile 1$ source ~/.bash_profile jEnv安装 安装 1$ brew install jenv 配置 安装了zsh，配置如下 12$ echo &apos;export PATH=&quot;$HOME/.jenv/bin:$PATH&quot;&apos; &gt;&gt; ~/.zshrc$ echo &apos;eval &quot;$(jenv init -)&quot;&apos; &gt;&gt; ~/.zshrc 如果是默认的bash 12$ echo &apos;export PATH=&quot;$HOME/.jenv/bin:$PATH&quot;&apos; &gt;&gt; ~/.bash_profile$ echo &apos;eval &quot;$(jenv init -)&quot;&apos; &gt;&gt; ~/.bash_profilec jEnv配置JDK查看安装的java版本，如果我们一开始未添加jdk，执行jenv versions 应该是空的，* 号位置表示当前的jdk版本 12345678$ jenv versions system 1.7* 1.7.0.80 (set by /Users/gulj/.java-version) 1.8 1.8.0.112 oracle64-1.7.0.80 oracle64-1.8.0.112 重启下terminal，为jEnv添加java版本 1234添加jdk7$ jenv add /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home添加jdk8$ jenv add /Library/Java/JavaVirtualMachines/jdk1.8.0_112.jdk/Contents/Home 添加完jdk7和jdk8之后，再执行 jenv versions 命令就会看到我们添加的jdk jEnv常用命令 移除指定版本jdk 1$ jenv remove 1.8.0.111 选择一个jdk版本 1$ jenv local 1.8.0.111 设置默认的jdk版本 1$ jenv global 1.8.0.111 查看当前版本jdk的路径 1jenv which java","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"react-tree-walker学习","date":"2019-10-19T09:02:13.000Z","path":"2019/10/19/react-tree-walker学习/","text":"react-tree-walker这个主要用于遍历react的dom树，用于在react服务端渲染数据请求的时候。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import reactTreeWalker from 'react-tree-walker'class DataFetcher extends React.Component &#123; constructor(props) &#123; super(props) this.getData = this.getData.bind(this) &#125; getData() &#123; // Supports promises! You could call an API for example to fetch some // data, or do whatever \"bootstrapping\" you desire. return Promise.resolve(this.props.id) &#125; render() &#123; return &lt;div&gt;&#123;this.props.children&#125;&lt;/div&gt; &#125;&#125;const app = ( &lt;div&gt; &lt;h1&gt;Hello World!&lt;/h1&gt; &lt;DataFetcher id=&#123;1&#125; /&gt; &lt;DataFetcher id=&#123;2&#125;&gt; &lt;DataFetcher id=&#123;3&#125;&gt; &lt;DataFetcher id=&#123;4&#125; /&gt; &lt;/DataFetcher&gt; &lt;/DataFetcher&gt; &lt;DataFetcher id=&#123;5&#125; /&gt; &lt;/div&gt;)const values = []// You provide this! See the API docs below for full details.function visitor(element, instance) &#123; if (instance &amp;&amp; typeof instance.getData) &#123; return instance.getData().then(value =&gt; &#123; values.push(value) // Return \"false\" to indicate that we do not want to visit \"3\"'s children, // therefore we do not expect \"4\" to make it into our values array. return value !== 3 &#125;) &#125;&#125;reactTreeWalker(app, visitor) .then(() =&gt; &#123; console.log(values) // [1, 2, 3, 5]; // Now is a good time to call React's renderToString whilst exposing // whatever values you built up to your app. &#125;) // since v3.0.0 you need to do your own error handling! .catch(err =&gt; console.error(err)) react-ssr-prepass这个项目还在维护，是一个不错的选择 安装123yarn add react-ssr-prepass# ornpm install --save react-ssr-prepass 使用12345678910111213141516171819import &#123; createElement &#125; from 'react'import &#123; renderToString &#125; from 'react-dom/server'import ssrPrepass from 'react-ssr-prepass'const renderApp = async App =&gt; &#123; const element = createElement(App) await ssrPrepass(element) return renderToString(element)&#125;ssrPrepass(&lt;App /&gt;, (element, instance) =&gt; &#123; if (element.type === SomeData) &#123; return fetchData() &#125; else if (instance &amp;&amp; instance.fetchData) &#123; return instance.fetchData() &#125;&#125;)","tags":[{"name":"react","slug":"react","permalink":"http://wumuwumu.github.io/tags/react/"}]},{"title":"weboack性能优化笔记","date":"2019-10-18T03:52:51.000Z","path":"2019/10/18/weboack性能优化笔记/","text":"https://juejin.im/post/5b652b036fb9a04fa01d616b","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"}]},{"title":"lodash按需加载","date":"2019-10-18T03:48:59.000Z","path":"2019/10/18/lodash按需加载/","text":"lodash提供了很多可用的方法供我们使用，绝对是一个很好用且用起来得心应手的工具库。但是同时，lodash的体积也不小，我们项目中使用的大概522K，可能只是使用了几个方法，但是却把整个lodash库引入了。为了吃几条鱼，就承包了整个鱼塘，代价有点大呀！ 对于这个问题，有几种方案可供选择。 一.引入单个函数 lodash整个安装完之后，引用方式： lodash/function 格式，单独引入某个函数，如 let _trim= require(‘lodash/trim’) 或者 import trim from ‘lodash/trim’ 或者 lodash 中的每个函数在 NPM 都有一个单独的发布模块，单独安装并引用部分模块，然后按以下方式引用 let _trim= require(‘lodash.trim’) 或者 import trim from ‘lodash.trim’ trim(‘ 123123 ‘) 二.借助 lodash-webpack-plugin，babel-plugin-lodash插件优化 使用上述两种方式，在使用较多个lodash中方法的情况下，不太美观，且并不方便。那么我们可以借助于lodash-webpack-plugin，去除未引入的模块，需要和babel-plugin-lodash插件配合使用。类似于webpack的tree-shaking。 1）安装插件：npm i -S lodash-webpack-plugin babel-plugin-lodash 2）webpack.conf.js中 var LodashModuleReplacementPlugin = require(‘lodash-webpack-plugin’) plugins: [ new LodashModuleReplacementPlugin()] 3）.babelrc中配置 “plugins”: [“transform-runtime”,”transform-vue-jsx”,”lodash”] 或者在webpack.conf.js的rules配置 1234567&#123; test: /\\.(js|jsx)$/, loader: &apos;babel-loader&apos;, exclude: /node_modules/, include: [resolve(&apos;src&apos;), resolve(&apos;test&apos;)] options: &#123;plugins: [&apos;lodash&apos;]&#125;&#125; 三.lodash-es结合tree-shaking lodash-es 是着具备 ES6 模块化的版本，只需要直接引入就可以。 import {isEmpty,forIn, cloneDeep} from ‘lodash-es’ tree-shaking的作用，即移除上下文中未引用的代码（dead code） 只有当函数给定输入后，产生相应的输出，且不修改任何外部的东西，才可以安全做shaking的操作 如何使用tree-shaking？ 1）.确保代码是es6格式,即 export，import 2）.package.json中，设置sideEffects 3）.确保tree-shaking的函数没有副作用 4）.babelrc中设置presets [[“env”, { “modules”: false }]] 禁止转换模块，交由webpack进行模块化处理 5）.结合uglifyjs-webpack-plugin","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"}]},{"title":"babel配置","date":"2019-10-18T03:21:01.000Z","path":"2019/10/18/babel配置/","text":"Babel6Babel6 现在使用的越来越少了，但是还是做一个笔记，现在基本都使用babel-preset-env，不需要写babel-preset-esxxxx了，但是babel-preset-stage-x还是要自己去加的。 安装1npm install -D babel-cli babel-preset-env 配置文件Babel6的配置文件是.babelrc 123&#123; //https://juejin.im/post/5a79adeef265da4e93116430&#125; Babel7Babel7 相对于babel6有很大的变化，相关的模块的名字有很大的变化，官方舍弃了babel-preset-esxxxx和babel-preset-stage-x，后者的原因是提案一直在变化。 安装1npm install -D @babel/cli @babel/react @babel/plugin-transform-runtime @babel/env 配置文件Babel7有两种配置文件，一个是.babelrc，是局部的，另外一个是babel.config.js是全局的，具体的可以看下官网。7版本的配置文件解析也变得更加严格。 ###","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"},{"name":"babel","slug":"babel","permalink":"http://wumuwumu.github.io/tags/babel/"}]},{"title":"react的children","date":"2019-10-14T12:44:11.000Z","path":"2019/10/14/react/react的children/","text":"React的核心为组件。你可以像嵌套HTML标签一样嵌套使用这些组件，这使得编写JSX更加容易因为它类似于标记语言。 当我刚开始学习React时，当时我认为“使用 props.children 就这么回事，我知道它的一切”。我错了。。 因为我们使用的事JavaScript，我们会改变children。我们能够给它们发送特殊的属性，以此来决定它们是否进行渲染。让我们来探究一下React中children的作用。 子组件我们有一个组件 &lt;Grid /&gt; 包含了几个组件 &lt;Row /&gt; 。你可能会这么使用它： 12345&lt;Grid&gt; &lt;Row /&gt; &lt;Row /&gt; &lt;Row /&gt;&lt;/Grid&gt; 这三个 Row 组件都成为了 Grid 的 props.children 。使用一个表达式容器，父组件就能够渲染它们的子组件： 12345class Grid extends React.Component &#123; render() &#123; return &lt;div&gt;&#123;this.props.children&#125;&lt;/div&gt; &#125;&#125; 父组件也能够决定不渲染任何的子组件或者在渲染之前对它们进行操作。例如，这个 &lt;Fullstop /&gt; 组件就没有渲染它的子组件： 12345class Fullstop extends React.Component &#123; render() &#123; return &lt;h1&gt;Hello world!&lt;/h1&gt; &#125;&#125; 不管你将什么子组件传递给这个组件，它都只会显示“Hello world!” 任何东西都能是一个childReact中的Children不一定是组件，它们可以使任何东西。例如，我们能够将上面的文字作为children传递我们的 &lt;Grid /&gt; 组件。 1&lt;Grid&gt;Hello world!&lt;/Grid&gt; JSX将会自动删除每行开头和结尾的空格，以及空行。它还会把字符串中间的空白行压缩为一个空格。 这意味着以下的这些例子都会渲染出一样的情况： 123456789101112131415&lt;Grid&gt;Hello world!&lt;/Grid&gt;&lt;Grid&gt; Hello world!&lt;/Grid&gt;&lt;Grid&gt; Hello world!&lt;/Grid&gt;&lt;Grid&gt; Hello world!&lt;/Grid&gt; 你也可以将多种类型的children完美的结合在一起： 123456&lt;Grid&gt; Here is a row: &lt;Row /&gt; Here is another row: &lt;Row /&gt;&lt;/Grid&gt; child 的功能我们能够传递任何的JavaScript表达式作为children，包括函数。 为了说明这种情况，以下是一个组件，它将执行一个传递过来的作为child的函数： 1234567class Executioner extends React.Component &#123; render() &#123; // See how we&apos;re calling the child as a function? // ↓ return this.props.children() &#125;&#125; 你会像这样的使用这个组件 123&lt;Executioner&gt; &#123;() =&gt; &lt;h1&gt;Hello World!&lt;/h1&gt;&#125;&lt;/Executioner&gt; 当然，这个例子并没什么用，只是展示了这个想法。 假设你想从服务器获取一些数据。你能使用多种方法实现，像这种将函数作为child的方法也是可行的。 123&lt;Fetch url=&quot;api.myself.com&quot;&gt; &#123;(result) =&gt; &lt;p&gt;&#123;result&#125;&lt;/p&gt;&#125;&lt;/Fetch&gt; 不要担心这些超出了你的脑容量。我想要的是当你以后遇到这种情况时不再惊讶。有了children什么事都会发生。 操作children如果你看过React的文档你就会说“children是一个不透明的数据结构”。从本质上来讲， props.children 可以使任何的类型，比如数组、函数、对象等等。 React提供了一系列的函数助手来使得操作children更加方便。 循环两个最显眼的函数助手就是 React.Children.map 以及 React.Children.forEach 。它们在对应数组的情况下能起作用，除此之外，当函数、对象或者任何东西作为children传递时，它们也会起作用。 1234567891011121314class IgnoreFirstChild extends React.Component &#123; render() &#123; const children = this.props.children return ( &lt;div&gt; &#123;React.Children.map(children, (child, i) =&gt; &#123; // Ignore the first child if (i &lt; 1) return return child &#125;)&#125; &lt;/div&gt; ) &#125;&#125; &lt;IgnoreFirstChild /&gt; 组件在这里会遍历所有的children，忽略第一个child然后返回其他的。 1234&lt;IgnoreFirstChild&gt; &lt;h1&gt;First&lt;/h1&gt; &lt;h1&gt;Second&lt;/h1&gt; // &lt;- Only this is rendered&lt;/IgnoreFirstChild&gt; 在这种情况下，我们也可以使用 this.props.children.map 的方法。但要是有人讲一个函数作为child传递过来将会发生什么呢？this.props.children 会是一个函数而不是一个数组，接着我们就会产生一个error！ 然而使用 React.Children.map 函数，无论什么都不会报错。 123&lt;IgnoreFirstChild&gt; &#123;() =&gt; &lt;h1&gt;First&lt;/h1&gt;&#125; // &lt;- Ignored ?&lt;/IgnoreFirstChild&gt; 计数因为this.props.children 可以是任何类型的，检查一个组件有多少个children是非常困难的。天真的使用 this.props.children.length ，当传递了字符串或者函数时程序便会中断。假设我们有个child：&quot;Hello World!&quot; ，但是使用 .length 的方法将会显示为12。 这就是为什么我们有 React.Children.count 方法的原因 12345class ChildrenCounter extends React.Component &#123; render() &#123; return &lt;p&gt;React.Children.count(this.props.children)&lt;/p&gt; &#125;&#125; 无论时什么类型它都会返回children的数量 1234567891011121314151617// Renders &quot;1&quot;&lt;ChildrenCounter&gt; Second!&lt;/ChildrenCounter&gt;// Renders &quot;2&quot;&lt;ChildrenCounter&gt; &lt;p&gt;First&lt;/p&gt; &lt;ChildComponent /&gt;&lt;/ChildrenCounter&gt;// Renders &quot;3&quot;&lt;ChildrenCounter&gt; &#123;() =&gt; &lt;h1&gt;First!&lt;/h1&gt;&#125; Second! &lt;p&gt;Third!&lt;/p&gt;&lt;/ChildrenCounter&gt; 转换为数组如果以上的方法你都不适合，你能将children转换为数组通过 React.Children.toArray 方法。如果你需要对它们进行排序，这个方法是非常有用的。 123456789101112class Sort extends React.Component &#123; render() &#123; const children = React.Children.toArray(this.props.children) // Sort and render the children return &lt;p&gt;&#123;children.sort().join(&apos; &apos;)&#125;&lt;/p&gt; &#125;&#125;&lt;Sort&gt; // We use expression containers to make sure our strings // are passed as three children, not as one string &#123;&apos;bananas&apos;&#125;&#123;&apos;oranges&apos;&#125;&#123;&apos;apples&apos;&#125;&lt;/Sort&gt; 上例会渲染为三个排好序的字符串。 执行单一child如果你回过来想刚才的 &lt;Executioner /&gt; 组件，它只能在传递单一child的情况下使用，而且child必须为函数。 12345class Executioner extends React.Component &#123; render() &#123; return this.props.children() &#125;&#125; 我们可以试着去强制执行 propTypes ，就像下面这样 123Executioner.propTypes = &#123; children: React.PropTypes.func.isRequired,&#125; 这会使控制台打印出一条消息，部分的开发者将会把它忽视。相反的，我们可以使用在 render 里面使用 React.Children.only 12345class Executioner extends React.Component &#123; render() &#123; return React.Children.only(this.props.children)() &#125;&#125; 这样只会返回一个child。如果不止一个child，它就会抛出错误，让整个程序陷入中断——完美的避开了试图破坏组件的懒惰的开发者。","tags":[{"name":"react","slug":"react","permalink":"http://wumuwumu.github.io/tags/react/"}]},{"title":"Hello World","date":"2019-10-14T09:01:07.430Z","path":"2019/10/14/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]},{"title":"react-cloneElement","date":"2019-10-13T11:40:12.000Z","path":"2019/10/13/react/react-cloneElement/","text":"react提供了一个克隆 API： 12345React.cloneElement( element, [props], [...children]) 官方定义： 1Clone and return a new React element using element as the starting point. The resulting element will have the original element&apos;s props with the new props merged in shallowly. New children will replace existing children. key and ref from the original element will be preserved. 下面实现一个demo，通过 React.cloneElement 向子组件传递 state 及 function，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import React, &#123; Component &#125; from &apos;react&apos;;import ReactDOM from &apos;react-dom&apos;;class MyContainer extends Component &#123; constructor(props) &#123; super(props) this.state = &#123; count: 1 &#125; this.handleClick = this.handleClick.bind(this); &#125; handleClick() &#123; this.state.count++; this.setState(&#123; count: this.state.count++ &#125;) console.log(this.state) &#125; render() &#123; const childrenWithProps = React.Children.map(this.props.children, child =&gt; React.cloneElement(child, &#123; parentState: this.state.count, handleClick: this.handleClick &#125; )); return ( &lt;div style=&#123;&#123;border:&quot;1px solid blue&quot;&#125;&#125;&gt; &lt;span&gt;父容器:&lt;/span&gt; &#123; childrenWithProps &#125; &lt;/div&gt; ) &#125;&#125;class MySub extends Component &#123; constructor(props) &#123; super(props) this.state = &#123; flag: false &#125; &#125; render() &#123; return ( &lt;div style=&#123;&#123;margin: &quot;15px&quot;, border: &quot;1px solid red&quot;&#125;&#125;&gt; 子元素:&#123;this.props.subInfo&#125; &lt;br/&gt; 父组件属性count值: &#123; this.props.parentState &#125; &lt;br/&gt; &lt;span onClick=&#123; () =&gt; this.props.handleClick() &#125; style=&#123;&#123;display:&quot;inline-block&quot;,padding: &quot;3px 5px&quot;, color:&quot;#ffffff&quot;, background: &quot;green&quot;, borderRadius: &quot;3px&quot;, cursor: &quot;pointer&quot;&#125;&#125; &gt;click me&lt;/span&gt; &lt;/div&gt; ) &#125;&#125;ReactDOM.render ( ( &lt;MyContainer&gt; &lt;MySub subInfo=&#123;&quot;1&quot;&#125;/&gt; &lt;MySub subInfo=&#123;&quot;2&quot;&#125;/&gt; &lt;/MyContainer&gt; ) , document.getElementById(&apos;content&apos;)) 12345678910111213&lt;!DOCTYPE html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;react drag components example...&lt;/title&gt; &lt;link rel=\"stylesheet\" href=\"/build/main.css\"&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"content\"&gt;&lt;/div&gt; &lt;script src=\"bundle.js\"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","tags":[{"name":"react","slug":"react","permalink":"http://wumuwumu.github.io/tags/react/"}]},{"title":"php5环境搭建","date":"2019-09-02T15:14:11.000Z","path":"2019/09/02/php5环境搭建/","text":"安装nginx12yum install epel-releaseyum install nginx 安装phpremi源可以获取更高的版本，php-fpm是要启动的 123rpm -ivh http://rpms.famillecollet.com/enterprise/remi-release-7.rpmyum install --enablerepo=remi --enablerepo=remi-php56 php php-fpmyum install --enablerepo=remi --enablerepo=remi-php56 php-opcache php-mbstring php-mysql* php-gd php-redis php-mcrypt php-xml php-redis nginx配置123456789101112131415161718server &#123; listen 80; server_name www.test.com test.com; root /data/www/Public; index index.php index.html; location / &#123; try_files $uri $uri/ /index.php?$args; &#125; location ~ index.php &#123; fastcgi_connect_timeout 20s; # default of 60s is just too long fastcgi_read_timeout 20s; # default of 60s is just too long include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_pass 127.0.0.1:9000; # assumes you are running php-fpm locally on port 9000 fastcgi_param PHP_VALUE \"open_basedir=/data/www/:/data/www/Data:/tmp/\"; &#125;&#125; 开启php的日志 修改 php-fpm.conf 文件，添加（或修改）如下配置： 12345[global] error_log = log/error_log [www] catch_workers_output = yes 修改 php.ini 文件，添加（或修改）如下配置： 123log_errors = Onerror_log = &quot;/usr/local/lnmp/php/var/log/error_log&quot;error_reporting=E_ALL&amp;~E_NOTICE 重启 php-fpm","tags":[]},{"title":"linux压缩","date":"2019-09-02T14:46:45.000Z","path":"2019/09/02/linux压缩/","text":"tar12345678910111213141516171819202122232425262728# 打包tar -cvf xx.tar dirName# 解包tar -xvf xx.tar# .gz# 解压gunzip fileName.gzgzip -d fileName.gz# 压缩gzip fileName# .tar.gz 和.tgz# 解压tar zxvf fileName.tar.gz# 压缩tar zcvf filename.tar.gz dirName# bz2# 解压bzip2 -d fileName.bzbunzip2 fileName.bz# .tar.bz# 解压tar jxvf fileName.tar.bz# 压缩tar jcvf fileName.tar.bz dirName zip12345678# 安装yum install zip unzip# 解压unzip mydata.zip -d mydatabak# 压缩zip -r abc123.zip abc 123.txt rar1234567891011# 安装wget http://www.rarlab.com/rar/rarlinux-x64-5.3.0.tar.gztar -zxvf rarlinux-x64-5.3.0.tar.gz // 对应64位下载的cd rarmake# 解压rar x fileName.rar# 压缩rar fileName.rar dirName 7z1234567891011# 安装yum install p7zip p7zip-plugins# 压缩7za a 压缩包.7z 被压缩文件或目录# 解压#将压缩包解压到指定目录，注意：指定目录参数-o后面不要有空格7za x 压缩包.7z -o解压目录#将压缩包解压到当前目录7za x 压缩包.7z","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"nginx伪静态","date":"2019-09-02T14:26:40.000Z","path":"2019/09/02/nginx/nginx伪静态/","text":"伪静态伪静态是一种可以把文件后缀改成任何可能的一种方法，如果我想把PHP文件伪静态成html文件，这种相当简单的。nginx里使用伪静态是直接在nginx.conf 中写规则的，而apache要开启写模块(mod_rewrite)才能进行伪静态。nginx只需要打开nginx.conf配置文件,然后在里面写需要的规则就可以了。 1、Nginx伪静态案例：（Nginx用伪静态是不需要配置的） 找到nginx.conf配置文件：nginx.conf，然后打开，找到server {} 在里面加上： 下面加的意思是隐藏掉index.php： 1234567location / &#123; # 其他的一些规则，自己加 if(!-e $request_filename) &#123; rewrite ^(.*)$ /index.php?s=$1 last; break; &#125;&#125; 2、每个网站独立的配置文件（独立的伪静态规则）： 我们正常的时候每个网站都会有独立的配置文件，直接去改配置文件就好了。然后nginx.conf引入他们所有的配置文件就好了： 如：在nginx.conf配置文件最下面添加以下代码： 1include vhost/*.conf; 说明：引入nginx.conf配置文件所在目录下vhost目录下的所有以.conf的配置文件！ 以下就是其中一个网站的配置文件内容：规则就是隐藏掉index.php 123456789101112131415161718192021222324server &#123; listen 80; root /www/web/admin/public; server_name www.admin.com; index index.html index.php index.htm; error_page 400 /errpage/400.html; error_page 403 /errpage/403.html; error_page 404 /errpage/404.html; error_page 503 /errpage/503.html; location ~ \\.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fcgi.conf; &#125; location ~ /\\.ht &#123; deny all; &#125; location / &#123; if (!-e $request_filename) &#123; rewrite ^(.*)$ /index.php?s=$1 last; break; &#125; &#125;&#125; nginx url重写url重写是指通过配置conf文件，以让网站的url中达到某种状态时则定向/跳转到某个规则，比如常见的伪静态、301重定向、浏览器定向等 rewrite语法在配置文件的server块中写，如： 123server &#123; rewrite 规则 定向路径 重写类型;&#125; 规则：可以是字符串或者正则来表示想匹配的目标url 定向路径：表示匹配到规则后要定向的路径，如果规则里有正则，则可以使用$index来表示正则里的捕获分组 重写类型： last ：相当于Apache里德(L)标记，表示完成rewrite，浏览器地址栏URL地址不变 break；本条规则匹配完成后，终止匹配，不再匹配后面的规则，浏览器地址栏URL地址不变 redirect：返回302临时重定向，浏览器地址会显示跳转后的URL地址 permanent：返回301永久重定向，浏览器地址栏会显示跳转后的URL地址 简单例子1234567891011121314server &#123; # 访问 /last.html 的时候，页面内容重写到 /index.html 中 rewrite /last.html /index.html last; # 访问 /break.html 的时候，页面内容重写到 /index.html 中，并停止后续的匹配 rewrite /break.html /index.html break; # 访问 /redirect.html 的时候，页面直接302定向到 /index.html中 rewrite /redirect.html /index.html redirect; # 访问 /permanent.html 的时候，页面直接301定向到 /index.html中 rewrite /permanent.html /index.html permanent; # 把 /html/*.html =&gt; /post/*.html ，301定向 rewrite ^/html/(.+?).html$ /post/$1.html permanent; # 把 /search/key =&gt; /search.html?keyword=key rewrite ^/search\\/([^\\/]+?)(\\/|$) /search.html?keyword=$1 permanent;&#125; last和break的区别因为301和302不能简单的只返回状态码，还必须有重定向的URL，这就是return指令无法返回301,302的原因了。这里 last 和 break 区别有点难以理解： last一般写在server和if中，而break一般使用在location中 last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配 break和last都能组织继续执行后面的rewrite指令 在location里一旦返回break则直接生效并停止后续的匹配location 123456789server &#123; location / &#123; rewrite /last/ /q.html last; rewrite /break/ /q.html break; &#125; location = /q.html &#123; return 400; &#125;&#125; 访问/last/时重写到/q.html，然后使用新的uri再匹配，正好匹配到locatoin = /q.html然后返回了400 访问/break时重写到/q.html，由于返回了break，则直接停止了 if判断只是上面的简单重写很多时候满足不了需求，比如需要判断当文件不存在时、当路径包含xx时等条件，则需要用到if 语法1if (表达式) &#123;&#125; 当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false 直接比较变量和内容时，使用=或!= ~正则表达式匹配，~*不区分大小写的匹配，!~区分大小写的不匹配 一些内置的条件判断： -f和!-f用来判断是否存在文件 -d和!-d用来判断是否存在目录 -e和!-e用来判断是否存在文件或目录 -x和!-x用来判断文件是否可执行 内置的全局变量123456789101112131415161718192021$args ：这个变量等于请求行中的参数，同$query_string$content_length ： 请求头中的Content-length字段。$content_type ： 请求头中的Content-Type字段。$document_root ： 当前请求在root指令中指定的值。$host ： 请求主机头字段，否则为服务器名称。$http_user_agent ： 客户端agent信息$http_cookie ： 客户端cookie信息$limit_rate ： 这个变量可以限制连接速率。$request_method ： 客户端请求的动作，通常为GET或POST。$remote_addr ： 客户端的IP地址。$remote_port ： 客户端的端口。$remote_user ： 已经经过Auth Basic Module验证的用户名。$request_filename ： 当前请求的文件路径，由root或alias指令与URI请求生成。$scheme ： HTTP方法（如http，https）。$server_protocol ： 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。$server_addr ： 服务器地址，在完成一次系统调用后可以确定这个值。$server_name ： 服务器名称。$server_port ： 请求到达服务器的端口号。$request_uri ： 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。$uri ： 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。$document_uri ： 与$uri相同。 如： 12345678访问链接是：http://localhost:88/test1/test2/test.php 网站路径是：/var/www/html$host：localhost$server_port：88$request_uri：http://localhost:88/test1/test2/test.php$document_uri：/test1/test2/test.php$document_root：/var/www/html$request_filename：/var/www/html/test1/test2/test.php 例子12345678910111213141516# 如果文件不存在则返回400if (!-f $request_filename) &#123; return 400;&#125;# 如果host不是xuexb.com，则301到xuexb.com中if ( $host != 'xuexb.com' )&#123; rewrite ^/(.*)$ https://xuexb.com/$1 permanent;&#125;# 如果请求类型不是POST则返回405if ($request_method = POST) &#123; return 405;&#125;# 如果参数中有 a=1 则301到指定域名if ($args ~ a=1) &#123; rewrite ^ http://example.com/ permanent;&#125; 在某种场景下可结合location规则来使用，如： 1234567891011# 访问 /test.html 时location = /test.html &#123; # 默认值为xiaowu set $name xiaowu; # 如果参数中有 name=xx 则使用该值 if ($args ~* name=(\\w+?)(&amp;|$)) &#123; set $name $1; &#125; # 301 rewrite ^ /$name.html permanent;&#125; 上面表示： /test.html =&gt; /xiaowu.html /test.html?name=ok =&gt; /ok.html?name=ok location语法在server块中使用，如： 123server &#123; location 表达式 &#123; &#125;&#125; location表达式类型 如果直接写一个路径，则匹配该路径下的 ~ 表示执行一个正则匹配，区分大小写 ~* 表示执行一个正则匹配，不区分大小写 ^~ 表示普通字符匹配。使用前缀匹配。如果匹配成功，则不再匹配其他location。 = 进行普通字符精确匹配。也就是完全匹配。 优先级 等号类型（=）的优先级最高。一旦匹配成功，则不再查找其他匹配项。 ^~类型表达式。一旦匹配成功，则不再查找其他匹配项。 正则表达式类型（~ ~*）的优先级次之。如果有多个location的正则能匹配的话，则使用正则表达式最长的那个。 常规字符串匹配类型。按前缀匹配。 例子 - 假地址掩饰真地址1234567891011server &#123; # 用 xxoo_admin 来掩饰 admin location / &#123; # 使用break拿一旦匹配成功则忽略后续location rewrite /xxoo_admin /admin break; &#125; # 访问真实地址直接报没权限 location /admin &#123; return 403; &#125;&#125; 参考https://www.toolnb.com/tools/rewriteTools.html","tags":[{"name":"nginx","slug":"nginx","permalink":"http://wumuwumu.github.io/tags/nginx/"}]},{"title":"spring数据库事务","date":"2019-09-01T12:22:24.000Z","path":"2019/09/01/spring数据库事务/","text":"接口PlatformTransactionManagerPlatformTransactionManager接口中定义了三个方法： 123456789Public interface PlatformTransactionManager()...&#123; // Return a currently active transaction or create a new one, according to the specified propagation behavior（根据指定的传播行为，返回当前活动的事务或创建一个新事务。） TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // Commit the given transaction, with regard to its status（使用事务目前的状态提交事务） Void commit(TransactionStatus status) throws TransactionException; // Perform a rollback of the given transaction（对执行的事务进行回滚） Void rollback(TransactionStatus status) throws TransactionException; &#125; 复制代码 我们刚刚也说了Spring中PlatformTransactionManager根据不同持久层框架所对应的接口实现类,几个比较常见的如下图所示 比如我们在使用JDBC或者iBatis（就是Mybatis）进行数据持久化操作时,我们的xml配置通常如下： 123456&lt;!-- 事务管理器 --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!-- 数据源 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt;&lt;/bean&gt; TransactionDefinition事务管理器接口 PlatformTransactionManager 通过 getTransaction(TransactionDefinition definition) 方法来得到一个事务，这个方法里面的参数是 TransactionDefinition类 ，这个类就定义了一些基本的事务属性。 那么什么是事务属性呢？ 事务属性可以理解成事务的一些基本配置，描述了事务策略如何应用到方法上。事务属性包含了5个方面。 TransactionDefinition接口中的方法如下：TransactionDefinition接口中定义了5个方法以及一些表示事务属性的常量比如隔离级别、传播行为等等的常量。 我下面只是列出了TransactionDefinition接口中的方法而没有给出接口中定义的常量，该接口中的常量信息会在后面依次介绍到。 123456789101112public interface TransactionDefinition &#123; // 返回事务的传播行为 int getPropagationBehavior(); // 返回事务的隔离级别，事务管理器根据它来控制另外一个事务可以看到本事务内的哪些数据 int getIsolationLevel(); // 返回事务必须在多少秒内完成 //返回事务的名字 String getName()； int getTimeout(); // 返回是否优化为只读事务。 boolean isReadOnly();&#125; TransactionStatusPlatformTransactionManager.getTransaction(…) 方法返回一个 TransactionStatus 对象。返回的TransactionStatus 对象可能代表一个新的或已经存在的事务（如果在当前调用堆栈有一个符合条件的事务）。TransactionStatus 接口提供了一个简单的控制事务执行和查询事务状态的方法。该接口定义如清单3所示： 清单3. TransactionStatus 接口中定义的主要方法1`public interface TransactionStatus&#123;`` ``boolean isNewTransaction();`` ``void setRollbackOnly();`` ``boolean isRollbackOnly();``&#125;` 事务管理API分析事务隔离级别隔离级别是指若干个并发的事务之间的隔离程度。TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是TransactionDefinition.ISOLATION_READ_COMMITTED。 TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读和不可重复读，因此很少使用该隔离级别。 TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。 TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。即使在多次查询之间有新增的数据满足该查询，这些新增的记录也会被忽略。该级别可以防止脏读和不可重复读。 TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 事务传播行为所谓事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务性方法的执行行为。在TransactionDefinition定义中包括了如下几个表示传播行为的常量： TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 这里需要指出的是，前面的六种事务传播行为是 Spring 从 EJB 中引入的，他们共享相同的概念。而 PROPAGATION_NESTED是 Spring 所特有的。以 PROPAGATION_NESTED 启动的事务内嵌于外部事务中（如果存在外部事务的话），此时，内嵌事务并不是一个独立的事务，它依赖于外部事务的存在，只有通过外部的事务提交，才能引起内部事务的提交，嵌套的子事务不能单独提交。如果熟悉 JDBC 中的保存点（SavePoint）的概念，那嵌套事务就很容易理解了，其实嵌套的子事务就是保存点的一个应用，一个事务中可以包括多个保存点，每一个嵌套子事务。另外，外部事务的回滚也会导致嵌套子事务的回滚。 事务超时所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。 事务的只读属性事务的只读属性是指，对事务性资源进行只读操作或者是读写操作。所谓事务性资源就是指那些被事务管理的资源，比如数据源、 JMS 资源，以及自定义的事务性资源等等。如果确定只对事务性资源进行只读操作，那么我们可以将事务标志为只读的，以提高事务处理的性能。在 TransactionDefinition 中以 boolean 类型来表示该事务是否只读。 应用场合： 如果你一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持SQL执行期间的读一致性；如果你一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询SQL必须保证整体的读一致性，否则，在前条SQL查询之后，后条SQL查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持。【注意是一次执行多次查询来统计某些信息，这时为了保证数据整体的一致性，要用只读事务】 怎样设置： 对于只读查询，可以指定事务类型为readonly，即只读事务。由于只读事务不存在数据的修改，因此数据库将会为只读事务提供一些优化手段，例如Oracle对于只读事务，不启动回滚段，不记录回滚log。 （1）在JDBC中，指定只读事务的办法为： connection.setReadOnly(true); （2）在Hibernate中，指定只读事务的办法为： session.setFlushMode(FlushMode.NEVER);此时，Hibernate也会为只读事务提供Session方面的一些优化手段 （3）在Spring的Hibernate封装中，指定只读事务的办法为： bean配置文件中，prop属性增加“readOnly”或者用注解方式@Transactional(readOnly=true)【 if the transaction is marked as read-only, Spring will set the Hibernate Session’s flush mode to FLUSH_NEVER,and will set the JDBC transaction to read-only】也就是说在Spring中设置只读事务是利用上面两种方式 事务的回滚规则通常情况下，如果在事务中抛出了未检查异常（继承自 RuntimeException 的异常），则默认将回滚事务。如果没有抛出任何异常，或者抛出了已检查异常，则仍然提交事务。这通常也是大多数开发者希望的处理方式，也是 EJB 中的默认处理方式。但是，我们可以根据需要人为控制事务在抛出某些未检查异常时任然提交事务，或者在抛出某些已检查异常时回滚事务。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"},{"name":"spring","slug":"spring","permalink":"http://wumuwumu.github.io/tags/spring/"}]},{"title":"数据库事务","date":"2019-09-01T12:18:43.000Z","path":"2019/09/01/mysql/数据库事务/","text":"本篇讲诉数据库中事务的四大特性（ACID），并且将会详细地说明事务的隔离级别。 如果一个数据库声称支持事务的操作，那么该数据库必须要具备以下四个特性： ⑴ 原子性（Atomicity） 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，这和前面两篇博客介绍事务的功能是一样的概念，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 ⑵ 一致性（Consistency） 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 ⑶ 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 关于事务的隔离性数据库提供了多种隔离级别，稍后会介绍到。 ⑷ 持久性（Durability） 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 以上介绍完事务的四大特性(简称ACID)，现在重点来说明下事务的隔离性，当多个线程都开启事务操作数据库中的数据时，数据库系统要能进行隔离操作，以保证各个线程获取数据的准确性，在介绍数据库提供的各种隔离级别之前，我们先看看如果不考虑事务的隔离性，会发生的几种问题： 1，脏读 脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。 当一个事务正在多次修改某个数据，而在这个事务中这多次的修改都还未提交，这时一个并发的事务来访问该数据，就会造成两个事务得到的数据不一致。例如：用户A向用户B转账100元，对应SQL命令如下 123update account set money=money+100 where name=’B’; (此时A通知B)update account set money=money - 100 where name=’A’; 当只执行第一条SQL时，A通知B查看账户，B发现确实钱已到账（此时即发生了脏读），而之后无论第二条SQL是否执行，只要该事务不提交，则所有操作都将回滚，那么当B以后再次查看账户时就会发现钱其实并没有转。 2，不可重复读 不可重复读是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。 例如事务T1在读取某一数据，而事务T2立马修改了这个数据并且提交事务给数据库，事务T1再次读取该数据就得到了不同的结果，发送了不可重复读。 不可重复读和脏读的区别是，脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。 在某些情况下，不可重复读并不是问题，比如我们多次查询某个数据当然以最后查询得到的结果为主。但在另一些情况下就有可能发生问题，例如对于同一个数据A和B依次查询就可能不同，A和B就可能打起来了…… 3，虚读(幻读) 幻读是事务非独立执行时发生的一种现象。例如事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有一行没有修改，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。 幻读和不可重复读都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 现在来看看MySQL数据库为我们提供的四种隔离级别： ① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 ② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 ③ Read committed (读已提交)：可避免脏读的发生。 ④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。 以上四种隔离级别最高的是Serializable级别，最低的是Read uncommitted级别，当然级别越高，执行效率就越低。像Serializable这样的级别，就是以锁表的方式(类似于Java多线程中的锁)使得其他的线程只能在锁外等待，所以平时选用何种隔离级别应该根据实际情况。在MySQL数据库中默认的隔离级别为Repeatable read (可重复读)。 在MySQL数据库中，支持上面四种隔离级别，默认的为Repeatable read (可重复读)；而在Oracle数据库中，只支持Serializable (串行化)级别和Read committed (读已提交)这两种级别，其中默认的为Read committed级别。 在MySQL数据库中查看当前事务的隔离级别： 1select @@tx_isolation; 在MySQL数据库中设置事务的隔离 级别： 12set [glogal | session] transaction isolation level 隔离级别名称;set tx_isolation=’隔离级别名称;’ 后记：隔离级别的设置只对当前链接有效。对于使用MySQL命令窗口而言，一个窗口就相当于一个链接，当前窗口设置的隔离级别只对当前窗口中的事务有效；对于JDBC操作数据库来说，一个Connection对象相当于一个链接，而对于Connection对象设置的隔离级别只对该Connection对象有效，与其他链接Connection对象无关。 参考博客：http://www.zhihu.com/question/23989904http://dev.mysql.com/doc/refman/5.6/en/set-transaction.htmlhttp://www.cnblogs.com/xdp-gacl/p/3984001.htmlhttps://www.cnblogs.com/fjdingsd/p/5273008.html","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"mysql性能检测","date":"2019-08-31T15:26:45.000Z","path":"2019/08/31/mysql/mysql性能检测/","text":"性能检测蝉蛹命令 show status show processlist show variables 瓶颈分析常用命令获取mysql用户下的进程总数1ps -ef | awk '&#123;print $1&#125;' | grep \"mysql\" | grep -v \"grep\" | wc -l 主机性能状态1uptime CPU使用率12topvmstat 磁盘IO量12vmstatiostat swap进出量1free -m 数据库性能状态QPS方法一 基于 questions 计算qps,基于 com_commit com_rollback 计算tps 12345questions = show global status like 'questions';uptime = show global status like 'uptime';qps=questions/uptime 1234567com_commit = show global status like 'com_commit';com_rollback = show global status like 'com_rollback';uptime = show global status like 'uptime';tps=(com_commit + com_rollback)/uptime 方法二 基于 com_* 的status 变量计算tps ,qps 使用如下命令： 1234567891011show global status where variable_name in('com_select','com_insert','com_delete','com_update');获取间隔1s 的 com_*的值，并作差值运算del_diff = (int(mystat2['com_delete']) - int(mystat1['com_delete']) ) / diffins_diff = (int(mystat2['com_insert']) - int(mystat1['com_insert']) ) / diffsel_diff = (int(mystat2['com_select']) - int(mystat1['com_select']) ) / diffupd_diff = (int(mystat2['com_update']) - int(mystat1['com_update']) ) / diff 总结： Questions 是记录了从mysqld启动以来所有的select，dml 次数包括show 命令的查询的次数。这样多少有失准确性，比如很多数据库有监控系统在运行，每5秒对数据库进行一次show 查询来获取当前数据库的状态，而这些查询就被记录到QPS,TPS统计中，造成一定的”数据污染”. 如果数据库中存在比较多的myisam表，则计算还是questions 比较合适。 如果数据库中存在比较多的innodb表，则计算以com_*数据来源比较合适 TPSTPS = (Com_commit + Com_rollback) / seconds 12show status like 'Com_commit'; show status like 'Com_rollback'; key Buffer 命中率key_buffer_read_hits = (1-key_reads / key_read_requests) 100%key_buffer_write_hits = (1-key_writes / key_write_requests) 100% 1show status like 'Key%'; InnoDB Buffer命中率innodb_buffer_read_hits = (1 - innodb_buffer_pool_reads / innodb_buffer_pool_read_requests) * 100% 1show status like 'innodb_buffer_pool_read%'; Query Cache命中率Query_cache_hits = (Qcahce_hits / (Qcache_hits + Qcache_inserts )) * 100%; 1show status like 'Qcache%'; Table Cache状态量1show status like 'open%'; Thread Cache 命中率Thread_cache_hits = (1 - Threads_created / connections ) * 100% 12show status like 'Thread%';show status like 'Connections'; 锁定状态1show status like '%lock%'; 复制延时量1show slave status; Tmp Table 状况(临时表状况)1show status like 'Create_tmp%'; Binlog Cache 使用状况1show status like 'Binlog_cache%'; Innodb_log_waits1show status like 'innodb_log_waits'; 参考https://blog.csdn.net/li_adou/article/details/78791972","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"hibernate_Embedded和@Embeddable","date":"2019-08-10T02:57:59.000Z","path":"2019/08/10/java/hibernate-@Embedded和-Embeddable/","text":"在使用实体类生成对应的数据库表时，很多的时候都会遇到这种情况：在一个实体类中引用另外的实体类，一般遇上这种情况，我们使用@OneToOne、@OneToMany、@ManyToOne、@ManyToMany这4个注解比较多，但是好奇害死猫，除了这四个有没有别的使用情况，尤其是一个实体类要在多个不同的实体类中进行使用，而本身又不需要独立生成一个数据库表，这就是需要@Embedded、@Embeddable的时候了，下面分成4类来说明在一个实体类中引用另外的实体类的情况，具体的数据库环境是MySQL 5.7。 使用的两个实体类如下： Address类123456789public class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; private String country; private String province; private String city; private String detail; //setter、getter&#125; Person类：123456789101112131415161718@Entitypublic class Person implements Serializable&#123; private static final long serialVersionUID = 8849870114127659929L; @Id @GeneratedValue private Long id; @Column(nullable = false) private String name; @Column(nullable = false) private Integer age; private Address address; //setter、getter&#125; 两个注解全不使用当这两个注解都不使用时，那么两个实体类和上面的相同，那么生成的表结构如下： Address属性字段会映射成tinyblob类型的字段，这是用来存储不超过255字符的二进制字符串的数据类型，显然我们通常不会这么使用。 只使用@Embeddable我们在Address实体类上加上@Embeddable注解，变成如下类： 1234567891011@Embeddablepublic class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; private String country; private String province; private String city; private String detail; //setter、getter&#125; 而Person实体类不变，生成的数据库表结构如下： 可以看出这次是把Address中的字段映射成数据库列嵌入到Person表中了，而这些字段的类型和长度也使用默认值。如果我们在Address中的字段中设置列的相关属性，则会按照我们设定的值去生成，如下Address类：1234567891011121314@Embeddablepublic class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; @Column(nullable = false) private String country; @Column(length = 30) private String province; @Column(unique = true) private String city; @Column(length = 50) private String detail; //setter、getter&#125; 生成的表结构如下： 我们在Address中配置的属性全部成功映射到Person表中。 只使用@Embedded这里我们只在Person中使用@Embedded,如下：12345678910111213141516171819@Entitypublic class Person implements Serializable&#123; private static final long serialVersionUID = 8849870114127659929L; @Id @GeneratedValue private Long id; @Column(nullable = false) private String name; @Column(nullable = false) private Integer age; @Embedded private Address address; //setter、getter&#125; Adddress类和最开始的不同POJO类相同，此时生成的表结构如下： 可以看出这个表结构和在Address中只使用@Embeddable注解时相同，在进入深一步试验，我们在Address中加入列属性，但是不使用@Embeddable注解会发生什么？Address类如下：12345678910111213public class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; @Column(nullable = false) private String country; @Column(length = 30) private String province; @Column(unique = true) private String city; @Column(length = 50) private String detail; //setter、getter&#125; 生成数据表结构如下： 所以只使用@Embedded和只使用@Embeddable产生的效果是相同的。 两个注解全使用既然单独使用@Embedded或者只使用@Embeddable都会产生作用，那么这两个都使用效果也一定是一样的，我们平时也是这么用的。所以在这部分我们就不演示和上面相同的效果了，而是说两个深入的话题。 覆盖@Embeddable类中字段的列属性这里就要使用另外的两个注解@AttributeOverrides和@AttributeOverride，这两个注解是用来覆盖@Embeddable类中字段的属性的。 @AttributeOverrides：里面只包含了@AttributeOverride类型数组；@AttributeOverride：包含要覆盖的@Embeddable类中字段名name和新增的@Column字段的属性；使用如下：Person类如下：123456789101112131415161718192021@Entitypublic class Person implements Serializable&#123; private static final long serialVersionUID = 8849870114127659929L; @Id @GeneratedValue private Long id; @Column(nullable = false) private String name; @Column(nullable = false) private Integer age; @Embedded @AttributeOverrides(&#123;@AttributeOverride(name=\"country\", column=@Column(name = \"person_country\", length = 25, nullable = false)), @AttributeOverride(name=\"city\", column = @Column(name = \"person_city\", length = 15))&#125;) private Address address; //setter、getter&#125; Address类如下：1234567891011121314@Embeddablepublic class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; @Column(nullable = false) private String country; @Column(length = 30) private String province; @Column(unique = true) private String city; @Column(length = 50) private String detail; //setter、getter&#125; 生成的数据表如下： 可以看出我们的@AttributeOverrides和@AttributeOverride两个注解起作用了。 多层嵌入实体类属性上面所有的例子都是使用两层实体类嵌入，其实这种实体类的嵌入映射是可以使用多层的，具体的例子如下。我们新建立一个类Direction表示方位如下：1234567@Embeddablepublic class Direction implements Serializable&#123; @Column(nullable = false) private Integer longitude; private Integer latitude;&#125; Address如下：12345678910111213141516@Embeddablepublic class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; @Column(nullable = false) private String country; @Column(length = 30) private String province; @Column(unique = true) private String city; @Column(length = 50) private String detail; @Embedded private Direction direction;&#125; Person类如下：12345678910111213141516171819@Entitypublic class Person implements Serializable&#123; private static final long serialVersionUID = 8849870114127659929L; @Id @GeneratedValue private Long id; @Column(nullable = false) private String name; @Column(nullable = false) private Integer age; @Embedded @AttributeOverrides(&#123;@AttributeOverride(name=\"direction.latitude\", column=@Column(name = \"person_latitude\")), @AttributeOverride(name=\"direction.longitude\", column = @Column(name = \"person_longitude\"))&#125;) private Address address;&#125; 生成的数据表如下： 在上面需要注意如下几点：在Person中定义Direction中的属性时，需要用”.”将所有相关的属性连接起来；在Direction中longitude属性定义为not null，但是由于使用了@AttributeOverride注解，其中虽然没有定义null属性，但是这时使用的是默认的nullable属性，默认为true; 参考 https://blog.csdn.net/lmy86263/article/details/52108130","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"create-react-app脚手架","date":"2019-08-07T01:38:30.000Z","path":"2019/08/07/react/create-react-app脚手架/","text":"安装12345npm install -g create-react-app# 切记项目名称不能大写create-react-app firstappcd firstappnpm run start eject这是一次性的操作 1npm run eject 启动less或者sasssasscreate-react-app默认有sass的配置，只需要安装依赖就行 1npm install node-sass --save less默认没有less的配置，需要自己在webpack中配置 安装依赖 1npm install less less-loader --save 运行完成之后，打开 config 目录下的 webpack.config.js 文件，找到 // style files regexes 注释位置，仿照其解析 sass 的规则，在下面添加两行代码 1234// 添加 less 解析规则const lessRegex = /\\.less$/;const lessModuleRegex = /\\.module\\.less$/;复制代码 找到 rules 属性配置，在其中添加 less 解析配置 !!!注意： 这里有一个需要注意的地方，下面的这些 less 配置规则放在 sass 的解析规则下面即可，如果放在了 file-loader 的解析规则下面，less 文件解析不会生效。 12345678910111213141516171819202122232425// Less 解析配置&#123; test: lessRegex, exclude: lessModuleRegex, use: getStyleLoaders( &#123; importLoaders: 2, sourceMap: isEnvProduction &amp;&amp; shouldUseSourceMap, &#125;, 'less-loader' ), sideEffects: true,&#125;,&#123; test: lessModuleRegex, use: getStyleLoaders( &#123; importLoaders: 2, sourceMap: isEnvProduction &amp;&amp; shouldUseSourceMap, modules: true, getLocalIdent: getCSSModuleLocalIdent, &#125;, 'less-loader' )&#125;, css module在css的命名中使用*.module.css就可以使用css module，也可以自己修改webpack的文件。 参考 https://www.jianshu.com/p/1f054623ecac","tags":[{"name":"react","slug":"react","permalink":"http://wumuwumu.github.io/tags/react/"}]},{"title":"python-类","date":"2019-08-04T10:12:22.000Z","path":"2019/08/04/python/python-类/","text":"类中默认函数new和init区别new:创建对象时调用，会返回当前对象的一个实例 init:创建完对象后调用，对当前对象的一些实例初始化，无返回值 1、在类中，如果new和init同时存在，会优先调用new 12345678class Data(object): def __new__(self): print \"new\" def __init__(self): print \"init\" data = Data()# new 2、new方法会返回所构造的对象，init则不会。init无返回值。 12345678910111213class Data(object): def __init__(cls): cls.x = 2 print \"init\" return clsdata = Data()'''initTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: __init__() should return None, not 'Data'''' 12345678910111213141516class Data(object): def __new__(cls): print(\"new\") cls.x = 1 return clsdef __init__(self): print(\"init\")data = Data()print(data.x)# new# 1data.x =2print(data.x)# 2 If new() returns an instance of cls, then the new instance’s init() method will be invoked like init(self[, …]), where self is the new instance and the remaining arguments are the same as were passed to new(). 如果new返回一个对象的实例，会隐式调用init If new() does not return an instance of cls, then the new instance’s init() method will not be invoked. 如果new不返回一个对象的实例，init不会被调用 123456789101112131415161718192021class A(object): def __new__(Class): object = super(A,Class).__new__(Class) print \"in New\" return object def __init__(self): print \"in init\" A()# in New# in initclass A(object): def __new__(cls): print \"in New\" return cls def __init__(self): print \"in init\" a = A() # in New object.init(self[, …])Called when the instance is created. The arguments are those passed to the class constructor expression. If a base class has an init() method, the derived class’s init() method, if any, must explicitly call it to ensure proper initialization of the base class part of the instance; for example: BaseClass.init(self, [args…]). As a special constraint on constructors, no value may be returned; doing so will cause a TypeError to be raised at runtime. 在对象的实例创建完成后调用。参数被传给类的构造函数。如果基类有init方法，子类必须显示调用基类的init。 没有返回值，否则会再引发TypeError错误。","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"spring的jms事务","date":"2019-08-03T07:05:17.000Z","path":"2019/08/03/java/spring的jms事务/","text":"","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"},{"name":"spring","slug":"spring","permalink":"http://wumuwumu.github.io/tags/spring/"}]},{"title":"spring事务实现","date":"2019-08-03T06:40:33.000Z","path":"2019/08/03/java/spring事务实现/","text":"事务概念回顾 什么是事务？ 事务是逻辑上的一组操作，要么都执行，要么都不执行. 事物的特性（ACID）： 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致； 隔离性： 并发访问数据库时，一个用户的事物不被其他事物所干扰，各并发事务之间数据库是独立的； 持久性: 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 Spring事务管理接口介绍 Spring事务管理接口： PlatformTransactionManager： （平台）事务管理器 TransactionDefinition： 事务定义信息(事务隔离级别、传播行为、超时、只读、回滚规则) TransactionStatus： 事务运行状态 所谓事务管理，其实就是“按照给定的事务规则来执行提交或者回滚操作”。 PlatformTransactionManager接口介绍 Spring并不直接管理事务，而是提供了多种事务管理器 ，他们将事务管理的职责委托给Hibernate或者JTA等持久化机制所提供的相关平台框架的事务来实现。 Spring事务管理器的接口是： org.springframework.transaction.PlatformTransactionManager ，通过这个接口，Spring为各个平台如JDBC、Hibernate等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。 PlatformTransactionManager接口代码如下：PlatformTransactionManager接口中定义了三个方法： 123456789Public interface PlatformTransactionManager()...&#123; // Return a currently active transaction or create a new one, according to the specified propagation behavior（根据指定的传播行为，返回当前活动的事务或创建一个新事务。） TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // Commit the given transaction, with regard to its status（使用事务目前的状态提交事务） Void commit(TransactionStatus status) throws TransactionException; // Perform a rollback of the given transaction（对执行的事务进行回滚） Void rollback(TransactionStatus status) throws TransactionException; &#125; 复制代码 我们刚刚也说了Spring中PlatformTransactionManager根据不同持久层框架所对应的接口实现类,几个比较常见的如下图所示 比如我们在使用JDBC或者iBatis（就是Mybatis）进行数据持久化操作时,我们的xml配置通常如下： 1234567 &lt;!-- 事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- 数据源 --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;/bean&gt;复制代码 TransactionDefinition接口介绍 事务管理器接口 PlatformTransactionManager 通过 getTransaction(TransactionDefinition definition) 方法来得到一个事务，这个方法里面的参数是 TransactionDefinition类 ，这个类就定义了一些基本的事务属性。 那么什么是事务属性呢？ 事务属性可以理解成事务的一些基本配置，描述了事务策略如何应用到方法上。事务属性包含了5个方面。 TransactionDefinition接口中的方法如下：TransactionDefinition接口中定义了5个方法以及一些表示事务属性的常量比如隔离级别、传播行为等等的常量。 我下面只是列出了TransactionDefinition接口中的方法而没有给出接口中定义的常量，该接口中的常量信息会在后面依次介绍到。 12345678910111213public interface TransactionDefinition &#123; // 返回事务的传播行为 int getPropagationBehavior(); // 返回事务的隔离级别，事务管理器根据它来控制另外一个事务可以看到本事务内的哪些数据 int getIsolationLevel(); // 返回事务必须在多少秒内完成 //返回事务的名字 String getName()； int getTimeout(); // 返回是否优化为只读事务。 boolean isReadOnly();&#125; 复制代码 （1）事务隔离级别（定义了一个事务可能受其他并发事务影响的程度）：我们先来看一下 并发事务带来的问题 ，然后再来介绍一下 TransactionDefinition 接口 中定义了五个表示隔离级别的常量。 并发事务带来的问题 在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对统一数据进行操作）。并发虽然是必须的，但可能会导致一下的问题。 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复度和幻读区别： 不可重复读的重点是修改，幻读的重点在于新增或者删除。 例1（同样的条件, 你读取过的数据, 再次读取出来发现值不一样了 ）：事务1中的A先生读取自己的工资为 1000的操作还没完成，事务2中的B先生就修改了A的工资为2000，导 致A再读自己的工资时工资变为 2000；这就是不可重复读。 例2（同样的条件, 第1次和第2次读出来的记录数不一样 ）：假某工资单表中工资大于3000的有4人，事务1读取了所有工资大于3000的人，共查到4条记录，这时事务2 又插入了一条工资大于3000的记录，事务1再次读取时查到的记录就变为了5条，这样就导致了幻读。 隔离级别 TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 （2）事务传播行为（为了解决业务层方法之间互相调用的事务问题）：当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。在TransactionDefinition定义中包括了如下几个表示传播行为的常量： 支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 这里需要指出的是，前面的六种事务传播行为是 Spring 从 EJB 中引入的，他们共享相同的概念。而 PROPAGATION_NESTED 是 Spring 所特有的。以 PROPAGATION_NESTED 启动的事务内嵌于外部事务中（如果存在外部事务的话），此时，内嵌事务并不是一个独立的事务，它依赖于外部事务的存在，只有通过外部的事务提交，才能引起内部事务的提交，嵌套的子事务不能单独提交。如果熟悉 JDBC 中的保存点（SavePoint）的概念，那嵌套事务就很容易理解了，其实嵌套的子事务就是保存点的一个应用，一个事务中可以包括多个保存点，每一个嵌套子事务。另外，外部事务的回滚也会导致嵌套子事务的回滚。 (3) 事务超时属性(一个事务允许执行的最长时间)所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。 (4) 事务只读属性（对事物资源是否执行只读操作）事务的只读属性是指，对事务性资源进行只读操作或者是读写操作。所谓事务性资源就是指那些被事务管理的资源，比如数据源、 JMS 资源，以及自定义的事务性资源等等。如果确定只对事务性资源进行只读操作，那么我们可以将事务标志为只读的，以提高事务处理的性能。在 TransactionDefinition 中以 boolean 类型来表示该事务是否只读。 (5) 回滚规则（定义事务回滚规则）例子使用API下面给出一个基于底层 API 的编程式事务管理的示例，基于PlatformTransactionManager、TransactionDefinition 和 TransactionStatus 三个核心接口，我们完全可以通过编程的方式来进行事务管理。 1234567891011121314151617181920public class BankServiceImpl implements BankService &#123; private BankDao bankDao; private TransactionDefinition txDefinition; private PlatformTransactionManager txManager;public boolean transfer(Long fromId， Long toId， double amount) &#123; // 获取一个事务 TransactionStatus txStatus = txManager.getTransaction(txDefinition); boolean result = false; try &#123; result = bankDao.transfer(fromId， toId， amount); txManager.commit(txStatus); // 事务提交 &#125; catch (Exception e) &#123; result = false; txManager.rollback(txStatus); // 事务回滚 System.out.println(\"Transfer Error!\"); &#125; return result;&#125;相应的配置文件如下所示： 123456789&lt;bean id=\"bankService\" class=\"footmark.spring.core.tx.programmatic.origin.BankServiceImpl\"&gt; &lt;property name=\"bankDao\" ref=\"bankDao\"/&gt; &lt;property name=\"txManager\" ref=\"transactionManager\"/&gt; &lt;property name=\"txDefinition\"&gt; &lt;bean class=\"org.springframework.transaction.support.DefaultTransactionDefinition\"&gt; &lt;property name=\"propagationBehaviorName\" value=\"PROPAGATION_REQUIRED\"/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt;如上所示，我们在BankServiceImpl类中增加了两个属性：一个是 TransactionDefinition 类型的属性，它用于定义事务的规则；另一个是 PlatformTransactionManager 类型的属性，用于执行事务管理操作。如果一个业务方法需要添加事务，我们首先需要在方法开始执行前调用PlatformTransactionManager.getTransaction(…) 方法启动一个事务；创建并启动了事务之后，便可以开始编写业务逻辑代码，然后在适当的地方执行事务的提交或者回滚。 基于 TransactionTemplate 的编程式事务管理 当然，除了可以使用基于底层 API 的编程式事务外，还可以使用基于 TransactionTemplate 的编程式事务管理。通过上面的示例可以发现，上述事务管理的代码散落在业务逻辑代码中，破坏了原有代码的条理性，并且每一个业务方法都包含了类似的启动事务、提交/回滚事务的样板代码。Spring 也意识到了这些，并提供了简化的方法，这就是 Spring 在数据访问层非常常见的 模板回调模式。 1234567891011121314151617181920public class BankServiceImpl implements BankService &#123; private BankDao bankDao; private TransactionTemplate transactionTemplate; ...... public boolean transfer(final Long fromId， final Long toId， final double amount) &#123; return (Boolean) transactionTemplate.execute(new TransactionCallback()&#123; public Object doInTransaction(TransactionStatus status) &#123; Object result; try &#123; result = bankDao.transfer(fromId， toId， amount); &#125; catch (Exception e) &#123; status.setRollbackOnly(); result = false; System.out.println(\"Transfer Error!\"); &#125; return result; &#125; &#125;); &#125;&#125; 相应的配置文件如下所示： 1234&lt;bean id=\"bankService\" class=\"footmark.spring.core.tx.programmatic.template.BankServiceImpl\"&gt; &lt;property name=\"bankDao\" ref=\"bankDao\"/&gt; &lt;property name=\"transactionTemplate\" ref=\"transactionTemplate\"/&gt;&lt;/bean&gt; TransactionTemplate 的 execute() 方法有一个 TransactionCallback 类型的参数，该接口中定义了一个 doInTransaction() 方法，通常我们以匿名内部类的方式实现 TransactionCallback 接口，并在其 doInTransaction() 方法中书写业务逻辑代码。这里可以使用默认的事务提交和回滚规则，这样在业务代码中就不需要显式调用任何事务管理的 API。doInTransaction() 方法有一个TransactionStatus 类型的参数，我们可以在方法的任何位置调用该参数的 setRollbackOnly() 方法将事务标识为回滚的，以执行事务回滚。 ​ 此外，TransactionCallback 接口有一个子接口 TransactionCallbackWithoutResult，该接口中定义了一个 doInTransactionWithoutResult() 方法，TransactionCallbackWithoutResult 接口主要用于事务过程中不需要返回值的情况。当然，对于不需要返回值的情况，我们仍然可以使用 TransactionCallback 接口，并在方法中返回任意值即可。 基于底层 API 的编程式事务管理 下面给出一个基于底层 API 的编程式事务管理的示例，基于PlatformTransactionManager、TransactionDefinition 和 TransactionStatus 三个核心接口，我们完全可以通过编程的方式来进行事务管理。 12345678910111213141516171819public class BankServiceImpl implements BankService &#123; private BankDao bankDao; private TransactionDefinition txDefinition; private PlatformTransactionManager txManager; public boolean transfer(Long fromId， Long toId， double amount) &#123; // 获取一个事务 TransactionStatus txStatus = txManager.getTransaction(txDefinition); boolean result = false; try &#123; result = bankDao.transfer(fromId， toId， amount); txManager.commit(txStatus); // 事务提交 &#125; catch (Exception e) &#123; result = false; txManager.rollback(txStatus); // 事务回滚 System.out.println(\"Transfer Error!\"); &#125; return result;&#125;相应的配置文件如下所示： 123456789&lt;bean id=\"bankService\" class=\"footmark.spring.core.tx.programmatic.origin.BankServiceImpl\"&gt; &lt;property name=\"bankDao\" ref=\"bankDao\"/&gt; &lt;property name=\"txManager\" ref=\"transactionManager\"/&gt; &lt;property name=\"txDefinition\"&gt; &lt;bean class=\"org.springframework.transaction.support.DefaultTransactionDefinition\"&gt; &lt;property name=\"propagationBehaviorName\" value=\"PROPAGATION_REQUIRED\"/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 如上所示，我们在BankServiceImpl类中增加了两个属性：一个是 TransactionDefinition 类型的属性，它用于定义事务的规则；另一个是 PlatformTransactionManager 类型的属性，用于执行事务管理操作。如果一个业务方法需要添加事务，我们首先需要在方法开始执行前调用PlatformTransactionManager.getTransaction(…) 方法启动一个事务；创建并启动了事务之后，便可以开始编写业务逻辑代码，然后在适当的地方执行事务的提交或者回滚。 基于 TransactionTemplate 的编程式事务管理 当然，除了可以使用基于底层 API 的编程式事务外，还可以使用基于 TransactionTemplate 的编程式事务管理。通过上面的示例可以发现，上述事务管理的代码散落在业务逻辑代码中，破坏了原有代码的条理性，并且每一个业务方法都包含了类似的启动事务、提交/回滚事务的样板代码。Spring 也意识到了这些，并提供了简化的方法，这就是 Spring 在数据访问层非常常见的 模板回调模式。 1234567891011121314151617181920public class BankServiceImpl implements BankService &#123; private BankDao bankDao; private TransactionTemplate transactionTemplate; ...... public boolean transfer(final Long fromId， final Long toId， final double amount) &#123; return (Boolean) transactionTemplate.execute(new TransactionCallback()&#123; public Object doInTransaction(TransactionStatus status) &#123; Object result; try &#123; result = bankDao.transfer(fromId， toId， amount); &#125; catch (Exception e) &#123; status.setRollbackOnly(); result = false; System.out.println(\"Transfer Error!\"); &#125; return result; &#125; &#125;); &#125;&#125; 相应的配置文件如下所示： 1234&lt;bean id=\"bankService\" class=\"footmark.spring.core.tx.programmatic.template.BankServiceImpl\"&gt; &lt;property name=\"bankDao\" ref=\"bankDao\"/&gt; &lt;property name=\"transactionTemplate\" ref=\"transactionTemplate\"/&gt;&lt;/bean&gt; TransactionTemplate 的 execute() 方法有一个 TransactionCallback 类型的参数，该接口中定义了一个 doInTransaction() 方法，通常我们以匿名内部类的方式实现 TransactionCallback 接口，并在其 doInTransaction() 方法中书写业务逻辑代码。这里可以使用默认的事务提交和回滚规则，这样在业务代码中就不需要显式调用任何事务管理的 API。doInTransaction() 方法有一个TransactionStatus 类型的参数，我们可以在方法的任何位置调用该参数的 setRollbackOnly() 方法将事务标识为回滚的，以执行事务回滚。 此外，TransactionCallback 接口有一个子接口 TransactionCallbackWithoutResult，该接口中定义了一个 doInTransactionWithoutResult() 方法，TransactionCallbackWithoutResult 接口主要用于事务过程中不需要返回值的情况。当然，对于不需要返回值的情况，我们仍然可以使用 TransactionCallback 接口，并在方法中返回任意值即可。 Spring 声明式事务管理 Spring 的声明式事务管理是建立在 Spring AOP 机制之上的，其本质是对目标方法前后进行拦截，并在目标方法开始之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。 声明式事务最大的优点就是不需要通过编程的方式管理事务，这样就不需要在业务逻辑代码中掺杂事务管理的代码，只需在配置文件中作相关的事务规则声明（或通过等价的基于标注的方式），便可以将事务规则应用到业务逻辑中。总的来说，声明式事务得益于 Spring IoC容器 和 Spring AOP 机制的支持：IoC容器为声明式事务管理提供了基础设施，使得 Bean 对于 Spring 框架而言是可管理的；而由于事务管理本身就是一个典型的横切逻辑（正是 AOP 的用武之地），因此 Spring AOP 机制是声明式事务管理的直接实现者。 显然，声明式事务管理要优于编程式事务管理，这正是spring倡导的非侵入式的开发方式。声明式事务管理使业务代码不受污染，一个普通的POJO对象，只要在XML文件中配置或者添加注解就可以获得完全的事务支持。因此，通常情况下，笔者强烈建议在开发中使用声明式事务，不仅因为其简单，更主要是因为这样使得纯业务代码不被污染，极大方便后期的代码维护。 基于 命名空间的声明式事务管理 Spring 2.x 引入了 命名空间，结合使用 命名空间，带给开发人员配置声明式事务的全新体验，配置变得更加简单和灵活。总的来说，开发者只需基于和命名空间在XML中进行简答配置便可实现声明式事务管理。下面基于使用Hibernate事务管理的配置文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;!-- 配置 DataSourece --&gt;&lt;bean id=\"dataSource\" class=\"org.apache.commons.dbcp.BasicDataSource\" destroy-method=\"close\"&gt; &lt;!-- results in a setDriverClassName(String) call --&gt; &lt;property name=\"driverClassName\"&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property name=\"url\"&gt; &lt;value&gt;jdbc:mysql://localhost:3306/ssh&lt;/value&gt; &lt;/property&gt; &lt;property name=\"username\"&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property name=\"password\"&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置 sessionFactory --&gt;&lt;bean id=\"sessionFactory\" class=\"org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean\"&gt; &lt;!-- 数据源的设置 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;!-- 用于持久化的实体类类列表 --&gt; &lt;property name=\"annotatedClasses\"&gt; &lt;list&gt; &lt;value&gt;cn.edu.tju.rico.model.entity.User&lt;/value&gt; &lt;value&gt;cn.edu.tju.rico.model.entity.Log&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- Hibernate 的配置 --&gt; &lt;property name=\"hibernateProperties\"&gt; &lt;props&gt; &lt;!-- 方言设置 --&gt; &lt;prop key=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;!-- 显示sql --&gt; &lt;prop key=\"hibernate.show_sql\"&gt;true&lt;/prop&gt; &lt;!-- 格式化sql --&gt; &lt;prop key=\"hibernate.format_sql\"&gt;true&lt;/prop&gt; &lt;!-- 自动创建/更新数据表 --&gt; &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;update&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置 TransactionManager --&gt;&lt;bean id=\"txManager\" class=\"org.springframework.orm.hibernate3.HibernateTransactionManager\"&gt; &lt;property name=\"sessionFactory\" ref=\"sessionFactory\" /&gt;&lt;/bean&gt;&lt;!-- 配置事务增强处理的切入点，以保证其被恰当的织入 --&gt; &lt;aop:config&gt; &lt;!-- 切点 --&gt; &lt;aop:pointcut expression=\"execution(* cn.edu.tju.rico.service.impl.*.*(..))\" id=\"bussinessService\" /&gt; &lt;!-- 声明式事务的切入 --&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"bussinessService\" /&gt;&lt;/aop:config&gt;&lt;!-- 由txAdvice切面定义事务增强处理 --&gt;&lt;tx:advice id=\"txAdvice\" transaction-manager=\"txManager\"&gt; &lt;tx:attributes&gt; &lt;!-- get打头的方法为只读方法,因此将read-only设为 true --&gt; &lt;tx:method name=\"get*\" read-only=\"true\" /&gt; &lt;!-- 其他方法为读写方法,因此将read-only设为 false --&gt; &lt;tx:method name=\"*\" read-only=\"false\" propagation=\"REQUIRED\" isolation=\"DEFAULT\" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 事实上，Spring配置文件中关于事务的配置总是由三个部分组成，即：DataSource、TransactionManager和代理机制三部分，无论哪种配置方式，一般变化的只是代理机制这部分。其中，DataSource、TransactionManager这两部分只是会根据数据访问方式有所变化，比如使用hibernate进行数据访问时，DataSource实际为SessionFactory，TransactionManager的实现为 HibernateTransactionManager。如下图所示： 基于 @Transactional 的声明式事务管理 除了基于命名空间的事务配置方式，Spring 还引入了基于 Annotation 的方式，具体主要涉及@Transactional 标注。@Transactional 可以作用于接口、接口方法、类以及类方法上：当作用于类上时，该类的所有 public 方法将都具有该类型的事务属性；当作用于方法上时，该标注来覆盖类级别的定义。如下所示： 1234@Transactional(propagation = Propagation.REQUIRED)public boolean transfer(Long fromId， Long toId， double amount) &#123; return bankDao.transfer(fromId， toId， amount);&#125; Spring 使用 BeanPostProcessor 来处理 Bean 中的标注，因此我们需要在配置文件中作如下声明来激活该后处理 Bean，如下所示： 1&lt;tx:annotation-driven transaction-manager=\"transactionManager”/&gt; 1 与前面相似，transaction-manager、datasource 和 sessionFactory的配置不变，只需将基于和命名空间的配置更换为上述配置即可。 Spring 声明式事务的本质 就Spring 声明式事务而言，无论其基于 命名空间的实现还是基于 @Transactional 的实现，其本质都是 Spring AOP 机制的应用：即通过以@Transactional的方式或者XML配置文件的方式向业务组件中的目标业务方法插入事务增强处理并生成相应的代理对象供应用程序(客户端)使用从而达到无污染地添加事务的目的。如下图所示： 参考https://juejin.im/post/5b00c52ef265da0b95276091 https://blog.csdn.net/justloveyou_/article/details/73733278","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"},{"name":"spring","slug":"spring","permalink":"http://wumuwumu.github.io/tags/spring/"}]},{"title":"python-装饰器","date":"2019-07-31T11:44:57.000Z","path":"2019/07/31/python/python-装饰器/","text":"简单的装饰器12345678910111213141516171819import loggingdef use_logging(func): def wrapper(): logging.warning(\"%s is running\" % func.__name__) return func() # 把 foo 当做参数传递进来时，执行func()就相当于执行foo() return wrapperdef foo(): print('i am foo')foo = use_logging(foo) # 因为装饰器 use_logging(foo) 返回的时函数对象 wrapper，这条语句相当于 foo = wrapperfoo() # 执行foo()就相当于执行 wrapper()'''WARNING:root:foo is runningi am foo''' @ 语法糖123456789101112def use_logging(func): def wrapper(): logging.warn(\"%s is running\" % func.__name__) return func() return wrapper@use_loggingdef foo(): print(\"i am foo\")foo() *args、**kwargs可能有人问，如果我的业务逻辑函数 foo 需要参数怎么办？比如： 12def foo(name): print(&quot;i am %s&quot; % name) 我们可以在定义 wrapper 函数的时候指定参数： 1234def wrapper(name): logging.warn(\"%s is running\" % func.__name__) return func(name) return wrapper 这样 foo 函数定义的参数就可以定义在 wrapper 函数中。这时，又有人要问了，如果 foo 函数接收两个参数呢？三个参数呢？更有甚者，我可能传很多个。当装饰器不知道 foo 到底有多少个参数时，我们可以用 *args 来代替： 1234def wrapper(*args): logging.warn(\"%s is running\" % func.__name__) return func(*args) return wrapper 如此一来，甭管 foo 定义了多少个参数，我都可以完整地传递到 func 中去。这样就不影响 foo 的业务逻辑了。这时还有读者会问，如果 foo 函数还定义了一些关键字参数呢？比如： 12def foo(name, age=None, height=None): print(\"I am %s, age %s, height %s\" % (name, age, height)) 这时，你就可以把 wrapper 函数指定关键字函数： 12345def wrapper(*args, **kwargs): # args是一个数组，kwargs一个字典 logging.warn(\"%s is running\" % func.__name__) return func(*args, **kwargs) return wrapper 带参数的装饰器装饰器还有更大的灵活性，例如带参数的装饰器，在上面的装饰器调用中，该装饰器接收唯一的参数就是执行业务的函数 foo 。装饰器的语法允许我们在调用时，提供其它参数，比如@decorator(a)。这样，就为装饰器的编写和使用提供了更大的灵活性。比如，我们可以在装饰器中指定日志的等级，因为不同业务函数可能需要的日志级别是不一样的。 1234567891011121314151617def use_logging(level): def decorator(func): def wrapper(*args, **kwargs): if level == \"warn\": logging.warn(\"%s is running\" % func.__name__) elif level == \"info\": logging.info(\"%s is running\" % func.__name__) return func(*args) return wrapper return decorator@use_logging(level=\"warn\")def foo(name='foo'): print(\"i am %s\" % name)foo() 上面的 use_logging 是允许带参数的装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有参数的闭包。当我 们使用@use_logging(level=&quot;warn&quot;)调用的时候，Python 能够发现这一层的封装，并把参数传递到装饰器的环境中。 1@use_logging(level=\"warn\")`等价于`@decorator 类装饰器没错，装饰器不仅可以是函数，还可以是类，相比函数装饰器，类装饰器具有灵活度大、高内聚、封装性等优点。使用类装饰器主要依靠类的__call__方法，当使用 @ 形式将装饰器附加到函数上时，就会调用此方法。 1234567891011121314class Foo(object): def __init__(self, func): self._func = func def __call__(self): print ('class decorator runing') self._func() print ('class decorator ending')@Foodef bar(): print ('bar')bar() functools.wraps使用装饰器极大地复用了代码，但是他有一个缺点就是原函数的元信息不见了，比如函数的docstring、__name__、参数列表，先看例子： 123456789101112131415# 装饰器def logged(func): def with_logging(*args, **kwargs): print func.__name__ # 输出 'with_logging' print func.__doc__ # 输出 None return func(*args, **kwargs) return with_logging# 函数@loggeddef f(x): \"\"\"does some math\"\"\" return x + x * xlogged(f) 不难发现，函数 f 被with_logging取代了，当然它的docstring，__name__就是变成了with_logging函数的信息了。好在我们有functools.wraps，wraps本身也是一个装饰器，它能把原函数的元信息拷贝到装饰器里面的 func 函数中，这使得装饰器里面的 func 函数也有和原函数 foo 一样的元信息了。 12345678910111213from functools import wrapsdef logged(func): @wraps(func) def with_logging(*args, **kwargs): print func.__name__ # 输出 'f' print func.__doc__ # 输出 'does some math' return func(*args, **kwargs) return with_logging@loggeddef f(x): \"\"\"does some math\"\"\" return x + x * x 装饰器顺序一个函数还可以同时定义多个装饰器，比如： 12345@a@b@cdef f (): pass 它的执行顺序是从里到外，最先调用最里层的装饰器，最后调用最外层的装饰器，它等效于 1f = a(b(c(f))) 补充*与**区别在Python的函数定义中使用args和**kwargs可传递可变参数。args用作传递非命名键值可变长参数列表（位置参数），**kwargs用作传递键值可变长参数列表。在函数调用的时候也有解构的使用 1234567891011def test_var_args(farg, *args): print \"formal arg:\", farg for arg in args: print \"another arg:\", arg test_var_args(1, \"two\", 3)'''formal arg: 1another arg: twoanother arg: 3''' 1234567891011121314def test_var_kwargs(farg, **kwargs): print \"formal arg:\", farg for key in kwargs: print \"another keyword arg: %s: %s\" % (key, kwargs[key]) test_var_kwargs(farg=1, myarg2=\"two\", myarg3=3)'''Required argument: 1Optional argument (*args): 2Optional argument (*args): 3Optional argument (*args): 4Optional argument k2 (*kwargs): 6Optional argument k1 (*kwargs): 5''' 1234567def test_var_args_call(arg1, arg2, arg3): print \"arg1:\", arg1 print \"arg2:\", arg2 print \"arg3:\", arg3 args = (\"two\", 3)test_var_args_call(1, *args) 1234567def test_var_args_call(arg1, arg2, arg3): print \"arg1:\", arg1 print \"arg2:\", arg2 print \"arg3:\", arg3 kwargs = &#123;\"arg3\": 3, \"arg2\": \"two\"&#125;test_var_args_call(1, **kwargs) 参考 https://foofish.net/python-decorator.html https://www.biaodianfu.com/python-args-kwargs.html https://my.oschina.net/leejun2005/blog/477614 例子介绍的很详细","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"python-字符串格式","date":"2019-07-31T02:59:16.000Z","path":"2019/07/31/python/python-字符串格式/","text":"格式化操作符（%）“%”是Python风格的字符串格式化操作符，非常类似C语言里的printf()函数的字符串格式化（C语言中也是使用%）。 下面整理了一下Python中字符串格式化符合： 格式化符号 说明 %c 转换成字符（ASCII 码值，或者长度为一的字符串） %r 优先用repr()函数进行字符串转换 %s 优先用str()函数进行字符串转换 %d / %i 转成有符号十进制数 %u 转成无符号十进制数 %o 转成无符号八进制数 %x / %X 转成无符号十六进制数（x / X 代表转换后的十六进制字符的大小写） %e / %E 转成科学计数法（e / E控制输出e / E） %f / %F 转成浮点数（小数部分自然截断） %g / %G %e和%f / %E和%F 的简写 %% 输出% （格式化字符串里面包括百分号，那么必须使用%%） 这里列出的格式化符合都比较简单，唯一想要强调一下的就是”%s”和”%r”的差别。 看个简单的代码： 123456789string = \"Hello\\tWill\\n\"print(\"%s\" %string)print(\"%r\" %string)'''Hello Will'Hello\\tWill\\n'''' 补充： Python打印值的时候会保持该值在Python代码中的状态，不是用户所希望看到的状态。而使用print打印值则不一样，print打印出来的值是用户所希望看到的状态。 str和repr的区别： str 把值转换为合理形式的字符串，给用户看的。str实际上类似于int，long，是一种类型。 12345678print str(\"Hello, world!\")# Hello, world! print str(1000L)# 1000 str(\"Hello, world!\")# 'Hello, world!' # 字符串转换之后仍然是字符串str(1000L)# '1000' repr() 创建一个字符串，以合法python表达式的形式来表示值。repr()是一个函数。 12345678print repr(\"Hello, world!\")# 'Hello, world!'print repr(1000L)# 1000Lrepr(\"Hello, world!\")# \"'Hello, world!'\"repr(1000L)# '1000L' 格式化操作辅助符通过”%”可以进行字符串格式化，但是”%”经常会结合下面的辅助符一起使用。 辅助符号 说明 * 定义宽度或者小数点精度 - 用做左对齐 + 在正数前面显示加号(+) # 在八进制数前面显示零(0)，在十六进制前面显示”0x”或者”0X”（取决于用的是”x”还是”X”） 0 显示的数字前面填充”0”而不是默认的空格 (var) 映射变量（通常用来处理字段类型的参数） m.n m 是显示的最小总宽度，n 是小数点后的位数（如果可用的话） 12345678910111213141516171819202122232425262728293031323334num = 100print(\"%d to hex is %x\" %(num, num))print(\"%d to hex is %X\" %(num, num))print(\"%d to hex is %#x\" %(num, num))print(\"%d to hex is %#X\" %(num, num))# 浮点数f = 3.1415926print(\"value of f is: %.4f\" %f)# 指定宽度和对齐students = [&#123;\"name\":\"Wilber\", \"age\":27&#125;, &#123;\"name\":\"Will\", \"age\":28&#125;, &#123;\"name\":\"June\", \"age\":27&#125;]print(\"name: %10s, age: %10d\" %(students[0][\"name\"], students[0][\"age\"]))print(\"name: %-10s, age: %-10d\" %(students[1][\"name\"], students[1][\"age\"]))print(\"name: %*s, age: %0*d\" %(10, students[2][\"name\"], 10, students[2][\"age\"]))# dict参数for student in students: print(\"%(name)s is %(age)d years old\" %student) '''100 to hex is 64100 to hex is 64100 to hex is 0x64100 to hex is 0X64value of f is: 3.1416name: Wilber, age: 27name: Will , age: 28 name: June, age: 0000000027Wilber is 27 years oldWill is 28 years oldJune is 27 years old''' 字符串模板其实，在Python中进行字符串的格式化，除了格式化操作符，还可以使用string模块中的字符串模板（Template）对象。下面就主要看看Template对象的substitute()方法： 123456from string import TemplatesTemp = Template('Hi ,$name,$$ ')print(sTemp.substitute(name='wumu'))'''Hi ,wumu,$ ''' format1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 位置参数print(\"&#123;&#125; is &#123;&#125; years old\".format(\"Wilber\", 28))print(\"Hi, &#123;0&#125;! &#123;0&#125; is &#123;1&#125; years old\".format(\"Wilber\", 28))# 关键字参数print(\"&#123;name&#125; is &#123;age&#125; years old\".format(name = \"Wilber\", age = 28))# 下标参数li = [\"Wilber\", 28]print(\"&#123;0[0]&#125; is &#123;0[1]&#125; years old\".format(li))# 填充与对齐# ^、&lt;、&gt;分别是居中、左对齐、右对齐，后面带宽度# :号后面带填充的字符，只能是一个字符，不指定的话默认是用空格填充print('&#123;:&gt;8&#125;'.format('3.14'))print('&#123;:&lt;8&#125;'.format('3.14'))print('&#123;:^8&#125;'.format('3.14'))print('&#123;:0&gt;8&#125;'.format('3.14'))print('&#123;:a&gt;8&#125;'.format('3.14'))# 浮点数精度print('&#123;:.4f&#125;'.format(3.1415926))print('&#123;:0&gt;10.4f&#125;'.format(3.1415926))# 进制# b、d、o、x分别是二进制、十进制、八进制、十六进制print('&#123;:b&#125;'.format(11))print('&#123;:d&#125;'.format(11))print('&#123;:o&#125;'.format(11))print('&#123;:x&#125;'.format(11))print('&#123;:#x&#125;'.format(11))print('&#123;:#X&#125;'.format(11))# 千位分隔符print('&#123;:,&#125;'.format(15700000000))'''Wilber is 28 years oldHi, Wilber! Wilber is 28 years oldWilber is 28 years oldWilber is 28 years old 3.143.14 3.14 00003.14aaaa3.143.141600003.141610111113b0xb0XB15,700,000,000''' 参考 https://www.cnblogs.com/wilber2013/p/4641616.html","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"mysql自带的数据库","date":"2019-07-27T10:06:05.000Z","path":"2019/07/27/mysql/mysql自带的数据库/","text":"information_schema SCHEMATA表：提供了当前mysql实例中所有数据库的信息。是show databases的结果取之此表。 TABLES表：提供了关于数据库中的表的信息（包括视图）。详细表述了某个表属于哪个schema，表类型，表引擎，创建时间等信息。是show tables from schemaname的 结果取之此表。 COLUMNS表：提供了表中的列信息。详细表述了某张表的所有列以及每个列的信息。是show columns from schemaname.tablename的结果取之此表。 STATISTICS表：提供了关于表索引的信息。是show index from schemaname.tablename的结果取之此表。 USER_PRIVILEGES（用户权限）表：给出了关于全程权限的信息。该信息源自mysql.user授权表。是非标准表。 SCHEMA_PRIVILEGES（方案权限）表：给出了关于方案（数据库）权限的信息。该信息来自mysql.db授权表。是非标准表。 TABLE_PRIVILEGES（表权限）表：给出了关于表权限的信息。该信息源自mysql.tables_priv授权表。是非标准表。 COLUMN_PRIVILEGES（列权限）表：给出了关于列权限的信息。该信息源自mysql.columns_priv授权表。是非标准表。 CHARACTER_SETS（字符集）表：提供了mysql实例可用字符集的信息。是SHOW CHARACTER SET结果集取之此表。 COLLATIONS表：提供了关于各字符集的对照信息。 COLLATION_CHARACTER_SET_APPLICABILITY表：指明了可用于校对的字符集。这些列等效于SHOW COLLATION的前两个显示字段。 TABLE_CONSTRAINTS表：描述了存在约束的表。以及表的约束类型。 KEY_COLUMN_USAGE表：描述了具有约束的键列。 ROUTINES表：提供了关于存储子程序（存储程序和函数）的信息。此时，ROUTINES表不包含自定义函数（UDF）。名为“mysql.proc name”的列指明了对应于 INFORMATION_SCHEMA.ROUTINES表的mysql.proc表列。 VIEWS表：给出了关于数据库中的视图的信息。需要有show views权限，否则无法查看视图信息。 TRIGGERS表：提供了关于触发程序的信息。必须有super权限才能查看该表。 mysqlperformance_schema 需要设置参数： performance_schema 才可以启动该功能 按照相关的标准对进行的事件统计表, 表也是只读的，只能turcate events_waits_summary_by_instance events_waits_summary_by_thread_by_event_name events_waits_summary_global_by_event_name file_summary_by_event_name file_summary_by_instance setup_consumers 描述各种事件 setup_instruments 描述这个数据库下的表名以及是否开启监控。 setup_timers 描述 监控选项已经采样频率的时间间隔 events_waits_current 记录当前正在发生的等待事件，这个表是只读的表，不能update ，delete ，但是可以truncate 性能历史表 ：events_waits_history 只保留每个线程（thread） 的最近的10个事件 性能历史表 ：events_waits_history_long 记录最近的10000个事件 标准的先进先出（FIFO) 这俩表也是只读表，只能truncate sakila 这是一个MySQL的一个样本数据库，里边都是一些例子表。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"mysql修改字符集","date":"2019-07-27T08:55:17.000Z","path":"2019/07/27/mysql/mysql修改字符集/","text":"概念 字符集（character set）：定义了字符以及字符的编码。 字符序（collation）：定义了字符的比较规则。 Mysql字符集 一个字符集对应至少一种字符序（一般是1对多）。 两个不同的字符集不能有相同的字符序。 每个字符集都有默认的字符序。 12345678910-- 第一种方式SHOW CHARACTER SET;-- 第二种方式use information_schema;select * from CHARACTER_SETS;-- 例子SHOW CHARACTER SET WHERE Charset=&quot;utf8&quot;;SHOW CHARACTER SET LIKE &quot;utf8%&quot;; Mysql字符序123456-- 第一种方式SHOW COLLATION WHERE Charset = &apos;utf8&apos;;-- 第二种方式USE information_schema;SELECT * FROM COLLATIONS WHERE CHARACTER_SET_NAME=&quot;utf8&quot;; 命名规范字符序的命名，以其对应的字符集作为前缀，如下所示。比如字符序utf8_general_ci，标明它是字符集utf8的字符序。 更多规则可以参考 官方文档。 1[information_schema]&gt; SELECT CHARACTER_SET_NAME, COLLATION_NAME FROM COLLATIONS WHERE CHARACTER_SET_NAME=&quot;utf8&quot; limit 2; 设置修改 修改数据库字符集 1234ALTER DATABASE db_name DEFAULT CHARACTER SET character_name [COLLATE ...];把表默认的字符集和所有字符列（CHAR,VARCHAR,TEXT）改为新的字符集：ALTER TABLE tbl_name CONVERT TO CHARACTER SET character_name [COLLATE ...]如：ALTER TABLE logtest CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci; 修改表的默认字符集 12ALTER TABLE tbl_name DEFAULT CHARACTER SET character_name [COLLATE...];如：ALTER TABLE logtest DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 修改字段的字符集 12ALTER TABLE tbl_name CHANGE c_name c_name CHARACTER SET character_name [COLLATE ...];如：ALTER TABLE logtest CHANGE title title VARCHAR(100) CHARACTER SET utf8 COLLATE utf8_general_ci; 查看数据库编码 1SHOW CREATE DATABASE db_name; 查看表编码 1SHOW CREATE TABLE tbl_name; 查看字段编码 1SHOW FULL COLUMNS FROM tbl_name; 查看系统的编码字符 1SHOW VARIABLES WHERE Variable_name LIKE &apos;character\\_set\\_%&apos; OR Variable_name LIKE &apos;collation%&apos;; MySQL字符集设置 系统变量： 12345678910111213– character_set_server：默认的内部操作字符集– character_set_client：客户端来源数据使用的字符集– character_set_connection：连接层字符集– character_set_results：查询结果字符集– character_set_database：当前选中数据库的默认字符集– character_set_system：系统元数据(字段名等)字符集– 还有以collation_开头的同上面对应的变量，用来描述字符序。 用introducer指定文本字符串的字符集： – 格式为：[_charset] ‘string’ [COLLATE collation] – 例如： 12345• SELECT _latin1 ‘string’;• SELECT _utf8 ‘你好’ COLLATE utf8_general_ci;–- 由introducer修饰的文本字符串在请求过程中不经过多余的转码，直接转换为内部字符集处理。 MySQL中的字符集转换过程 MySQL Server收到请求时将请求数据从character_set_client转换为character_set_connection； 进行内部操作前将请求数据从character_set_connection转换为内部操作字符集，其确定方法如下： • 使用每个数据字段的CHARACTER SET设定值； • 若上述值不存在，则使用对应数据表的DEFAULT CHARACTER SET设定值(MySQL扩展，非SQL标准)； • 若上述值不存在，则使用对应数据库的DEFAULT CHARACTER SET设定值； • 若上述值不存在，则使用character_set_server设定值。 参考 https://www.cnblogs.com/chyingp/p/mysql-character-set-collation.html https://www.cnblogs.com/qiumingcheng/p/10336170.html","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"管理Odoo服务器实例","date":"2019-06-18T06:02:38.000Z","path":"2019/06/18/odoo/Odoo 12开发者指南第二章 管理Odoo服务器实例/","text":"全书完整目录请见：Odoo 12开发者指南（Cookbook）第三版 本章中，我们将讲解如下内容： 配置插件路径 更新插件模块列表 标准化你的实例目录布局 安装并升级本地插件模块 对插件应用修改 应用及尝试建议的拉取请求 引言在第一章 安装Odoo开发环境中，我们看了如何使用与编辑器一同发布的标准核心插件来设置 Odoo 实例。本章集中讲解为 Odoo 实例添加非核心插件。Odoo中，你可以从多个目录中加载插件。此外，推荐你将第三方插件（如OCA模块）或你自定义的插件放在一个单独的文件夹中，这样可以避免与 Odoo 核心模块产生冲突。甚至Odoo 企业版也是一种类型的插件目录，你需要像普通插件目录一样加载它。 ℹ️有关用词 – 插件(add-on) vs. 模块(module) 本书中，我们使用插件或插件模块来指代 Odoo 所预期安装的 Python 包。用户界面常使用应用（app）或模块的表达 ，但我们更愿意保留模块一词来表示Python模块或包，它们不一定是 Odoo 插件，而应用（app）来表示适当定义为应用的插件模块，表示它不是Odoo主菜单中的入口。 配置插件路径通过addons_path参数的配置，你可以在 Odoo 中加载自己的插件模块。在Odoo初始化一个新数据库时，它会搜索在addons_path配置参数中给定的这些目录。addons_path会在这些目录中搜索潜在的插件模块。addons_path中所列出的目录预期应包含子目录，每个子目录是一个插件模块。在数据库初始化完成后，你将能够安装这些目录中所给出的模块。 准备工作这一部分假定你已经准备好了实例并生成了配置文件，如在第一章 安装Odoo开发环境中在一个文件中存储实例配置一节所描述。Odoo的源码存放在~/odoo-dev/odoo中，而配置文件存放在~/odoo-dev/myinstance.cfg中。 如何配置…按如下步骤在实例的addons_path中添加~/odoo-dev/local-addons目录： 编辑你的实例的配置文件，即 ~/odoo-dev/my-instance.cfg。 定位到以addons_path =开头一行，默认，你会看到如下内容： 1addons_path = ~/odoo-dev/odoo/odoo/addons,~/odoo-dev/odoo/add-ons 译者注： 当前默认生成的配置文件中为绝对路径，且仅包含xxx/odoo/addons 修改该行，添加一个逗号（英文半角），并接你想想要添加为addons_的目录名称，如以下代码所示： 1addons_path = ~/odoo-dev/odoo/odoo/addons,~/odoo-dev/odoo/addons,~/odoo-dev/local-addons 重启你的实例 1$ ~/odoo-dev/odoo/odoo-bin -c my-instance.cfg 运行原理…在重启 Odoo 时，会读取配置文件。addons_path变量的值应为一个逗号分隔的目录列表。可接受相对路径，但它们是相对于当前工作目录的，因此应在配置文件中尽量避免。 至此，~/odoo-dev/local-addons中包含的新插件尚不在该实例的可用模块列表中。为此，你需要执行一个额外的操作，在下一部分更新插件模块列表中会进行讲解。 扩展知识…在第一次调用 odoo-bin脚本来初始化新数据库时，你可以传递一个带逗号分隔目录列表的–addons-path命令行参数。这会以所提供插件路径中所找到的所有插件来初始化可用插件模块列表。这么做时，你要显式地包含基础插件目录（odoo/odoo/addons）以及核心插件目录（odoo/addons）。 与前面稍有不同的是本地插件目录不能为空（译者注：请先阅读下面的小贴士），它必须要至少包含一个子目录，并包含插件模块的最小化结构。在第四章 创建Odoo插件模块中，我们会来看如何编写你自己的模块。同时，这里有一个生成内容来满足Odoo要求的快捷版黑科技： 1$ mkdir -p ~/odoo-dev/local-addons/dummy$ touch ~/odoo-dev/local-addons/dummy/__init__.py$ echo &apos;&#123;&quot;name&quot;: &quot;dummy&quot;, &quot;installable&quot;: False&#125;&apos; &gt; \\~/odoo-dev/local-addons/dummy/__manifest__.py 你可以使用–save选项来保存配置文件的路径： 1$ odoo/odoo-bin -d mydatabase \\--add-ons-path=&quot;odoo/odoo/addons,odoo/addons,~/odoo-dev/local-addons&quot; \\--save -c ~/odoo-dev/my-instance.cfg --stop-after-init 本例中，使用相对路径不会有问题，因为它们会在配置文件中转化为绝对路径。 小贴士：因为Odoo仅当从命令行中设置路径时在插件路径的目录中查看插件，而不是在从配置文件中加载路径的时候，dummy已不再必要。因此，你可以删除它（或保留到你确定不需要新建一个配置文件时）。 更新插件模块列表我们在前面的部分已经说到，在向插件路径添加目录时，仅仅重启Odoo服务是不足以安装其中一个新插件模块的。Odoo还需要有一个指定动作来扫描路径并更新可用插件模块的列表。 准备工作启动你的实例并使用管理员账号连接它。然后，激活开发者模式（如果你不知道如何激活开发者模式，请参见第一章 安装Odoo开发环境）。 如何更新…要更新你实例中的可用插件模块列表，你需要执行如下步骤： 打开Apps菜单 点击Update Apps List：[ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050902052063.jpg) 在弹出对话框中，点击Update按钮：[ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050902070776.jpg) 在更新的最后，你可以点击Apps入口来查看已更新的可用插件模块列表。你将需要删除Apps搜索框中的默认过滤器来查看所有模块。 运行原理…在点击了Update按钮之后，Odoo会读取插件路径配置变量。对于列表中的每一个目录，它会查看包含保存在插件模块目录中名为manifest.py的插件声明文件的直接子目录。Odoo读取声明内容，并预期查找其中的Python字典。除非声明内容中包含一个键installable的值为False，插件模块的元数据就会存储在数据库中。如果模块已存在，则更新相关信息。否则，会创建一条新记录。如果此前可用的插件模块未找到，则从列表中删除该记录。 ℹ️仅在初始化数据库后添加了新的插件路径时才需要更新应用列表。如果你在初始化数据库之前在配置文件中添加了新插件路径，那么就无需手动更新模块列表。 标准化你的实例目录布局我们推荐你在开发和生产环境都使用相似的目录布局。这一标准化会在你要执行运维时体现出用处，它也会缓解你日常工作的压力。 这一部分创建将相似生命周期或相似用途的文件分组放在标准化子目录中的目录结构。请自由按照自己的需求来调整这一结构，但请确保你将这一结构在某处进行记录存档。 如何标准化…创建所推荐实例布局，你需要执行如下步骤： 译者注：读者也可直接使用 Alan 在 GitHub 上准备的安装脚本进行操作 为实例创建一个目录： 1$ mkdir ~/odoo-dev/projectname$ cd ~/odoo-dev/projectname 在名为env/的子目录中创建一个Python虚拟环境： 1$ virtualenv -p python3 env 创建一些子目录，如下： 1$ mkdir src local bin filestore logs 这些子目录的功能如下： src/：这包含Odoo本身的一个拷贝，以及一些第三方插件项目（我们在下一步中添加了Odoo源码） local/：这用于保存你针对具体实例的插件 bin/：这包含各类帮助可执行shell脚本 filestore/：这用于文件存储 logs/（可选）：这用于存储服务日志文件 克隆Odoo并安装所需依赖包（参见 第一章 安装Odoo开发环境 获取更多内容）： 12$ git clone https://github.com/odoo/odoo.git src/odoo$ env/bin/pip3 install -r src/odoo/requirements.txt 以bin/odoo保存如下shell脚本： 12345ROOT=$(dirname $0)/..PYTHON=$ROOT/env/bin/python3ODOO=$ROOT/src/odoo/odoo-bin$PYTHON $ODOO -c $ROOT/projectname.cfg \"$@\"exit $? 让该脚本可执行： 1$ chmod +x bin/odoo 创建一个空的本地模块dummy： 123$ mkdir -p local/dummy$ touch local/dummy/__init__.py$ echo &apos;&#123;&quot;name&quot;: &quot;dummy&quot;, &quot;installable&quot;: False&#125;&apos; &gt;\\local/dummy/__manifest__.py 为你的实例生成配置文件： 1$ bin/odoo --stop-after-init --save \\ --addons-path src/odoo/odoo/addons,src/odoo/addons,local \\ --data-dir filestore 添加一个.gitignore文件，用于告诉GitHub排除这些给定目录，这样Git在提交代码时就会忽略掉这些目录，例如 filestore/, env/, logs/和src/： 123456789101112# dotfiles, with exceptions:.*!.gitignore# python compiled files*.py[co]# emacs backup files*~# not tracked subdirectories/env//src//filestore//logs/ 为这个实例创建一个Git仓库并将已添加的文件添加到Git中： 123$ git init$ git add .$ git commit -m \"initial version of projectname\" 运行原理…我们生成了一个有明确标签目录和独立角色的干净的目录结构。我使用了不同的目录来存储如下内容： 由其它人所维护的代码（src/中） 本地相关的具体代码 实例的文件存储 通过为每个项目建一个virtualenv环境，我们可以确保该项目的依赖文件不会与其它项目的依赖产生冲突，这些项目你可能运行着不同的Odoo版本或使用了不同的第三方插件模块，这将需要不同版本的Python依赖。这当然也会带来一部分磁盘空间的开销。 以类似的方式，通过为我们不同的项目使用不同的Odoo拷贝以及第三方插件模块，我们可以让每个项目单独的进行推进并仅在需要时在这些实例上安装更新，因此也减少了引入回退的风险。 bin/odoo允许我们不用记住各个路径或激活虚拟环境就可以运行服务。这还为我们设置了配置文件。你可以在其中添加其它脚本来协助你的日常工作。例如，你可以添加一个脚本来检查运行实例所需的第三方项目。 有关配置文件，我们仅展示了这里需要设置的最小化选项，但很明显你可以设置更多，例如数据库名、数据库过滤器或项目所监听的端口。有关这一话题的更多信息，请参见第一章 安装Odoo开发环境。 最后，通过在Git仓库中管理所有这些，在不同的电脑上复制这一设置及在团队中分享开发内容变得相当容易。 小贴士：加速贴士 要加速项目的创建，你可以创建一个包含空结构的模板仓库，并为每个项目复制（fork）该仓库。这会省却你重新输入bin/odoo脚本、.gitignore及其它所需模板文件（持续集成配置、README.md、ChangeLog等等）所花费的时间。 参见内容如果你喜欢这种方法，我们建议你尝试第三章 服务器部署中的使用 Docker 运行 Odoo 一部分的内容。 扩展知识…复杂模块的开发要求有各类配置选项，在想要尝试任何配置选项时都会要更新配置文件。更新配置常常是一件头痛的事，避免它的一种方式是通过命令行传递所有配置选项，如下： 手动激活虚拟环境： 1$ source env/bin/activate 进行Odoo源代码目录： 1$ cd src/odoo 运行服务： 1./odoo-bin --addons-path=addons,../../local -d test-12 -i account,sale,purchase --log-level=debug 第三步中，我们直接通过命令行传递了一些参数。第一个是–addons-path，它加载Odoo的核心插件目录addons，以及你自己的插件目录local，在其中你可以放自己的插件模块。选项-d会使用test-12数据库或者在该数据库不存在时新建一个数据库。选项-i 会安装会计、销售和采购模块。接着，我们传递了log-level选项来将日志级别提升为debug，这样日志中会显示更多的信息。 ℹ️通过使用命令行，你可以快速地修改配置选项。你也可以在Terminal中查看实时日志。所有可用选项可参见第一章 安装Odoo开发环境，或使用-help命令来查看所有的选项列表及各个选项的描述。 安装并升级本地插件模块Odoo 功能的核心来自于它的插件模块。Odoo自带的插件是你所拥有的财富，同时你也可以在应用商店下载一些插件模块或者自己写。 这一部分中，我们将展示如何通过网页界面及命令行来安装并升级插件模块。 对这些操作使用命令行的主要好处包含可以同时作用于一个以上的插件以及在安装或升级的过程中可以清晰地浏览到服务端日志，对于开发模式或编写脚本安装实例时都非常有用。 准备工作确保你有一个运行中的 Odoo 实例，且数据库已初始化、插件路径已进行恰当地设置。在这一部分中，我们将安装/升级一些插件模块。 如何安装升级…安装或升级插件有两种方法-可以使用网页界面或命令行。 通过网页界面可按照如下步骤来使用网页界面安装新的插件模块到数据库中： 使用管理员账户连接实例并打开Apps菜单[ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050906002399.jpg) 使用搜索框来定位你想要安装的插件。这里有一些帮助你完成该任务的操作指南： 激活Not Installed过滤器 如果你要查找一个具体的功能插件而不是广泛的功能插件，删除Apps过滤器 在搜索框中输入模块名的一部分并使用它来作为模块过滤器 你会发现使用列表视图可以阅读到更多的信息 点击卡片中模块名下的Install按钮。 注意有些Odoo插件模块需要有外部Python依赖，如果你的系统中未安装该Python依赖，那么 Odoo 会中止安装并显示如下的对话框： [ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050906125210.jpg) 译者注：按正常安装不会出现一错误，需通过 pip uninstall pyldap 才能复现这一错误 修复这一问题，仅需在你的系统中安装相关的Python依赖即可。 要升级已安装到数据库的模块，使用如下步骤： 使用管理员账户连接到实例 打开Apps菜单 点击Apps:[ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050906203077.jpg) 使用搜索框来定位你所安装的插件。有如下的小贴士： 激活Installed过滤器 如果你要查找一个具体的功能插件而不是广泛的功能插件，删除Apps过滤器 在搜索框中输入部分插件模块的名称并按下 Enter 来使用它作为模块过滤器。例如，输入CRM并按下 Enter 来搜索CRM应用 你会发现使用列表视图可以阅读到更多的信息 点击卡片右上角的的三个点，然后点击Upgrade选项： [ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050906265357.jpg) 激活开发者模式来查看模块的技术名称。如果你不知道如何激活开发者模式，请参见第一章 安装Odoo开发环境： [ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050906302261.jpg) 在激活开发者模式之后，它会以红色显示模块的技术名称。如果你使用的是Odoo社区版，会看到一些带有Upgrade的附加应用。这些是Odoo企业版的应用，要想安装/使用它们，需要购买一个证书。 通过命令行要在你的数据库中安装新插件，可按照如下步骤： 查找插件的名称。这是包含manifest.py文件的目录名，不带前面的路径。 停止实例。如果你在操作生产数据库，请进行备份。 运行如下命令： 1odoo/odoo-bin -c instance.cfg -d dbname -i addon1,addon2 --stop-after-init 译者注： 请将addon1,addon2替换为你所要安装的插件名 小贴士：你可以省略掉-d dbname，因为这在配置文件中进行了设置。 重新启动实例 运行原理…插件模块的安装和升级是两个紧密关联的操作，但有一些重要的区别，在下面两部分中进行了强调： 插件安装在你安装插件时，Odoo以提供的名称检查它的可用插件列表中未安装插件。它还会检查该插件的依赖，并且如果有依赖的话，它会在安装插件前递归安装这些依赖。 单个模块的安装包含如下步骤： 如果存在，运行插件preinit钩子 从Python源代码中加载模型定义并在必要时更新数据库结构（参见第五章 应用模型了解更多信息） 加载插件的数据文件并在必要时更新数据库内容（参见第七章 模块数据了解更多信息） 如果实例中启用了演示数据则安装插件演示数据 如果存在，运行插件postinit钩子 运行对插件视图定义的验证 如果启用了演示数据并启用了测试，运行该插件的测试（参见第十八章 自动化测试用例了解更多信息） 在数据库中更新模块状态 从插件的翻译文件中更新数据库中的翻译（参见第十二章 国际化了解更多信息） ℹ️preinit和postinit钩子分别使用pre_init_hook和post_init_hook键名在manifest.py文件中定义。这些钩子用于在插件模块的安装之前及之后触发Python函数。参见第四章 创建Odoo插件模块了解更多有关 init 钩子的知识。 插件升级升级插件时，Odoo以给定的名称在可用的插件模块列表中检查已安装插件。它还会检查该插件的反向依赖（即依赖于所升级插件的那些插件）。如果存在，则也会对它们进行递归升级。 单个插件模块的升级过程包含如下步骤： 如果有的话，先运行插件模块的预迁移步骤（参见第七章 模块数据了解更多信息） 从Python源码中加载模型定义并在必要时更新数据库结构（参见第五章 应用模型了解更多信息） 加载插件的数据文件并在必要时更新数据库内容（参见第七章 模块数据了解更多信息） 如果实例中启用了演示数据更新插件演示数据 如果模块有任何迁移方法的话，先运行插件模块的后置迁移步骤（参见第七章 模块数据了解更多信息） 运行对插件视图定义的验证 如果启用了演示数据并启用了测试，运行该插件的测试（参见第十八章 自动化测试用例了解更多信息） 在数据库中更新模块状态 从插件的翻译文件中更新数据库中的翻译（参见第十二章 国际化了解更多信息） ℹ️注意更新未安装的插件模块什么也不会做。但是安装已安装的插件模块会重新安装该插件，这会通过一些包含数据的数据文件产生一些预期外的问题，这些文件可能应由用户进行更新而非在常规的模块升级处理时进行更新（参见第七章 模块数据中使用noupdate和forcecreate标记部分的内容）。通过用户界面不存在错误的风险，但通过命令行时则有可能发生。 扩展知识…要当心依赖的处理。假定有一个实例你想要安装sale、sale_stock和sale_specific插件，sale_specific依赖于sale_stock，而sale_stock依赖于sale。要安装这三者，你只需要安装sale_specific，因为它会递归安装sale_stock和sale这两个依赖。要升级这两者，你需要升级sale，因为这样会递归升级其反向依赖，sale_stock和sale_specific。 管理依赖另一个比较搞的地方是在你向已经有一个版本安装了的插件添加依赖的时候。我们继续通过前例来理解这一问题。想像一下你在sale_specific中添加了一个对stock_dropshipping的依赖。更新sale_specific插件不会自动安装新的依赖，也会要求安装sale_specific。在这种情况下，你会收到非常糟糕的错误消息，因为插件的Python代码没有成功的加载，而插件的数据和模型表则存在于数据库中。要解决这一问题，你需要停止该实例并手动安装新的依赖。 从GitHub安装插件模块GitHub是第三方插件的一个很好的来源。很多Odoo合作伙伴使用GitHub来分享他们内部维护的插件，而Odoo社区联盟（OCA）在GitHub上共同维护着几百个插件。在你开始编写自己的插件之前，确保查看是否已有可用的插件或者作为初始以继续扩展插件。 这一部分向你展示如何从GitHub上克隆OCA的partner-contact项目并让其中所包含的插件模块在我们实例中可用。 准备工作假设你想要改变你的实例中地址的处理方式，你的客户需要在Odoo两个字段（街道和街道2）之外的第三个字段来存储地址。你肯定是可以编写自己的插件来为res.partne添加一个字段的，但如果想要让地址在发票上以合适的格式显示，问题就要比看上去麻烦一些了。所幸，你邮件列表上的某个人告诉了你partner_address_street3插件，由OCA作为partner-contact项目的一部分进行维护。 本部分中所使用的路径反映了我们在标准化你的实例目录布局一节中所推荐的布局。 如何安装…按照如下步骤来安装partner_address_street3： 进入你的项目目录： 1$ cd ~/odoo-dev/my-odoo/src 在src/目录中克隆partner-contact项目的12.0分支： 1$ git clone --branch 12.0 \\https://github.com/OCA/partner-contact.git src/partner-contact 修改插件路径来包含该目录并更新你的实例中的插件列表（参见本章中的配置插件路径和更新插件模块列表一节）。instance.cfg中的addons_path一行应该是这样的： 1addons_path = ~/odoo-dev/my-odoo/src/odoo/odoo/addons, \\~/odoo-dev/my-odoo/src/odoo/addons, \\~/odoo-dev/my-odoo/src/, \\~/odoo-dev/local-addons 安装partner_address_street3插件（如果你不知道如何安装该模块，参见前面一节，安装并升级本地插件模块） 运行原理…所有 Odoo社区联盟的代码仓库都将他们自己的插件放在单独的目录中，这与Odoo对插件路径中目录的预期是相一致的。因此，只需复制某处的仓库并将其添加到插件路径中就够了。 扩展知识…有些维护者遵循不同的方法，每个插件模块一个仓库，放在仓库的根目录下。这种情况下，你需要创建一个新的目录，在这个目录中添加插件路径并克隆你所需的维护者的插件到该目录中。记住在每次添加一个新仓库拷贝时要更新插件模块列表。 对插件应用修改GitHub上可用的大部分插件需要进行修改并且不遵循Odoo对其稳定发行版所强制的规则。它们可能收到漏洞修复或改善，包含你提交的问题或功能请求，这些修改可能会引入数据库模式的修改或数据文件和视图中的更新。这一部分讲解如何安装升级后的版本。 准备工作假定你对partner_address_street3报告了一个问题并收到通知说该问题已在partner-contact项目12.0分支的最近一次修订中得以解决。这种情况下，你可以使用最新版本来更新你的实例。 如何修改…要对GitHub的插件进行源的变更，需执行如下步骤： 停止使用该插件的实例。 如果是生产实例请做一个备份（参见第一章 安装Odoo开发环境中管理Odoo服务端数据库一节）。 进入克隆了partner-contact的目录： 1$ cd ~/odoo-dev/my-odoo/src/partner-contact 为该项目创建一个本地标签，这样万一出现了崩溃你可以进行回退： 1$ git checkout 12.0$ git tag 12.0-before-update-$(date --iso) 获取源码的最新版本： 1$ git pull --ff-only 在你的数据库中更新partner_address_street3插件（参见安装并升级本地插件模块一节） 重启实例 运行原理…通常，插件模块的开发者有时会发布插件的最新版本。这一更新一般包含漏洞修复及新功能。这里，我们将获取一个插件的新版本并在我们的实例中更新它。 如果git pull –ff-only失败的话，你可以使用如下命令回退到前一个版本： 1$ git reset --hard 12.0-before-update-$(date --iso) 然后，你可以尝试git pull（不添加–ff-only），它会产生一个合并，但这表示你对插件做了本地修改。 扩展知识…如果更新这一步崩溃了，参见第一章 安装Odoo开发环境从源码更新Odoo一节获取恢复的操作指南。记住要总是在一个生产数据库的拷贝上先进行测试。 应用及尝试建议的拉取请求在GitHub的世界中，拉取请求（PR）是由开发者所提交的请求，这样项目维护人员可以添加一些新的开发。比如一个 PR 可能包含漏洞修复或新功能。这里请求在拉取到主分支之前会进行审核和测试。 这一部分讲解如何对你的 Odoo 项目应用一个PR来测试漏洞修复的改进。 准备工作在前一节中，假定你对partner_address_street3 报告了一个问题并收到一条通知在拉取请求中问题已修复，尚未合并到项目的12.0分支中。开发人员要求你验证PR #123中的修复状况。你需要使用这一分支更新一个测试实例。 你不应在生产数据库直接使用该分支，因此先创建一个带有生产数据库拷贝的测试环境（参见第一章 安装Odoo开发环境和第三章 服务器部署）。 如何操作…应用并测试一个插件的GitHub拉取请求，你需要执行如下步骤： 停止实例 进入partner-contact所被克隆的目录： 1$ cd ~/odoo-dev/my-odoo/src/partner-contact 为该项目创建一个本地标签，这样万一出现崩溃时你可以回退： 1$ git checkout 12.0$ git tag 12.0-before-update-$(date --iso 拉取pull请求的分支。这么做最容易的方式是使用PR编号，在开发者与你沟通时你应该可以看到。在本例中，这个拉取请求编号是123： 1$ git pull origin pull/123/head 在你的数据库中更新partner_address_street3插件模块并重启该实例（如果你不知道如何更新该模块的话请参见安装并升级本地插件模块一节） 测试该更新 – 尝试重现问题，或测试你想要的功能。 如果这不能运行，在GitHub的PR页面进行评论，说明你做了什么以及什么不能运行，这样开发者可以更新这个拉取请求。 如果它没有问题，也在PR页面说下；这是PR验证流程中非常重要的一部分；这会加速主分支中的合并。 运行原理…我们在使用一个GitHub功能，使用pull/nnnn/head分支名称来通过编号进行拉取请求的拉取，其中nnnn是PR的编号。Git pull命令合并远程分支到我们的分支，在我们基础代码中应用修改。在这之后，我们更新插件模块、对其测试并向作者报回修改是成功或是失败。 扩展知识…如果你想要同步测试它们，你可以针对相同仓库的不同拉取请求重复本节中的第4步。如果你对结果很满意，你可以创建一个分支来保留对应用了改变的结果的引用： 1$ git checkout -b 12.0-custom 使用一个不同的分支会帮助你记住你没有从GitHub使用该版本，而是一个自定义的版本。 ℹ️git branch命令可用于列出你仓库中的所有本地分支。 从这开始，如果你需要应用来自GitHub中12.0分支的最近一个审核版本，你需要不使用–ff-only来拉取它： 1$ git pull origin 12.0","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"},{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"__import__在python中的区别","date":"2019-06-01T07:49:16.000Z","path":"2019/06/01/import-在python中的区别/","text":"import作用：导入/引入一个python标准模块，其中包括.py文件、带有init.py文件的目录(自定义模块)。 import module_name[,module1,…] from module import *|child[,child1,…] 注意：多次重复使用import语句时，不会重新加载被指定的模块，只是把对该模块的内存地址给引用到本地变量环境。 实例： pythontab.py 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `os ``print` `'in pythontab'``,``id``(os)` test.py 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `pythontab ``#第一次会打印pythontab里面的语句 ``import` `os ``#再次导入os后，其内存地址和pythontab里面的是一样的，因此这里只是对os的本地引用 ``print` `&apos;in c&apos;``,``id``(os) ``import` `pythontab ``#第二次不会打印pythontab里面的语句，因为没有重新加载` reload作用：对已经加载的模块进行重新加载，一般用于原模块有变化等特殊情况，reload前该模块必须已经import过。 import os reload(os) 说明： reload会重新加载已加载的模块，但原来已经使用的实例还是会使用旧的模块，而新生产的实例会使用新的模块；reload后还是用原来的内存地址；不能支持from。。import。。格式的模块进行重新加载。 实例： pythontab.py 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `os ``print` `'in pythontab'``,``id``(os)` test.py 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `pythontab ``#第一次import会打印pythontab里面的语句 ``print` `id``(pythontab) ``#原来pythontab的内存地址 ``reload``(pythontab) ``#第二次reload还会打印pythontab里面的语句，因为有重新加载 ``print` `id``(pythontab) ``#reload后pythontab的内存地址，和原来一样` 扩展： 上面说了，在特殊情况的下才会使用reload函数；除了原来模块文件有修改外，还有哪些情况需要使用reload函数呢，这里举个例子。 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `sys ``#引用sys模块进来，并不是进行sys的第一次加载 ``reload``(sys) ``#重新加载sys ``sys.setdefaultencoding(``'utf8'``) ``##调用setdefaultencoding函数` 上面的代码是正确的，再测试下面的代码 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `sys ``sys.setdefaultencoding(``'utf8'``)` 上面的测试会失败，那么为什么要在调用setdefaultencoding时必须要先reload一次sys模块呢？因为这里的import语句其实并不是sys的第一次导入语句，也就是说这里其实可能是第二、三次进行sys模块的import，这里只是一个对sys的引用，只能reload才能进行重新加载；那么为什么要重新加载，而直接引用过来则不能调用该函数呢？因为setdefaultencoding函数在被系统调用后被删除了，所以通过import引用进来时其实已经没有了，所以必须reload一次sys模块，这样setdefaultencoding才会为可用，才能在代码里修改解释器当前的字符编码。试试下面的代码，同样会报错： 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `sys ``reload``(sys) ``sys.setdefaultencoding(``'utf8'``) ``del` `sys.setdefaultencoding ``##删除原来的setdefaultencoding函数 ``sys.setdefaultencoding(``'gb2312'``)` 那么到底是谁在之前就导入sys并且调用了setdefaultencoding函数呢？答案就在python安装目录的Lib文件夹下，有一个叫site.py的文件【python2.6】，在里面可以找到main() –&gt; setencoding()–&gt;sys.setdefaultencoding(encoding),因为这个site.py每次启动python解释器时会自动加载，所以main函数每次都会被执行，setdefaultencoding函数一出来就已经被删除了。 import作用： 同import语句同样的功能，但import是一个函数，并且只接收字符串作为参数，所以它的作用就可想而知了。其实import语句就是调用这个函数进行导入工作的，import sys &lt;==&gt;sys = import(‘sys’) 使用： import(module_name[, globals[, locals[, fromlist]]]) #可选参数默认为globals(),locals(),[] import(‘os’) import(‘os’,globals(),locals(),[‘path’,’pip’]) #等价于from os import path, pip 说明： 通常在动态加载时可以使用到这个函数，比如你希望加载某个文件夹下的所用模块，但是其下的模块名称又会经常变化时，就可以使用这个函数动态加载所有模块了，最常见的场景就是插件功能的支持。 扩展： 既然可以通过字符串来动态导入模块，那么是否可以通过字符串动态重新加载模块吗？试试reload(‘os’)直接报错，是不是没有其他方式呢?虽然不能直接reload但是可以先unimport一个模块，然后再import来重新加载模块。现在看看unimport操作如何实现，在Python解释里可以通过globals(),locals(),vars(),dir()等函数查看到当前环境下加载的模块及其位置，但是这些都只能看不能删除，所以无法unimport；不过除此之外还有一个地方是专门存放模块的，这就是sys.modules，通过sys.modules可以查看所有的已加载并且成功的模块，而且比globals要多，说明默认会加载一些额外的模块，接下来就是unimport了。 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `sys ``__import__``(``'a'``) ``#第一次导入会打印消息 ``del` `sys.modules[``'a'``] ``#unimport ``__import__``(``'a'``) ``#再次导入还是会打印消息，因为已经unimport一次了 ``__import__``(``'a'``) ``#这次就不会打印消息了`","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"java多线程自问","date":"2019-04-15T02:31:35.000Z","path":"2019/04/15/java/java多线程自问/","text":"java创建线程的方式 java的线程的类型 Timer与TimerTask的区别 怎么启动、停止、加入、礼让线程 线程的生命周期以及其切换 CountDownLatch、CyclicBarrier和Semaphore 什么是线程安全？Vector是一个线程安全类吗？","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"redis分布式锁","date":"2019-04-11T09:55:56.000Z","path":"2019/04/11/redis分布式锁/","text":"单机Redis实现分布式锁获取锁获取锁的过程很简单，客户端向Redis发送命令： 12SET resource_name my_random_value NX PX 30000复制代码 my_random_value是由客户端生成的一个随机字符串，它要保证在足够长的一段时间内在所有客户端的所有获取锁的请求中都是唯一的。 NX表示只有当resource_name对应的key值不存在的时候才能SET成功。这保证了只有第一个请求的客户端才能获得锁，而其它客户端在锁被释放之前都无法获得锁。 PX 30000表示这个锁有一个30秒的自动过期时间。 释放锁123456if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1])else return 0end复制代码 之前获取锁的时候生成的my_random_value 作为参数传到Lua脚本里面，作为：ARGV[1],而 resource_name作为KEYS[1]。Lua脚本可以保证操作的原子性。 关于单点Redis实现分布式锁的讨论网络上有文章说用如下命令获取锁: 123SETNX resource_name my_random_valueEXPIRE resource_name 30复制代码 由于这两个命令不是原子的。如果客户端在执行完SETNX后crash了，那么就没有机会执行EXPIRE了，导致它一直持有这个锁，其他的客户端就永远获取不到这个锁了。 为什么my_random_value 要设置成随机值? 保证了一个客户端释放的锁是自己持有的那个锁。如若不然，可能出现锁不安全的情况。 123456客户端1获取锁成功。客户端1在某个操作上阻塞了很长时间。过期时间到了，锁自动释放了。客户端2获取到了对应同一个资源的锁。客户端1从阻塞中恢复过来，释放掉了客户端2持有的锁。复制代码 用 SETNX获取锁 网上大量文章说用如下命令获取锁： 12SETNX lock.foo &lt;current Unix time + lock timeout + 1&gt;复制代码 原文在Redis对SETNX的官网说明，Redis官网文档建议用Set命令来代替，主要原因是SETNX不支持超时时间的设置。 redis.io/commands/se… Redis集群实现分布式锁上面的讨论中我们有一个非常重要的假设：Redis是单点的。如果Redis是集群模式，我们考虑如下场景: 123456客户端1从Master获取了锁。Master宕机了，存储锁的key还没有来得及同步到Slave上。Slave升级为Master。客户端2从新的Master获取到了对应同一个资源的锁。客户端1和客户端2同时持有了同一个资源的锁，锁不再具有安全性。复制代码 就此问题，Redis作者antirez写了RedLock算法来解决这种问题。 RedLock获取锁 获取当前时间。 按顺序依次向N个Redis节点执行获取锁的操作。这个获取操作跟前面基于单Redis节点的获取锁的过程相同，包含随机字符串my_random_value，也包含过期时间(比如PX 30000，即锁的有效时间)。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个Redis节点获取锁失败以后，应该立即尝试下一个Redis节点。 计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。如果客户端从大多数Redis节点（&gt;= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。 如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。 如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有Redis节点发起释放锁的操作（即前面介绍的单机Redis Lua脚本释放锁的方法）。 RedLock释放锁客户端向所有Redis节点发起释放锁的操作，不管这些节点当时在获取锁的时候成功与否。 关于RedLock的问题讨论 如果有节点发生崩溃重启 假设一共有5个Redis节点：A, B, C, D, E。设想发生了如下的事件序列： 12345客户端1成功锁住了A, B, C，获取锁成功（但D和E没有锁住）。节点C崩溃重启了，但客户端1在C上加的锁没有持久化下来，丢失了。节点C重启后，客户端2锁住了C, D, E，获取锁成功。客户端1和客户端2同时获得了锁。复制代码 为了应对这一问题，antirez又提出了延迟重启(delayed restarts)的概念。也就是说，一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，这段时间应该大于锁的有效时间(lock validity time)。这样的话，这个节点在重启前所参与的锁都会过期，它在重启后就不会对现有的锁造成影响。 如果客户端长期阻塞导致锁过期 解释一下这个时序图，客户端1在获得锁之后发生了很长时间的GC pause，在此期间，它获得的锁过期了，而客户端2获得了锁。当客户端1从GC pause中恢复过来的时候，它不知道自己持有的锁已经过期了，它依然向共享资源（上图中是一个存储服务）发起了写数据请求，而这时锁实际上被客户端2持有，因此两个客户端的写请求就有可能冲突（锁的互斥作用失效了）。 如何解决这个问题呢?引入了fencing token的概念： 客户端1先获取到的锁，因此有一个较小的fencing token，等于33，而客户端2后获取到的锁，有一个较大的fencing token，等于34。客户端1从GC pause中恢复过来之后，依然是向存储服务发送访问请求，但是带了fencing token = 33。存储服务发现它之前已经处理过34的请求，所以会拒绝掉这次33的请求。这样就避免了冲突。 但是其实这已经超出了Redis实现分布式锁的范围，单纯用Redis没有命令来实现生成Token。 时钟跳跃问题 假设有5个Redis节点A, B, C, D, E。 123456客户端1从Redis节点A, B, C成功获取了锁（多数节点）。由于网络问题，与D和E通信失败。节点C上的时钟发生了向前跳跃，导致它上面维护的锁快速过期。客户端2从Redis节点C, D, E成功获取了同一个资源的锁（多数节点）。客户端1和客户端2现在都认为自己持有了锁。这个问题用Redis实现分布式锁暂时无解。而生产环境这种情况是存在的。复制代码 结论 Redis并不能实现严格意义上的分布式锁。但是这并不意味着上面讨论的方案一无是处。如果你的应用场景为了效率(efficiency)，协调各个客户端避免做重复的工作，即使锁失效了，只是可能把某些操作多做一遍而已，不会产生其它的不良后果。但是如果你的应用场景是为了正确性(correctness)，那么用Redis实现分布式锁并不合适，会存在各种各样的问题，且解决起来就很复杂，为了正确性，需要使用zab、raft共识算法，或者使用带有事务的数据库来实现严格意义上的分布式锁。 参考资料 Distributed locks with Redis 基于Redis的分布式锁到底安全吗（上）？ - 铁蕾的个人博客 martin.kleppmann.com/2016/02/08/…","tags":[{"name":"redis","slug":"redis","permalink":"http://wumuwumu.github.io/tags/redis/"}]},{"title":"protobuf使用","date":"2019-04-10T02:31:04.000Z","path":"2019/04/10/protobuf使用/","text":"安装1234wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.zipunzop protobuf-all-3.6.1.zipcd protobuf-all-3.6.1./configure &amp;&amp; make &amp;&amp; make install 语法规则123456789101112131415161718192021222324252627282930313233343536// 声明版本，默认是proto2syntax = &quot;proto3&quot;;// 声明包名package tutorialoption java_package = &quot;com.example.tutorial&quot;;// java类名option java_outer_classname = &quot;AddressBookProtos&quot;;message Person &#123; required string name =1; required int32 id = 2; optional string email = 3; enum PhoneType &#123; MOBILE = 0; HOME = 1; WORK = 2; &#125; message PhoneNumber &#123; required string number = 1; optional PhoneType type = 2[default = HOME]; &#125; repeated PhoneNumber phones = 4;&#125;message AddressBook &#123; repreated Person people = 1;&#125;// 保留字段，编程过程中某些功能没有想好，可以先把该tag 进行保留，以备以后使用。message Foo &#123; reserved 2, 15, 9 to 11; reserved &quot;foo&quot;, &quot;bar&quot;;&#125; 编码 https://blog.csdn.net/zxhoo/article/details/53228303 方法 Standard Message Methods isInitialized(): checks if all the required fields have been set. toString(): returns a human-readable representation of the message, particularly useful for debugging. mergeFrom(Message other): (builder only) merges the contents of other into this message, overwriting singular scalar fields, merging composite fields, and concatenating repeated fields. clear(): (builder only) clears all the fields back to the empty state. Parsing and Serialization byte[] toByteArray();: serializes the message and returns a byte array containing its raw bytes. static Person parseFrom(byte[] data);: parses a message from the given byte array. void writeTo(OutputStream output);: serializes the message and writes it to an OutputStream. static Person parseFrom(InputStream input);: reads and parses a message from an InputStream. 编译注意 升级协议 you must not change the tag numbers of any existing fields. you must not add or delete any required fields. you may delete optional or repeated fields. you may add new optional or repeated fields but you must use fresh tag numbers (i.e. tag numbers that were never used in this protocol buffer, not even by deleted fields). protobuf对repeated压缩不够好，所以尽量在后面加上[packed = true]。 不要让protobuf对象成为全局变量或者类成员，因为其clear方法只会把占用的内存空间清零，而不会释放，使得进程空间越来越大，可参考《Protobuf使用不当导致的程序内存上涨问题》。 https://www.jianshu.com/p/27fdf44dd63b","tags":[{"name":"go","slug":"go","permalink":"http://wumuwumu.github.io/tags/go/"}]},{"title":"go基本语法","date":"2019-04-10T02:29:55.000Z","path":"2019/04/10/go基本语法/","text":"接口 duck typing了解 在程序设计中，鸭子类型（英语：duck typing）是动态类型的一种风格。在这种风格中，一个对象有效的语义，不是由继承自特定的类或实现特定的接口，而是由“当前方法)和属性的集合”决定。 flagSync1. WaitGroup123Add()Done()Wait() 2. Context12 Regexp https://www.cnblogs.com/golove/p/3269099.html 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// MatchStringmatched, err := regexp.MatchString(&quot;foo.*&quot;, &quot;seafood&quot;)fmt.Println(matched, err)matched, err = regexp.MatchString(&quot;bar.*&quot;, &quot;seafood&quot;)fmt.Println(matched, err)// false error parsing regexp: missing closing ): `a(b`matched, err = regexp.MatchString(&quot;a(b&quot;, &quot;seafood&quot;)fmt.Println(matched, err)// true &lt;nil&gt;matched, err = regexp.MatchString(`a\\(b`, &quot;a(b&quot;)fmt.Println(matched, err)// false error parsing regexp: missing closing ): `a(b`matched, err = regexp.MatchString(`a(b`, &quot;a(b&quot;)fmt.Println(matched, err)// true &lt;nil&gt;matched, err = regexp.MatchString(&quot;a\\\\(b&quot;, &quot;a(b&quot;)fmt.Println(matched, err)// 将所有特殊字符进行转义fmt.Println(regexp.QuoteMeta(&quot;Escaping symbols like: .+*?()|[]&#123;&#125;^$&quot;))// ExpandStringcontent := ` # comment line option1: value1 option2: value2 # another comment line option3: value3`// Regex pattern captures &quot;key: value&quot; pair from the content.pattern := regexp.MustCompile(`(?m)(?P&lt;key&gt;\\w+):\\s+(?P&lt;value&gt;\\w+)$`)// Template to convert &quot;key: value&quot; to &quot;key=value&quot; by// referencing the values captured by the regex pattern.template := &quot;$key=$value\\n&quot;result := []byte&#123;&#125; // For each match of the regex in the content.for _, submatches := range pattern.FindAllStringSubmatchIndex(content, -1) &#123; // Apply the captured submatches to the template and append the output // to the result. result = pattern.ExpandString(result, template, content, submatches)&#125;fmt.Println(string(result))// findAllStringre := regexp.MustCompile(&quot;a.&quot;)fmt.Println(re.FindAllString(&quot;paranormal&quot;, -1))fmt.Println(re.FindAllString(&quot;paranormal&quot;, 2))fmt.Println(re.FindAllString(&quot;graal&quot;, -1))fmt.Println(re.FindAllString(&quot;none&quot;, -1))// FindAllStringSubmatchre := regexp.MustCompile(&quot;a(x*)b&quot;)fmt.Printf(&quot;%q\\n&quot;, re.FindAllStringSubmatch(&quot;-ab-&quot;, -1))fmt.Printf(&quot;%q\\n&quot;, re.FindAllStringSubmatch(&quot;-axxb-&quot;, -1))fmt.Printf(&quot;%q\\n&quot;, re.FindAllStringSubmatch(&quot;-ab-axb-&quot;, -1))fmt.Printf(&quot;%q\\n&quot;, re.FindAllStringSubmatch(&quot;-axxb-ab-&quot;, -1))// findStringSubmatch，只查找第一个re := regexp.MustCompile(&quot;a(x*)b(y|z)c&quot;)fmt.Printf(&quot;%q\\n&quot;, re.FindStringSubmatch(&quot;-axxxbyc-&quot;))fmt.Printf(&quot;%q\\n&quot;, re.FindStringSubmatch(&quot;-abzc-&quot;))","tags":[{"name":"go","slug":"go","permalink":"http://wumuwumu.github.io/tags/go/"}]},{"title":"makefile编写","date":"2019-04-10T02:27:53.000Z","path":"2019/04/10/makefile编写/","text":"例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091.PHONY: build clean test package package-deb ui api statics requirements ui-requirements serve update-vendor internal/statics internal/migrations static/swagger/api.swagger.jsonPKGS := $(shell go list ./... | grep -v /vendor |grep -v lora-app-server/api | grep -v /migrations | grep -v /static | grep -v /ui)VERSION := $(shell git describe --always |sed -e &quot;s/^v//&quot;)build: ui/build internal/statics internal/migrations mkdir -p build go build $(GO_EXTRA_BUILD_ARGS) -ldflags &quot;-s -w -X main.version=$(VERSION)&quot; -o build/lora-app-server cmd/lora-app-server/main.goclean: @echo &quot;Cleaning up workspace&quot; @rm -rf build dist internal/migrations internal/static ui/build static/static @rm -f static/index.html static/icon.png static/manifest.json static/asset-manifest.json static/service-worker.js @rm -rf static/logo @rm -rf docs/public @rm -rf disttest: internal/statics internal/migrations @echo &quot;Running tests&quot; @for pkg in $(PKGS) ; do \\ golint $$pkg ; \\ done @go vet $(PKGS) @go test -p 1 -v $(PKGS)documentation: @echo &quot;Building documentation&quot; @mkdir -p dist/docs @cd docs &amp;&amp; hugo @cd docs/public/ &amp;&amp; tar -pczf ../../dist/lora-app-server-documentation.tar.gz .dist: ui/build internal/statics internal/migrations @goreleaserbuild-snapshot: ui/build internal/statics internal/migrations @goreleaser --snapshotpackage-deb: package @echo &quot;Building deb package&quot; @cd packaging &amp;&amp; TARGET=deb ./package.shui/build: @echo &quot;Building ui&quot; @cd ui &amp;&amp; npm run build @mv ui/build/* staticapi: @echo &quot;Generating API code from .proto files&quot; @go generate api/api.gointernal/statics internal/migrations: static/swagger/api.swagger.json @echo &quot;Generating static files&quot; @go generate cmd/lora-app-server/main.gostatic/swagger/api.swagger.json: @echo &quot;Generating combined Swagger JSON&quot; @GOOS=&quot;&quot; GOARCH=&quot;&quot; go run api/swagger/main.go api/swagger &gt; static/swagger/api.swagger.json @cp api/swagger/*.json static/swagger# shortcuts for developmentrequirements: echo &quot;Installing development tools&quot; go get -u github.com/golang/lint/golint go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-swagger go get -u github.com/golang/protobuf/protoc-gen-go go get -u github.com/elazarl/go-bindata-assetfs/... go get -u github.com/jteeuwen/go-bindata/... go get -u github.com/kisielk/errcheck go get -u github.com/smartystreets/goconvey go get -u golang.org/x/tools/cmd/stringer go get -u github.com/golang/dep/cmd/dep go get -u github.com/goreleaser/goreleaser dep ensure -vui-requirements: @echo &quot;Installing UI requirements&quot; @cd ui &amp;&amp; npm installserve: build @echo &quot;Starting Lora App Server&quot; ./build/lora-app-serverupdate-vendor: @echo &quot;Updating vendored packages&quot; @govendor update +externalrun-compose-test: docker-compose run --rm appserver make test 文件格式12&lt;target&gt; : &lt;prerequisites&gt; [tab] &lt;commands&gt; target：执行的命令或者文件名。如果只是执行的命令这是伪指令，在大部分时候使用.PHONY声明伪指令，这样不仅仅提供效率，同时也避免和文件名冲突。 prerequisites：前置条件。 commands：需要执行的命令， 前面需要添加[tab]，如果想要换成其他的，使用.RECIPEPREFIX = ？换成你喜欢的。 执行命令的时候会打印出相关的命令内容，这个叫做回显，如果不想显示出来可以在命令前面添加@。 命令执行的时候，每行命令在不同一个shell中执行，如果想在同一个shell中执行，有下面几个办法。 将命令写在同一行 在命令后面添加\\，实现命令多行 使用.ONESHELL: 内置变量makefile可以通过=、:=、?=、+=给变量赋值，同时Make命令提供一系列内置变量，比如，((CC)指向当前使用的编译器，)(MAKE) 指向当前使用的Make工具。这主要是为了跨平台的兼容性，详细的内置变量清单见手册。 参考https://blog.csdn.net/u010230971/article/details/80335613 https://www.cnblogs.com/wang_yb/p/3990952.html http://www.ruanyifeng.com/blog/2015/02/make.html","tags":[{"name":"go","slug":"go","permalink":"http://wumuwumu.github.io/tags/go/"}]},{"title":"mybatis-sessions","date":"2019-04-10T02:25:51.000Z","path":"2019/04/10/mybatis/mybatis-sessions/","text":"SqlSessionFactorysqlSessionFactory是工厂类的接口，默认实现是DefaultSqlSessionFactory，通过sqlSessionFactoryBuilder创建，我们不具体讨论配置文件的具体解析，主要分析mybatis的运行流程。 SqlSessionFactory主要是用来创建SqlSession，SqlSession是线程不安全的，因此每次操作都要重新创建。 12345678910111213141516171819202122// 通过数据源创建SqlSession，是我们比较常用的一种方式private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); //通过事务工厂来产生一个事务 tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); //生成一个执行器(事务包含在执行器里) final Executor executor = configuration.newExecutor(tx, execType); //然后产生一个DefaultSqlSession return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; //如果打开事务出错，则关闭它 closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(\"Error opening session. Cause: \" + e, e); &#125; finally &#123; //最后清空错误上下文 ErrorContext.instance().reset(); &#125; &#125;SqlSession SqlSession有两方式调用方法，第一种方式是通过命名空间调用，第二种方式是JavaBean调用，也就是通过我们常用的Mapper接口进行调用。现在Myabtis3我们基本使用第二种方式。 通过Mapper接口进行调用，核心是 获取Mapper接口，并通过动态代理，进行方法拦截。 SqlSession通过getMapper获取相应的Mapper接口。SqlSession的的数据库操作是调用Executor的相关方法。 在getMapper调用的时候，有几个核心的类 MapperProxyFactory:用于创建MapperProxyd的工厂方法 MapperProxy:动态代理的InvocationHandler的实现，实际中就是执行sql语句 MapperRegistry MapperMethood:调用SqlSession的方法","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"},{"name":"mybatis","slug":"mybatis","permalink":"http://wumuwumu.github.io/tags/mybatis/"}]},{"title":"git基本操作","date":"2019-04-09T05:59:25.000Z","path":"2019/04/09/git基本操作/","text":"简介在实际开发中，会使用git作为版本控制工具来完成团队协作。因此，对基本的git操作指令进行总结是十分有必要的，本文对一些术语或者理论基础，不重新码字，可以参考廖雪峰老师的博文，本文只对命令做归纳总结。 git的通用操作流程如下图（来源于网络） 主要涉及到四个关键点： 工作区：本地电脑存放项目文件的地方，比如learnGitProject文件夹； 暂存区（Index/Stage）：在使用git管理项目文件的时候，其本地的项目文件会多出一个.git的文件夹，将这个.git文件夹称之为版本库。其中.git文件夹中包含了两个部分，一个是暂存区（Index或者Stage）,顾名思义就是暂时存放文件的地方，通常使用add命令将工作区的文件添加到暂存区里； 本地仓库：.git文件夹里还包括git自动创建的master分支，并且将HEAD指针指向master分支。使用commit命令可以将暂存区中的文件添加到本地仓库中； 远程仓库：不是在本地仓库中，项目代码在远程git服务器上，比如项目放在github上，就是一个远程仓库，通常使用clone命令将远程仓库拷贝到本地仓库中，开发后推送到远程仓库中即可； 更细节的来看： 日常开发时代码实际上放置在工作区中，也就是本地的XXX.java这些文件，通过add等这些命令将代码文教提交给暂存区（Index/Stage），也就意味着代码全权交给了git进行管理，之后通过commit等命令将暂存区提交给master分支上，也就是意味打了一个版本，也可以说代码提交到了本地仓库中。另外，团队协作过程中自然而然还涉及到与远程仓库的交互。 因此，经过这样的分析，git命令可以分为这样的逻辑进行理解和记忆： git管理配置的命令； 几个核心存储区的交互命令： 工作区与暂存区的交互； 暂存区与本地仓库（分支）上的交互； 本地仓库与远程仓库的交互。 安装git安装 https://git-scm.com/ 配置123456$ git config --global user.name \"Your Name\"$ git config --global user.email \"email@example.com\"$ git config --global core.editor emacs$ git config --list$ git config user.name 快速开始1234$ git init # 初始化工程$ git add * # 将文件添加到暂存区$ git commit -m # 提交$ git clone https://github.com/libgit2/libgit2 常用命令add git add -A 保存所有的修改 git add . 保存新的添加和修改，但是不包括删除 git add -u 保存修改和删除，但是不包括新建文件。 commit git commit -m git commit -ma // -a是添加全部修改 git commit –amend checkout git checkout — //使用暂缓区替换工作区 git checkout 切换分支 git checkout head — //直接使用本地参考的文件覆盖工作区文件 rm git rm // 删除工作区，并且提交 git rm —cached // 只删除暂存区 git rm -f // 暂存区和工作区都删除 reset谨慎使用！！！！！ –soft – 缓存区和工作目录都不会被改变 –mixed – 默认选项。缓存区和你指定的提交同步，但工作目录不受影响 –hard – 缓存区和工作目录都同步到你指定的提交 revert前提是已经提交，缺点：一次回滚过个记录会出现冲突。","tags":[{"name":"git","slug":"git","permalink":"http://wumuwumu.github.io/tags/git/"}]},{"title":"go工程搭建","date":"2019-04-09T01:26:21.000Z","path":"2019/04/09/go/go工程搭建/","text":"工程基本结构","tags":[{"name":"go","slug":"go","permalink":"http://wumuwumu.github.io/tags/go/"}]},{"title":"mysql权限管理","date":"2019-03-29T08:55:22.000Z","path":"2019/03/29/mysql/mysql权限管理/","text":"用户管理基本操作12345678create user zhangsan identified by 'zhangsan';SELECT current_user(); ← 查看当前用户SELECT user,host FROM mysql.user; ← 查看用户信息SHOW GRANTS; ← 当前用户权限，会生成SQL语句CREATE USER 'user'@'host' IDENTIFIED BY 'password'; ← 创建用户DROP USER 'user'@'host'; ← 删除用户RENAME USER 'user'@'host' TO 'fool'@'host'; 修改密码12345678mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'new-password'; ← 修改密码(recommand)mysql&gt; SET PASSWORD FOR 'root'@'localhost'=PASSWORD('new-password'); ← 修改密码mysql&gt; UPDATE mysql.user SET password=PASSWORD('new-password') WHERE USER='root' AND Host='127.0.0.1';mysql&gt; UPDATE mysql.user SET password='' WHERE user='root'; ← 清除密码mysql&gt; FLUSH PRIVILEGES;$ mysqladmin -uROOT -pOLD_PASSWD password NEW_PASSWD ← 通过mysqladmin修改$ mysqladmin -uROOT -p flush-privileges 权限管理1234567891011mysql&gt; GRANT ALL ON *.* TO 'user'@'%' [IDENTIFIED BY 'password'];mysql&gt; GRANT ALL ON [TABLE | DATABASE] student,course TO user1,user2;mysql&gt; GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, CREATE TEMPORARY, ALTER, DROP, REFERENCES, INDEX, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, EXECUTE ON db.tbl TO 'user'@'host' [IDENTIFIED BY 'password'];mysql&gt; GRANT ALL ON sampdb.* TO PUBLIC WITH GRANT OPTION; ← 所有人，可以授权给其他人mysql&gt; GRANT UPDATE(col),SELECT ON TABLE tbl TO user; ← 针对列赋值mysql&gt; SHOW GRANTS [FOR 'user'@'host']; ← 查看权限信息mysql&gt; REVOKE ALL ON *.* FROM 'user'@'host'; ← 撤销权限mysql&gt; REVOKE SELECT(user, host), UPDATE(host) ON db.tbl FROM 'user'@'%'; 权限admin12345mysql&gt; CREATE USER &apos;admin&apos;@&apos;IP&apos; IDENTIFIED BY &apos;password&apos;;mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;admin&apos;@&apos;IP&apos;;mysql&gt; REVOKE ALL PRIVILEGES ON *.* FROM &apos;admin&apos;@&apos;IP&apos;;mysql&gt; DROP USER &apos;admin&apos;@&apos;IP&apos;; root1mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'localhost' WITH GRANT OPTION; 其他重置root密码123456789101112131415161718----- 1. 停止mysql服务器# systemctl stop mysqld# /opt/mysql-5.7/bin/mysqladmin -uroot -p'init-password' shutdownShutting down MySQL.. done----- 2. 获取跳过认证的启动参数# mysqld --help --verbose | grep 'skip-grant-tables' -A1 --skip-grant-tables Start without grant tables. This gives all users FULL ACCESS to all tables.----- 3. 启动服务器，跳过认证# mysqld --skip-grant-tables --user=mysql &amp;[1] 10209----- 4. 取消密码mysql&gt; UPDATE mysql.user SET password='' WHERE user='root';Query OK, 2 rows affected (0.00 sec)Rows matched: 2 Changed: 2 Warnings: 0 密码策略参数解释validate_password_dictionary_file插件用于验证密码强度的字典文件路径。 validate_password_length密码最小长度，参数默认为8，它有最小值的限制，最小值为：validate_password_number_count + validate_password_special_char_count + (2 * validate_password_mixed_case_count) validate_password_mixed_case_count密码至少要包含的小写字母个数和大写字母个数。 validate_password_number_count密码至少要包含的数字个数。 validate_password_policy密码强度检查等级，0/LOW、1/MEDIUM、2/STRONG。有以下取值：Policy Tests Performed0 or LOW Length1 or MEDIUM Length; numeric, lowercase/uppercase, and special characters2 or STRONG Length; numeric, lowercase/uppercase, and special characters; dictionary file默认是1，即MEDIUM，所以刚开始设置的密码必须符合长度，且必须含有数字，小写或大写字母，特殊字符。 validate_password_special_char_count密码至少要包含的特殊字符数。 修改mysql参数配置123456789101112131415161718192021222324252627mysql&gt; set global validate_password_policy=0;Query OK, 0 rows affected (0.05 sec)mysql&gt; set global validate_password_mixed_case_count=0;Query OK, 0 rows affected (0.00 sec) mysql&gt; set global validate_password_number_count=3;Query OK, 0 rows affected (0.00 sec) mysql&gt; set global validate_password_special_char_count=0;Query OK, 0 rows affected (0.00 sec) mysql&gt; set global validate_password_length=3;Query OK, 0 rows affected (0.00 sec) mysql&gt; SHOW VARIABLES LIKE &apos;validate_password%&apos;;+--------------------------------------+-------+| Variable_name | Value |+--------------------------------------+-------+| validate_password_dictionary_file | || validate_password_length | 3 || validate_password_mixed_case_count | 0 || validate_password_number_count | 3 || validate_password_policy | LOW || validate_password_special_char_count | 0 |+--------------------------------------+-------+6 rows in set (0.00 sec) MySQL 中 localhost 127.0.0.1 区别% 是一个通配符，用以匹配所有的 IP 地址，但是不能匹配到 locahost 这个特殊的域名。 也就是说，如果要允许本地登录，单纯只配置一个 % 是不够的 (应该是说对这种方式是不够的)，需要同时配置一个 locahost 的账号。 123456789101112mysql&gt; GRANT ALL ON *.* TO 'foobar'@'%' IDENTIFIED BY '123456';Query OK, 0 rows affected (0.01 sec)mysql&gt; SELECT user, host, password FROM mysql.user WHERE user like 'foobar%';+--------+------+-------------------------------------------+| user | host | password |+--------+------+-------------------------------------------+| foobar | % | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |+--------+------+-------------------------------------------+1 row in set (0.00 sec)$ mysql -ufoobar -h127.0.0.1 -P3307 -p'123456'ERROR 1045 (28000): Access denied for user 'foobar'@'localhost' (using password: YES) https://jin-yang.github.io/post/mysql-localhost-vs-127.0.0.1-introduce.html 参考https://jin-yang.github.io/post/mysql-users.html https://www.cnblogs.com/Richardzhu/p/3318595.html","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"centos安装mysql","date":"2019-03-29T07:45:32.000Z","path":"2019/03/29/mysql/mysql安装/","text":"添加 MySQL YUM 源123456$wget 'https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm'$sudo rpm -Uvh mysql57-community-release-el7-11.noarch.rpm$yum repolist all | grep mysqlmysql-connectors-community/x86_64 MySQL Connectors Community 36mysql-tools-community/x86_64 MySQL Tools Community 47mysql57-community/x86_64 MySQL 5.7 Community Server 187 安装MySQL12345678910111213141516171819202122232425## 安装最新版$sudo yum install mysql-community-server$ sudo yum install mysql ## 安装客户端## 安装老版本## 1. yum-config-manager$ sudo dnf config-manager --disable mysql57-community$ sudo dnf config-manager --enable mysql56-community$ yum repolist | grep mysqlmysql-connectors-community/x86_64 MySQL Connectors Community 36mysql-tools-community/x86_64 MySQL Tools Community 47mysql56-community/x86_64 MySQL 5.6 Community Server 327## 2. 直接修改 /etc/yum.repos.d/mysql-community.repo# Enable to use MySQL 5.6[mysql56-community]name=MySQL 5.6 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.6-community/el/7/$basearch/enabled=1 #表示当前版本是安装gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql57-community]name=MySQL 5.7 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/7/$basearch/enabled=0 #默认这个是 1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql 启动Mysql123456789101112$sudo service mysqld start $sudo systemctl start mysqld #CentOS 7$sudo systemctl status mysqld● mysqld.service - MySQL Community Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2017-05-27 12:56:26 CST; 15s ago Process: 2482 ExecStartPost=/usr/bin/mysql-systemd-start post (code=exited, status=0/SUCCESS) Process: 2421 ExecStartPre=/usr/bin/mysql-systemd-start pre (code=exited, status=0/SUCCESS) Main PID: 2481 (mysqld_safe) CGroup: /system.slice/mysqld.service ├─2481 /bin/sh /usr/bin/mysqld_safe --basedir=/usr └─2647 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/... 修改密码1234## 获取临时密码sudo grep &apos;temporary password&apos; /var/log/mysqld.log$ mysql -uroot -p #输入查看到的密码mysql&gt; ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;MyNewPass4!&apos;; mysql的密码存在安全等级 1shell&gt; mysql_secure_installation 1mysql&gt; SHOW VARIABLES LIKE &apos;validate_password%&apos;; validate_password_number_count参数是密码中至少含有的数字个数，当密码策略是MEDIUM或以上时生效。 validate_password_special_char_count参数是密码中非英文数字等特殊字符的个数，当密码策略是MEDIUM或以上时生效。 validate_password_mixed_case_count参数是密码中英文字符大小写的个数，当密码策略是MEDIUM或以上时生效。 validate_password_length参数是密码的长度，这个参数由下面的公式生成 validate_password_number_count+ validate_password_special_char_count+ (2 * validate_password_mixed_case_count) validate_password_dictionary_file参数是指定密码验证的字典文件路径。 validate_password_policy这个参数可以设为0、1、2，分别代表从低到高的密码强度，此参数的默认值为1，如果想将密码强度改弱，则更改此参数为0。 修改密码策略更改密码策略为LOW 1mysql&gt; set global validate_password_policy=0; 更改密码长度 1mysql&gt; set global validate_password_length=0; 安全设置1234567## 会提示设置5个关键位置## 设置 root 密码## 禁止 root 账号远程登录## 禁止匿名账号（anonymous）登录## 删除测试库## 是否确认修改$ mysql_secure_installation 安装三方插件1yum --disablerepo=\\* --enablerepo=&apos;mysql*-community*&apos; list available 修改编码123456789## /etc/my.cnf[client]default-character-set = utf8[mysqld]default-storage-engine = INNODBcharacter-set-server = utf8collation-server = utf8_general_ci #不区分大小写collation-server = utf8_bin #区分大小写collation-server = utf8_unicode_ci #比 utf8_general_ci 更准确 修改服务器时间123456789101112131415## mysql 中默认的时间戳是 UTC 时间，需要改为服务器时间的话官网提供了 3 种方式$ mysql_tzinfo_to_sql tz_dir$ mysql_tzinfo_to_sql tz_file tz_name$ mysql_tzinfo_to_sql --leap tz_file## tz_dir 代表服务器时间数据库，CentOS 7 中默认的目录为 /usr/share/zoneinfo ，tz_name 为具体的时区。如果设置的时区需要闰秒，则使用 --leap，具体的用法如下：$ mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root -p mysql$ mysql_tzinfo_to_sql tz_file tz_name | mysql -u root mysql$ mysql_tzinfo_to_sql --leap tz_file | mysql -u root mysql&gt; set global time_zone = &apos;+8:00&apos;; ##修改mysql全局时区为北京时间，即我们所在的东8区&gt; set time_zone = &apos;+8:00&apos;; ##修改当前会话时区&gt; flush privileges; #立即生效## 通过修改my.cnf配置文件来修改时区# vim /etc/my.cnf ##在[mysqld]区域中加上default-time_zone = &apos;+8:00&apos;# /etc/init.d/mysqld restart ##重启mysql使新时区生效","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"ngrok环境搭建","date":"2019-03-29T01:28:43.000Z","path":"2019/03/29/运维/ngrok环境搭建/","text":"下载安装 配置golang环境 安装go 1yum install golang 配置GOPATH 安装git2 1234sudo yum remove gitsudo yum install epel-releasesudo yum install https://centos7.iuscommunity.org/ius-release.rpmsudo yum install git2u 下载ngrok 1go get github.com/inconshreveable/ngrok 生成证书 使用let’s encrypt证书 申请证书（具体看申请证书，主要通配符证书和三级域名） 修改证书 客户端证书 12cd ngrokcp /etc/letsencrypt/live/xncoding.com/chain.pem assets/client/tls/ngrokroot.crt 服务端证书 12cp /etc/letsencrypt/live/xncoding.com/cert.pem assets/server/tls/snakeoil.crtcp /etc/letsencrypt/live/xncoding.com/privkey.pem assets/server/tls/snakeoil.key 编译 编译服务端 1make release-server 编译客户端 不同平台的客户端需要分开编译。不同平台使用不同的 GOOS 和 GOARCH，GOOS为go编译出来的操作系统 (windows,linux,darwin)，GOARCH, 对应的构架 (386,amd64,arm) 123GOOS=linux GOARCH=amd64 make release-clientGOOS=windows GOARCH=amd64 make release-clientGOOS=linux GOARCH=arm make release-client 启动服务器在开启之前，请主要端口是否开放 1./ngrokd -domain=ngrok.sciento.top -httpAddr=:9580 -httpsAddr=:9443 -tunnelAddr=\":9444\" 启动客户端 配置文件,具体看官方文档 123456789101112server_addr: &quot;ngrok.sciento.top:9444&quot;trust_host_root_certs: falsetunnels: http: subdomain: &quot;demo&quot; proto: http: &quot;9000&quot; https: subdomain: &quot;demo&quot; proto: https: &quot;9000&quot; 启动 1./ngrok -config=ngrok.cfg start http https nginx配置 安装nginx 配置 123456789101112131415161718192021222324252627server &#123; listen 80; server_name demo.ngrok.xncoding.com; return 301 https://demo.ngrok.xncoding.com$request_uri;&#125;server &#123; listen 443 ssl http2; server_name demo.ngrok.xncoding.com; charset utf-8; ssl_certificate /etc/letsencrypt/live/demo.ngrok.xncoding.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/demo.ngrok.xncoding.com/privkey.pem; ssl_trusted_certificate /etc/letsencrypt/live/demo.ngrok.xncoding.com/chain.pem; access_log /var/log/nginx/ngrok.log main; error_log /var/log/nginx/ngrok_error.log error; location / &#123; proxy_pass http://127.0.0.1:5442; proxy_redirect off; proxy_set_header Host $http_host:5442; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; 参考https://www.xncoding.com/2017/12/29/web/ngrok.html https://www.coldawn.com/how-to-issue-acmev2-wildcard-certificates-with-certbot-on-centos-7/ https://www.jianshu.com/p/c5c9d071e395 http://ngrok.cn/docs.html#tcp","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"Druid初始化","date":"2019-03-25T10:17:33.000Z","path":"2019/03/25/Druid初始化/","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236public void init() throws SQLException &#123; if (inited) &#123; return; &#125; // bug fixed for dead lock, for issue #2980 DruidDriver.getInstance(); final ReentrantLock lock = this.lock; try &#123; lock.lockInterruptibly(); &#125; catch (InterruptedException e) &#123; throw new SQLException(\"interrupt\", e); &#125; boolean init = false; try &#123; //双重检查 if (inited) &#123; return; &#125; initStackTrace = Utils.toString(Thread.currentThread().getStackTrace()); this.id = DruidDriver.createDataSourceId(); if (this.id &gt; 1) &#123; long delta = (this.id - 1) * 100000; this.connectionIdSeedUpdater.addAndGet(this, delta); this.statementIdSeedUpdater.addAndGet(this, delta); this.resultSetIdSeedUpdater.addAndGet(this, delta); this.transactionIdSeedUpdater.addAndGet(this, delta); &#125; if (this.jdbcUrl != null) &#123; this.jdbcUrl = this.jdbcUrl.trim(); initFromWrapDriverUrl(); &#125; for (Filter filter : filters) &#123; filter.init(this); &#125; if (this.dbType == null || this.dbType.length() == 0) &#123; this.dbType = JdbcUtils.getDbType(jdbcUrl, null); &#125; if (JdbcConstants.MYSQL.equals(this.dbType) || JdbcConstants.MARIADB.equals(this.dbType) || JdbcConstants.ALIYUN_ADS.equals(this.dbType)) &#123; boolean cacheServerConfigurationSet = false; if (this.connectProperties.containsKey(\"cacheServerConfiguration\")) &#123; cacheServerConfigurationSet = true; &#125; else if (this.jdbcUrl.indexOf(\"cacheServerConfiguration\") != -1) &#123; cacheServerConfigurationSet = true; &#125; if (cacheServerConfigurationSet) &#123; this.connectProperties.put(\"cacheServerConfiguration\", \"true\"); &#125; &#125; if (maxActive &lt;= 0) &#123; throw new IllegalArgumentException(\"illegal maxActive \" + maxActive); &#125; if (maxActive &lt; minIdle) &#123; throw new IllegalArgumentException(\"illegal maxActive \" + maxActive); &#125; if (getInitialSize() &gt; maxActive) &#123; throw new IllegalArgumentException(\"illegal initialSize \" + this.initialSize + \", maxActive \" + maxActive); &#125; if (timeBetweenLogStatsMillis &gt; 0 &amp;&amp; useGlobalDataSourceStat) &#123; throw new IllegalArgumentException(\"timeBetweenLogStatsMillis not support useGlobalDataSourceStat=true\"); &#125; if (maxEvictableIdleTimeMillis &lt; minEvictableIdleTimeMillis) &#123; throw new SQLException(\"maxEvictableIdleTimeMillis must be grater than minEvictableIdleTimeMillis\"); &#125; if (this.driverClass != null) &#123; this.driverClass = driverClass.trim(); &#125; initFromSPIServiceLoader(); // 处理驱动 if (this.driver == null) &#123; if (this.driverClass == null || this.driverClass.isEmpty()) &#123; this.driverClass = JdbcUtils.getDriverClassName(this.jdbcUrl); &#125; if (MockDriver.class.getName().equals(driverClass)) &#123; driver = MockDriver.instance; &#125; else &#123; if (jdbcUrl == null &amp;&amp; (driverClass == null || driverClass.length() == 0)) &#123; throw new SQLException(\"url not set\"); &#125; driver = JdbcUtils.createDriver(driverClassLoader, driverClass); &#125; &#125; else &#123; if (this.driverClass == null) &#123; this.driverClass = driver.getClass().getName(); &#125; &#125; // 进行参数的核对，没有什么逻辑 initCheck(); // 为不同的数据库处理异常，这个可以借鉴 initExceptionSorter(); initValidConnectionChecker(); // 做了一些检查，不知道 validationQueryCheck(); // 创建数据统计对象 if (isUseGlobalDataSourceStat()) &#123; dataSourceStat = JdbcDataSourceStat.getGlobal(); if (dataSourceStat == null) &#123; dataSourceStat = new JdbcDataSourceStat(\"Global\", \"Global\", this.dbType); JdbcDataSourceStat.setGlobal(dataSourceStat); &#125; if (dataSourceStat.getDbType() == null) &#123; dataSourceStat.setDbType(this.dbType); &#125; &#125; else &#123; dataSourceStat = new JdbcDataSourceStat(this.name, this.jdbcUrl, this.dbType, this.connectProperties); &#125; dataSourceStat.setResetStatEnable(this.resetStatEnable); // 创建连接池 connections = new DruidConnectionHolder[maxActive]; evictConnections = new DruidConnectionHolder[maxActive]; keepAliveConnections = new DruidConnectionHolder[maxActive]; SQLException connectError = null; // 同步或者异步创建线程池 if (createScheduler != null &amp;&amp; asyncInit) &#123; for (int i = 0; i &lt; initialSize; ++i) &#123; createTaskCount++; CreateConnectionTask task = new CreateConnectionTask(true); this.createSchedulerFuture = createScheduler.submit(task); &#125; &#125; else if (!asyncInit) &#123; // init connections while (poolingCount &lt; initialSize) &#123; try &#123; PhysicalConnectionInfo pyConnectInfo = createPhysicalConnection(); DruidConnectionHolder holder = new DruidConnectionHolder(this, pyConnectInfo); connections[poolingCount++] = holder; &#125; catch (SQLException ex) &#123; LOG.error(\"init datasource error, url: \" + this.getUrl(), ex); if (initExceptionThrow) &#123; connectError = ex; break; &#125; else &#123; Thread.sleep(3000); &#125; &#125; &#125; if (poolingCount &gt; 0) &#123; poolingPeak = poolingCount; poolingPeakTime = System.currentTimeMillis(); &#125; &#125; // 用来打印线程池 createAndLogThread(); createAndStartCreatorThread(); // 停止 createAndStartDestroyThread(); // 等待线程创建完成 initedLatch.await(); init = true; initedTime = new Date(); // 注册mbean registerMbean(); if (connectError != null &amp;&amp; poolingCount == 0) &#123; throw connectError; &#125; // 检查连接池，防止连接池超出最大连接池 if (keepAlive) &#123; // async fill to minIdle if (createScheduler != null) &#123; for (int i = 0; i &lt; minIdle; ++i) &#123; createTaskCount++; CreateConnectionTask task = new CreateConnectionTask(true); this.createSchedulerFuture = createScheduler.submit(task); &#125; &#125; else &#123; this.emptySignal(); &#125; &#125; &#125; catch (SQLException e) &#123; LOG.error(\"&#123;dataSource-\" + this.getID() + \"&#125; init error\", e); throw e; &#125; catch (InterruptedException e) &#123; throw new SQLException(e.getMessage(), e); &#125; catch (RuntimeException e)&#123; LOG.error(\"&#123;dataSource-\" + this.getID() + \"&#125; init error\", e); throw e; &#125; catch (Error e)&#123; LOG.error(\"&#123;dataSource-\" + this.getID() + \"&#125; init error\", e); throw e; &#125; finally &#123; // 初始化成功 inited = true; // 解锁 lock.unlock(); if (init &amp;&amp; LOG.isInfoEnabled()) &#123; String msg = \"&#123;dataSource-\" + this.getID(); if (this.name != null &amp;&amp; !this.name.isEmpty()) &#123; msg += \",\"; msg += this.name; &#125; msg += \"&#125; inited\"; LOG.info(msg); &#125; &#125; &#125;","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"java多线程","date":"2019-02-15T08:37:30.000Z","path":"2019/02/15/java/java多线程/","text":"相关的类 Runnable Thread Callable:比Runnable有个返回值 Future FutureTask","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"sqlx基本使用","date":"2019-02-13T08:15:58.000Z","path":"2019/02/13/go/sqlx基本使用/","text":"安装1go get github.com/jmoiron/sqlx 连接数据库12345678910var db *sqlx.DB // exactly the same as the built-indb = sqlx.Open(\"sqlite3\", \":memory:\") // from a pre-existing sql.DB; note the required driverNamedb = sqlx.NewDb(sql.Open(\"sqlite3\", \":memory:\"), \"sqlite3\") // force a connection and test that it workederr = db.Ping() 查询Exec直接执行，适合add,update,delete 1234567891011121314schema := `CREATE TABLE place ( country text, city text NULL, telcode integer);` // execute a query on the serverresult, err := db.Exec(schema) // or, you can use MustExec, which panics on errorcityState := `INSERT INTO place (country, telcode) VALUES (?, ?)`countryCity := `INSERT INTO place (country, city, telcode) VALUES (?, ?, ?)`db.MustExec(cityState, \"Hong Kong\", 852)db.MustExec(cityState, \"Singapore\", 65)db.MustExec(countryCity, \"South Africa\", \"Johannesburg\", 27) Query查询数据库，适合select 12345678910111213141516171819202122// fetch all places from the dbrows, err := db.Query(\"SELECT country, city, telcode FROM place\") // iterate over each rowfor rows.Next() &#123; var country string // note that city can be NULL, so we use the NullString type var city sql.NullString var telcode int err = rows.Scan(&amp;country, &amp;city, &amp;telcode)&#125;// queryx 可以对结果转换成结构体var person2 User rowxs,err :=db.Queryx(\"SELECT * FROM sys_user LIMIT 1\") if err != nil&#123; panic(err) &#125; for rowxs.Next()&#123; rowxs.StructScan(&amp;person2) fmt.Println(person2) &#125; Select12345678910111213141516p := Place&#123;&#125;pp := []Place&#123;&#125; // this will pull the first place directly into perr = db.Get(&amp;p, \"SELECT * FROM place LIMIT 1\") // this will pull places with telcode &gt; 50 into the slice pperr = db.Select(&amp;pp, \"SELECT * FROM place WHERE telcode &gt; ?\", 50) // they work with regular types as wellvar id interr = db.Get(&amp;id, \"SELECT count(*) FROM place\") // fetch at most 10 place namesvar names []stringerr = db.Select(&amp;names, \"SELECT name FROM place LIMIT 10\") 事务1234// this will not work if connection pool &gt; 1db.MustExec(\"BEGIN;\")db.MustExec(...)db.MustExec(\"COMMIT;\") 预编译123456stmt, err := db.Prepare(`SELECT * FROM place WHERE telcode=?`)row = stmt.QueryRow(65) tx, err := db.Begin()txStmt, err := tx.Prepare(`SELECT * FROM place WHERE telcode=?`)row = txStmt.QueryRow(852) Named Queries12345678910111213141516171819202122232425// named query with a structp := Place&#123;Country: \"South Africa\"&#125;rows, err := db.NamedQuery(`SELECT * FROM place WHERE country=:country`, p) // named query with a mapm := map[string]interface&#123;&#125;&#123;\"city\": \"Johannesburg\"&#125;result, err := db.NamedExec(`SELECT * FROM place WHERE city=:city`, m)p := Place&#123;TelephoneCode: 50&#125;pp := []Place&#123;&#125; // select all telcodes &gt; 50nstmt, err := db.PrepareNamed(`SELECT * FROM place WHERE telcode &gt; :telcode`)err = nstmt.Select(&amp;pp, p)arg := map[string]interface&#123;&#125;&#123; \"published\": true, \"authors\": []&#123;8, 19, 32, 44&#125;,&#125;query, args, err := sqlx.Named(\"SELECT * FROM articles WHERE published=:published AND author_id IN (:authors)\", arg)query, args, err := sqlx.In(query, args...)query = db.Rebind(query)db.Query(query, args...) 参考 http://jmoiron.github.io/sqlx/","tags":[{"name":"go","slug":"go","permalink":"http://wumuwumu.github.io/tags/go/"}]},{"title":"jquery基本操作","date":"2019-01-12T08:20:45.000Z","path":"2019/01/12/jquery基本操作/","text":"选择器123456789101112// 基本选择器$('#id')$('.class')$('element')$('*')$('select1 ,select2')//可以使用css选择器// 层次选择器$('ancestor descendant')$('parent &gt;child')$('prev+next')$('prev~siblings')//获取所有同辈元素 DOM操作基本操作123456789101112131415161718192021222324252627282930313233343536// attr$('div').attr(\"background\")//获取属性$('div').attr(\"background\",\"white\")$('div').attr(&#123;\"background\":\"white\",\"height\":\"200px\"&#125;)// css$(\"div\").css('background')$('div').css(\"background\",\"white\")$('div').css(&#123;'background':'blue',\"height\":'200px'&#125;)// width heightwidth()height()// addClass$('div').addClass('className');// removeAttr$('div').removeAttr('background')// removeClass 没参数删除所有// hasClass// 创建节点var p $('&lt;p&gt;hello&lt;/p&gt;')// append() 添加内容// appendTo()// prepend() 向元素内部前面添加内容// prependTo()​``` html&lt;p&gt;hello&lt;/p&gt;​ $(‘hi!‘).prependTo(“p”)​12&lt;p&gt;&lt;i&gt;hi!&lt;/i&gt;hello&lt;/p&gt;​ // 在相应位置添加元素，是在元素的外面// after// insertAfter// before//insertBefore // remove()// detach()：和remove()几乎一样，不同的是detach方法不会删除节点所绑定的事件和附加的数据// empty() 清空内容 // clone()复制节点，可以有参数true，当有true参数时，将同时复制节点所绑定的事件// replaceWith 将匹配的节点替换成指定的节点// replaceAll() 只是用一个 // wrap 包裹节点// wrapAll// wrapInner 将匹配的节点内部的节点或者文本内容用指定的节点包裹起来​12&lt;p&gt;我是内容&lt;/p&gt;​ $(“p”).wrapInner(““);​12&lt;p&gt;&lt;span&gt;我是内容&lt;/span&gt;&lt;/p&gt;​ // html()// text()// val() // children()// next()// prev()// siblings()// closest() 获取最近的符合匹配的一个父元素​123456&lt;div&gt;&lt;div class=&quot;div2&quot;&gt;&lt;p&gt;我是内容&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;​ var $div=$(“p”).closest();//返回class为div2的div元素 // parent()// parents() // offset()// position() // scrollTop()// scrollLeft() 1234567891011121314151617181920212223242526272829303132# 事件与动画```js$().ready()$(&apos;&apos;).bind(type,func)$(&apos;&apos;).click()$(&apos;&apos;).mouseover// 合成事件hover(enter,leave)toggle(fn1,fn2) // 阻止事件event.stopPropagation();event.preventDefault();// unbind 移除事件// trigger 触发事件// 动画hide();show(time);fadeLn();fadeOut();slideUp();slideDown();slideToggle();fadeTo();fadeToggle();animate();delay(); 参考 jQuery简明参考手册——30分钟快速入门jQuery","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"},{"name":"jquery","slug":"jquery","permalink":"http://wumuwumu.github.io/tags/jquery/"}]},{"title":"springcloud-eureka","date":"2019-01-06T10:27:25.000Z","path":"2019/01/06/springcloud-eureka/","text":"建立工程 添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt;&lt;/dependency&gt; 添加Application 1234567@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication &#123; public static void main(String[] arg)&#123; SpringApplication.run(EurekaApplication.class,arg); &#125;&#125; 添加配置文件 123456789101112131415server: port: 8761eureka: instance: hostname: localhost client: registerWithEureka: false ## 是否注册到eureka server fetchRegistry: false ## 是否获取Eureka server 注册信息，单机可以设置为false serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ ## 默认http://localhost:8761/eurekaspring: application: name: eurka-server 运行工程，访问127.0.0.1:9761可以看到web界面。 安全 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; 添加配置 老版本 123456security: basic: true user: name: wumu password: wumu 新版本 1234security: user: name: wumu password: wumu 问题 在依赖包中同时添加的spring-cloud-starter-netflix-eureka-server与springb-boot-starter-web两个依赖会导致tomcat的依赖问题，应用不能启动。","tags":[{"name":"spring-cloud","slug":"spring-cloud","permalink":"http://wumuwumu.github.io/tags/spring-cloud/"}]},{"title":"Tiemline设计方案","date":"2019-01-04T12:33:08.000Z","path":"2019/01/04/Tiemline设计方案/","text":"参考 朋友圈式的TIMELINE设计方案 朋友圈的设计及实现 几个大型网站的Feeds(Timeline)设计简单对比","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"查找附近的人","date":"2019-01-04T11:46:12.000Z","path":"2019/01/04/查找附近的人/","text":"GeoHash比较原始的方法，简单方便 Mysql计算公式 12C = sin(MLatA)*sin(MLatB)*cos(MLonA-MLonB) + cos(MLatA)*cos(MLatB)Distance = R*Arccos(C)*Pi/180 在经纬度小节中我们了解了两个公式用来计算两个位置之间的距离, 该小节我们以测试数据说明如何实现.测试需要的表结构和数据: 1234567891011121314151617表结构:CREATE TABLE `geotest` ( `userid` int(10) NOT NULL, `longitude` decimal(9,6) NOT NULL, `latitude` decimal(9,6) NOT NULL, `create_time` datetime DEFAULT NULL, UNIQUE KEY `unq_uid` (`userid`), KEY `idx_lat_lng` (`longitude`,`latitude`)) ENGINE=InnoDB DEFAULT CHARSET=utf8测试数据:insert geotest values(10000, 116.417480, 40.003033, now());insert geotest values(10001, 116.437480, 40.004033, now());insert geotest values(10002, 116.457480, 40.005033, now());insert geotest values(10003, 116.477480, 40.006033, now());............ 第一种公式中, google 为我们介绍了如何使用 sql 来获取附近的点, 如下所示, 我们选用 6371km 作为地球的半径,根据上述小节的计算公式推断: 12C = sin(MLatA)*sin(MLatB)*cos(MLonA-MLonB) + cos(MLatA)*cos(MLatB)Distance = R*Arccos(C)*Pi/180 google 地图的计算公式可以参考 geo_search 两个位置之间的距离则可以换算成以下公式: 1R*arccos( cos( radians(latA)*cos( radians(latB) ) * cos( radians(lonA - lonB) )) + sin( radians(latA)*cos(latB) )) radians 函数计算出相应的弧度信息, 得到下面的 sql: 1234567891011121314SELECT user_id, ( 6371 * acos ( cos ( radians(40.003033) ) * cos( radians( latitude ) ) * cos( radians( longitude ) - radians(116.417481) ) + sin ( radians(40.003033) ) * sin( radians( latitude ) ) ) ) AS distanceFROM geotestHAVING distance &lt; 1ORDER BY distanceLIMIT 0 , 20; 上面的 sql 从 geotest 中从 geotest 表中获取到经度(116.417481) 和纬度(40.003033) 位置附近 1km 所有的user_id 信息.观察这个 sql, 可以预见到在表数据较大的时候仅建立复合索引 idx_lat_lng 肯定会遇到性能瓶颈, 因为每行记录都需要做相关的运算, 才能跑出最后的结果. 所以要提高该 sql 的性能就需要尽量过滤不需要的 longitude 和 latitude 两列的值. 参考 geo_search 和 fastest-way-to-find-distance, 在近距离的情况下我们可以认为当前区域内的所有位置都在一个平面内, 虽然有点误差, 但是比起地球这么大的椭球, 我们完全可以忽略其中的误差. 以经纬度来讲, 1 纬度约等于 69 英里, 大约 111044.736 米, 其中的换算公式为: 121°latitude ~= 69 miles1°longitude ~= cos(latitude)*69 miles 所以对于位置信息(lng, lat), 我们可以计算出以其为中心周边指定距离的四个点, 如下图所示: 1234567+-------------+| || || + || || |+-------------+ 计算公式如下: 1234lng1 = lon - dist/abs(cos(radians(lat))*69)lng2 = lon + dist/abs(cos(radians(lat))*69)lat1 = lat - (dist/69);lat2 = lat + (dist/69); 四个点的坐标就分别为 (lng1, lat1), (lng1, lat2), (lng2, lat1), (lng2, lat2), 所以存在于该四个点组成的平面之间的点即可以被认为在(lng, lat) 的 dist 距离内. 基于上述的规则, 修改 sql 为以下: 12345678910111213141516SELECT user_id, ( 6371 * acos ( cos ( radians(40.003033) ) * cos( radians( latitude ) ) * cos( radians( longitude ) - radians(116.417481) ) + sin ( radians(40.003033) ) * sin( radians( latitude ) ) ) ) AS distanceFROM geotestWHERE longitude BETWEEN lng1 AND lng2AND latitude BETWEEN lat1 AND lat2HAVING distance &lt; 1ORDER BY distanceLIMIT 0 , 20; 这样就能很好的使用索引, 如果还想增加超时设置, 可以在 sql 里加上 create_time 条件进行过滤, 比如只查找最近一天的附近的用户. 另外开发者也可以结合使用 sphinx 或 elasticsearch 得到更好的性能. 下面为根据上面介绍的规则整理成存储过程, 方便开发者调用访问. 这里我们将地球半径的公里数转换为米即为 6371392.89m, 69英里则转为 111044.736m, 如下存储过程返回 user_id 和 距离(米): 12345678910111213141516171819202122232425262728293031DELIMITER $$drop procedure if exists geo_dist$$create procedure geo_dist(IN lng decimal(9, 6), IN lat decimal(9, 6), IN dist int)begin declare lng1 decimal(9, 6); declare lng2 decimal(16, 13); declare lat1 decimal(9, 6); declare lat1 decimal(16, 13); -- calculate lng and lat for the rectangle, in meters unit set lng1 = lng - dist/abs(cos(radians(lat))*111044.736); set lng2 = lng + dist/abs(cos(radians(lat))*111044.736); set lat1 = lat - (dist/111044.736); set lat2 = lat + (dist/111044.736); -- run the query select user_id, round(( 6371392.89 * acos ( cos ( radians(lat) ) * cos( radians( latitude ) ) * cos( radians( longitude ) - radians(lng) ) + sin ( radians(lat) ) * sin( radians( latitude ) ) ) ), 0) AS distance from user_position where lng between lng1 and lng2 and lat between lat1 and lat2 having distance &lt; dist ORDER BY distance LIMIT 0 , 20;END$$DELIMITER ; 运行存储过程, 取出该经纬度下附近 5km 的用户和距离(m): 1234567891011mysql &gt; call geo_dist(116.4174800000000, 40.0030330000000, 5000);+---------+----------+| user_id | distance |+---------+----------+| 10000 | 0 || 10001 | 1707 || 10002 | 3414 |+---------+----------+3 rows in set (0.00 sec)Query OK, 0 rows affected (0.01 sec) 10001 用户和指定的经纬度距离为1707米, 我们在 redis 3.2 版本中进行简单测试, 可以看到结果都很相近: 123456127.0.0.1:6380&gt; geoadd tttt 116.417480 40.003033 t1(integer) 0127.0.0.1:6380&gt; geoadd tttt 116.437481 40.004034 t2(integer) 0127.0.0.1:6380&gt; GEODIST tttt t1 t2&quot;1707.5093&quot; mongodb创建位置索引 参考 使用 MySQL 实现搜索附近的人 GeoHash算法学习讲解、解析及原理分析","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"},{"name":"mongodb","slug":"mongodb","permalink":"http://wumuwumu.github.io/tags/mongodb/"},{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"springboot-mongodb使用","date":"2019-01-04T01:43:06.000Z","path":"2019/01/04/springboot-mongodb使用/","text":"基本注解 @id @Document @DBRef $Indexed @CompoundIndex @GenSpatialIndexed @Transient @PersistenceConstructor","tags":[{"name":"springboot","slug":"springboot","permalink":"http://wumuwumu.github.io/tags/springboot/"},{"name":"mongodb","slug":"mongodb","permalink":"http://wumuwumu.github.io/tags/mongodb/"}]},{"title":"cordova打包vue","date":"2019-01-02T09:12:09.000Z","path":"2019/01/02/cordova打包vue/","text":"https://segmentfault.com/a/1190000013159076","tags":[{"name":"vue","slug":"vue","permalink":"http://wumuwumu.github.io/tags/vue/"}]},{"title":"Oracle表管理","date":"2018-12-29T13:45:47.000Z","path":"2018/12/29/oracle/Oracle表管理/","text":"数据类型123456789101112131415## 字符型char 定长，后面空格补全varchar2() 变长clob 字符型大对象## 数字类型numbernumber(5，2) 标识5位有效数，2位小数-999.99-999.99number(5) 5位整数## 日期类型datetimestramp## 图片blob 二进制4g,为了安全可以放入数据库 表操作12345678910create table table_name()drop table table_name;rename table_name to other_table_name;alter table table_name add ...;alter table table_name modify ...;alter table table_name drop column ...;","tags":[{"name":"oracle","slug":"oracle","permalink":"http://wumuwumu.github.io/tags/oracle/"}]},{"title":"Oracle基本管理","date":"2018-12-29T12:37:37.000Z","path":"2018/12/29/oracle/Oracle基本管理/","text":"用户管理12345678910111213141516171819202122232425## 创建用户create user test identified by test;show user;## 删除用户delete test (cascade);## 修改用户alter user test identified by wumu;alter user test expire;## 用户口令## 密码输错三次就密码锁定2天create profile lock_account limit failed_login_attempts 3 password_lock_time 2;alter user tea profile lock_account;## 解锁alter user tea account unlock;## 每10天需要修改密码，宽限期为两天create profile myprofile limit password_life_time 10 password_grace_time 2;alter user tea profile myprofile;## 口令10天后可以重用create profile password_history limit password_lift_time 10 password_grace_time 2 password_reuse_time 10## 撤销profiledrop profile my_profile CASCADE； 权限管理12345678910111213141516171819202122232425## 授权grant system_privilege|all privileges to &#123;user identified by password |role|&#125;[with admin option]grant object_privileage | Allon schema.objectto user | role[with admin option][with the grant any object]grant select on test to wumu with grant option;grant connect tp wumu with admin option;## create session 用于登录## dba 管路员## resource 可以建表## desc table_name## 撤销权限## 如果授权者的权限被撤回，那么它的被授予者也会失去相关的权限invoke system_privilege from user|roleinvoke object_privilege|All on scheme.object from user|role [cascade contraints]## 查询权限## 系统权限放在DBA_SYS_PRIVS## 对象权限放在数据字典DBA_TAB_PRIVS","tags":[{"name":"oracle","slug":"oracle","permalink":"http://wumuwumu.github.io/tags/oracle/"}]},{"title":"Vue插件开发","date":"2018-12-17T12:52:34.000Z","path":"2018/12/17/vue/Vue插件开发/","text":"基本结构插件的功能包括全局方法和属性、指令、mixin、实例方法。插件都有一个install方法，第一个参数是Vue，第二个参数是options。 1234567891011121314151617181920MyPlugin.install = function (Vue, options) &#123; Vue.myGlobalMethod = function () &#123; // 1. 添加全局方法或属性，如: vue-custom-element // 逻辑... &#125; Vue.directive('my-directive', &#123; // 2. 添加全局资源：指令/过滤器/过渡等，如 vue-touch bind (el, binding, vnode, oldVnode) &#123; // 逻辑... &#125; ... &#125;) Vue.mixin(&#123; created: function () &#123; // 3. 通过全局 mixin方法添加一些组件选项，如: vuex // 逻辑... &#125; ... &#125;) Vue.prototype.$myMethod = function (options) &#123; // 4. 添加实例方法，通过把它们添加到 Vue.prototype 上实现 // 逻辑... &#125;&#125; vue-toast","tags":[{"name":"vue","slug":"vue","permalink":"http://wumuwumu.github.io/tags/vue/"},{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"}]},{"title":"JSqlParser教程","date":"2018-12-05T13:58:31.000Z","path":"2018/12/05/JSqlParser教程/","text":"解析获取表名12345//获取所有使用过的表Statement statement = CCJSqlParserUtil.parse(\"SELECT * FROM MY_TABLE1\"); Select selectStatement = (Select) statement; TablesNamesFinder tablesNamesFinder = new TablesNamesFinder(); List&lt;String&gt; tableList = tablesNamesFinder.getTableList(selectStatement); 应用别名1234// SELECT a AS A1, b AS A2, c AS A3 FROM testSelect select = (Select) CCJSqlParserUtil.parse(\"select a,b,c from test\"); final AddAliasesVisitor instance = new AddAliasesVisitor(); select.getSelectBody().accept(instance); 添加一列或者表达式123// SELECT a, b FROM mytableSelect select = (Select) CCJSqlParserUtil.parse(\"select a from mytable\");SelectUtils.addExpression(select, new Column(\"b\")); 添加where语句新建where12345678Select select = (Select) CCJSqlParserUtil.parse(\"select name from user\"); PlainSelect plainSelect = (PlainSelect) select.getSelectBody(); if (plainSelect.getWhere() == null) &#123; EqualsTo equalsTo = new EqualsTo(); equalsTo.setLeftExpression(new Column(\"id\")); equalsTo.setRightExpression(new LongValue(1000L)); plainSelect.setWhere(equalsTo); &#125; 添加where12345678910111213Select select = (Select) CCJSqlParserUtil.parse(\"select name from user where id = 1000\"); PlainSelect plainSelect = (PlainSelect) select.getSelectBody(); // 原where表达式 Expression where = plainSelect.getWhere(); // 新增的查询条件表达式 EqualsTo equalsTo = new EqualsTo(); equalsTo.setLeftExpression(new Column(\"name\")); equalsTo.setRightExpression(new StringValue(\"'张三'\")); // 用and链接条件 AndExpression and = new AndExpression(where, equalsTo); // 设置新的where条件 plainSelect.setWhere(and); 添加null12345678910111213Select select = (Select) CCJSqlParserUtil.parse(\"select name from user where id = 1000\"); PlainSelect plainSelect = (PlainSelect) select.getSelectBody(); // 原where表达式 Expression where = plainSelect.getWhere(); // 新增的null判断条件 IsNullExpression isNullExpression = new IsNullExpression(); isNullExpression.setLeftExpression(new Column(\"name\")); isNullExpression.setNot(true); // 用and链接条件 AndExpression and = new AndExpression(where, isNullExpression); // 设置新的where条件 plainSelect.setWhere(and); 生成扩展插入123456789101112131415161718192021222324// INSERT INTO mytable (col1) VALUES (1)// INSERT INTO mytable (col1, col2) VALUES (1, 5)// INSERT INTO mytable (col1, col2, col3) VALUES (1, 5, 10)Insert insert = (Insert) CCJSqlParserUtil.parse(\"insert into mytable (col1) values (1)\"); System.out.println(insert.toString()); insert.getColumns().add(new Column(\"col2\")); insert.getItemsList().accept(new ItemsListVisitor() &#123; public void visit(SubSelect subSelect) &#123; throw new UnsupportedOperationException(\"Not supported yet.\"); &#125; public void visit(ExpressionList expressionList) &#123; expressionList.getExpressions().add(new LongValue(5)); &#125; public void visit(MultiExpressionList multiExprList) &#123; throw new UnsupportedOperationException(\"Not supported yet.\"); &#125; &#125;); System.out.println(insert.toString()); insert.getColumns().add(new Column(\"col3\")); ((ExpressionList) insert.getItemsList()).getExpressions().add(new LongValue(10)); 建立select12345Select select = SelectUtils.buildSelectFromTable(new Table(\"mytable\"));Select select = SelectUtils.buildSelectFromTableAndExpressions(new Table(\"mytable\"), new Column(\"a\"), new Column(\"b\"));Select select = SelectUtils.buildSelectFromTableAndExpressions(new Table(\"mytable\"), \"a+b\", \"test\"); 代替字符串的值1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677String sql =\"SELECT NAME, ADDRESS, COL1 FROM USER WHERE SSN IN ('11111111111111', '22222222222222');\";Select select = (Select) CCJSqlParserUtil.parse(sql);//Start of value modificationStringBuilder buffer = new StringBuilder();ExpressionDeParser expressionDeParser = new ExpressionDeParser() &#123; @Override public void visit(StringValue stringValue) &#123; this.getBuffer().append(\"XXXX\"); &#125; &#125;;SelectDeParser deparser = new SelectDeParser(expressionDeParser,buffer );expressionDeParser.setSelectVisitor(deparser);expressionDeParser.setBuffer(buffer);select.getSelectBody().accept(deparser);//End of value modificationSystem.out.println(buffer.toString());//Result is: SELECT NAME, ADDRESS, COL1 FROM USER WHERE SSN IN (XXXX, XXXX)import net.sf.jsqlparser.JSQLParserException;import net.sf.jsqlparser.expression.LongValue;import net.sf.jsqlparser.expression.StringValue;import net.sf.jsqlparser.parser.CCJSqlParserUtil;import net.sf.jsqlparser.statement.Statement;import net.sf.jsqlparser.util.deparser.ExpressionDeParser;import net.sf.jsqlparser.util.deparser.SelectDeParser;import net.sf.jsqlparser.util.deparser.StatementDeParser;public class ReplaceColumnValues &#123; static class ReplaceColumnAndLongValues extends ExpressionDeParser &#123; @Override public void visit(StringValue stringValue) &#123; this.getBuffer().append(\"?\"); &#125; @Override public void visit(LongValue longValue) &#123; this.getBuffer().append(\"?\"); &#125; &#125; public static String cleanStatement(String sql) throws JSQLParserException &#123; StringBuilder buffer = new StringBuilder(); ExpressionDeParser expr = new ReplaceColumnAndLongValues(); SelectDeParser selectDeparser = new SelectDeParser(expr, buffer); expr.setSelectVisitor(selectDeparser); expr.setBuffer(buffer); StatementDeParser stmtDeparser = new StatementDeParser(expr, selectDeparser, buffer); Statement stmt = CCJSqlParserUtil.parse(sql); stmt.accept(stmtDeparser); return stmtDeparser.getBuffer().toString(); &#125; public static void main(String[] args) throws JSQLParserException &#123; System.out.println(cleanStatement(\"SELECT 'abc', 5 FROM mytable WHERE col='test'\")); System.out.println(cleanStatement(\"UPDATE table1 A SET A.columna = 'XXX' WHERE A.cod_table = 'YYY'\")); System.out.println(cleanStatement(\"INSERT INTO example (num, name, address, tel) VALUES (1, 'name', 'test ', '1234-1234')\")); System.out.println(cleanStatement(\"DELETE FROM table1 where col=5 and col2=4\")); &#125;&#125;/*SELECT ?, ? FROM mytable WHERE col = ?UPDATE table1 A SET A.columna = ? WHERE A.cod_table = ?INSERT INTO example (num, name, address, tel) VALUES (?, ?, ?, ?)DELETE FROM table1 WHERE col = ? AND col2 = ?*/ 参考 https://github.com/JSQLParser/JSqlParser/wiki","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"react入门教程","date":"2018-12-05T13:56:22.000Z","path":"2018/12/05/react入门教程/","text":"webpack4初始化12cnpm i -D webpackcnpm i -D webpack-cli //相关的命令 相应包的安装 react 专门用于创建组件和虚拟DOM，同事组件的生命周期在这个包中。 react-dom 专门进行dom操作的，最主要的应用场景，就是ReactDom.render() babel babel-node 一个命令行工具 babel-register 可以实现动态转换 babel-core 核心包 babel-preset-env 一个套餐 jsx使用安装babel插件123cnpm i babel-core babel-loader babel-plugin-transform-runtime -Dcnpm i babel-preset-env babel-preset-stage-0 -Dcnpm i babel-preset-react -D 添加.babelrc配置文件1234&#123; &quot;presets&quot;:[&quot;env&quot;,&quot;stage-0&quot;,&quot;react&quot;], &quot;plugins&quot;:[&quot;transform-runtime&quot;]&#125; ##添加babel-loader配置项 12345module：&#123; rules:[ &#123;test:/\\.js|jsx/,use:&apos;babel-loader&apos;,exclude:/node_modules/&#125; ]&#125;","tags":[{"name":"react","slug":"react","permalink":"http://wumuwumu.github.io/tags/react/"}]},{"title":"nginx配置","date":"2018-12-05T13:47:32.000Z","path":"2018/12/05/nginx/nginx配置/","text":"配置web服务器1234567891011server &#123; listen 80; server_name api.lufficc.com *.lufficc.com; location /images/ &#123; root /data; &#125; location / &#123; proxy_pass https://lufficc.com; &#125;&#125; 反向代理1234567server&#123; listen 80; server_name search.lufficc.com; location / &#123; proxy_pass https://www.baidu.com; &#125;&#125; 参考 https://lufficc.com/blog/configure-nginx-as-a-web-server https://blog.csdn.net/hj7jay/article/details/53905943 http://www.nginx.cn/76.html","tags":[{"name":"web","slug":"web","permalink":"http://wumuwumu.github.io/tags/web/"},{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"centos7修改网卡","date":"2018-12-05T13:40:23.000Z","path":"2018/12/05/运维/centos7修改网卡/","text":"修改mac使用virtualbox导入一个虚拟机时mac地址是一样的，此时需要修改。 修改mac地址直接在virtualBox的setting&gt;network配置中进行修改。 修改网卡名称修改网卡的配置文件123vim /etc/sysconfig/network-scripts/ifcfg-eno16777736 //修改NAME，DEVICE 成希望的（不要加ifcfg）mv ifcfg-eno16777736 ifcfg-eth0 //修改配置文件的名字 禁用可预测命名规则1vim /etc/default/grub 添加内核参数： net.ifnames=0 biosdevname=0 12345678[root@ansheng network-scripts]# vi /etc/default/grubGRUB_TIMEOUT=5GRUB_DISTRIBUTOR=&quot;$(sed &apos;s, release .*$,,g&apos; /etc/system-release)&quot;GRUB_DEFAULT=savedGRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT=&quot;console&quot;GRUB_CMDLINE_LINUX=&quot;rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet net.ifnames=0 biosdevname=0&quot;GRUB_DISABLE_RECOVERY=&quot;true&quot; 用 grub2-mkconfig 命令重新生成GRUB配置并更新内核1234567[root@ansheng network-scripts]# grub2-mkconfig -o /boot/grub2/grub.cfgGenerating grub configuration file ...Found linux image: /boot/vmlinuz-3.10.0-327.el7.x86_64Found initrd image: /boot/initramfs-3.10.0-327.el7.x86_64.imgFound linux image: /boot/vmlinuz-0-rescue-4dd6b54f74c94bff9e92c61d669fc195Found initrd image: /boot/initramfs-0-rescue-4dd6b54f74c94bff9e92c61d669fc195.imgdone 重启系统","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"nvc-server安装","date":"2018-12-05T13:39:00.000Z","path":"2018/12/05/运维/nvc-server安装/","text":"centos 安装 vnc serverVNC需要系统安装的有桌面，如果是生产环境服务器，安装时使用的最小化安装，那么进行下面操作按章GNOME 桌面。 123456789101112131415# 列出的组列表里有GNOME Desktop。yum grouplist #安装之yum groupinstall -y \"GNOME Desktop\" # 安装完成后，修改默认启动方式为图形化界面systemctl set-default graphical.target //设置成图形模式 # 如果要换回来 systemctl set-default multi-user.target //设置成命令模式 #然后重启系统即可 第一步：安装VNC服务软件，使用root用户执行以下命令（以下操作没有特别说明均在root用户）： 1yum install tigervnc-server -y img 安装后可以使用如下命令来验证是否安装成功： 1rpm -qa|grep tigervnc-server img 第二步：复制vnc的启动操作脚本, vncserver@:1.service中的：1表示”桌面号”，启动的端口号就是5900+桌面号，即是5901，如果再有一个就是2啦，端口号加1就是5902，以此类推： 1cp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver@:1.service 第三步：编辑 /etc/systemd/system/vncserver@:1.service 1vim /etc/systemd/system/vncserver@\\:1.service img vnc配置文件修改前找到其中的 ，修改成自己的用户名，如果是root用户登录桌面就使用root用户，如果使用普通用户登录桌面使用普通用户，这里笔者使用用户名：cy img vnc配置文件修改后修改完毕后保存退出vim。第四步：设置vnc密码，执行su cy，切换到刚配置文件设置的cy用户，执行（这一步是在cy用户下操作），输入两次密码，输入完成后会提示是否设置view-only password（“View-only password”密码，只允许查看,无控制权限。）这个可设可不设：1vncpasswd img 第五步：启动服务： 1systemctl start vncserver@\\:1.service 第一次输入启动服务命令可能会要求输入（从新加载配置文件，新增和配置文件发生变化时都需要执行 daemon-reload 子命令）： 1systemctl daemon-reload 执行完毕之后在执行启动命令就可以了： img 可以加入开机启动，下次开机就会自动启动啦： 1systemctl enable vncserver@\\:1.service 第六步：查看端口是否监听： 1netstat -lnpt|grep Xvnc img 这里我们可以看到5901端口已经被监听第七步：开放防火墙的5901端口：1firewall-cmd --zone=public --add-port=5901/tcp --permanent 如果防火墙没有启动需要先启动防火墙。 img 当然也可以狠一点，直接停止防火墙： 1systemctl stop firewalld.service img 停止之后该需要禁止开机启动： 1systemctl disable firewalld.service 第八步：关闭SELinux，编辑/etc/selinux/config 文件： 1vim /etc/selinux/config img 将selinux设置为disabled img 到这里vnc服务已经安装完毕，下面就可使用vnc客户端来连接。 第九步：在vnc客户端（vnc viewer）输入服务器IP:桌面号（如192.168.31.100:1），输入后回车： img 第十步：输入IP后会弹出确认，点击contiue即可： img 第十一步：输入vnc密码： img 第十二步：登录成功，输入远程机器密码（登录成功后需要输入远程机器的用户的密码，如果没有密码就可以直接进入系统）： img 第十三步：成功进入远程桌面： img 至此整个CentOS7.x 的VNC服务安装完毕^_^。 小贴士：vnc服务只能在局域网使用，如果在外网，则需要有公网IP地址，VNC不仅具备内网穿透功能。 ubuntu 安装 vnc viewervnc view的网站https://www.realvnc.com/en/connect/download/viewer/ 1sudo dpkg -i VNC-Viewer-6.17.1113-Linux-x64.deb 参考 https://my.oschina.net/huhaoren/blog/497394","tags":[{"name":"web","slug":"web","permalink":"http://wumuwumu.github.io/tags/web/"}]},{"title":"VirtualBox磁盘扩容","date":"2018-12-05T13:36:46.000Z","path":"2018/12/05/VirtualBox磁盘扩容/","text":"扩展磁盘文件VDI1VBoxManage modifyhd centos.vdi --resize 16000 # 单位M VMDK123VBoxManage clonehd &quot;centos.vmdk&quot; &quot;centos.vdi&quot; --format vdi # vmdk是转换前的文件，vdi是转换之后的文件VBoxManage modifyhd &quot;centos.vdi&quot; --resize 16000 # 这里的单位是MVBoxManage clonehd &quot;centos.vdi&quot; &quot;resized.vmdk&quot; --format vmdk #可以再转回来 使用克隆本人在使用的时候，前面两种方式不能实现，采用第三种方式 12VBoxManage createhd -filename centos7-main-64g -size 65536 -format VDI -variant Standard # 创建一个新的磁盘，磁盘大小为想要的大小VBoxManage clonemedium ../centos7-main\\ Clone/centos7-main\\ Clone.vdi centos7-main-64g.vdi --existing # 将原有的磁盘复制到新磁盘上 磁盘扩容这里可以使用gparted进行磁盘的扩容 下载gparted-live镜像 设置iso镜像开机启动 进行分区的修改 LVM扩容如果你没有使用逻辑卷就可以跳过这节。如果使用逻辑卷也可以通过添加新磁盘的形式对文件系统进行扩容，这种方式更加简单方便。 创建PE、VG扩展LV12sudo vgextend VolGroup /dev/sda4 # 通过新卷的方式扩展到卷组lvresize -l +122 /dev/centos/root # 直接扩容 刷新逻辑分区容量1xfs_growfs /devices/centos/root # resize2fs是不能成功的","tags":[{"name":"web","slug":"web","permalink":"http://wumuwumu.github.io/tags/web/"}]},{"title":"lorawan协议（中文版）","date":"2018-12-05T13:34:09.000Z","path":"2018/12/05/lorawan协议（中文版）/","text":"介绍网关和服务器之间的协议是有目的的非常基本的，仅用于演示目的，或用于私有和可靠的网络。 这里没有网关或服务器的认证，并且确认仅用于网络质量评估，而不是 纠正UDP数据报丢失（无重试）。 系统原理和相关定义1234567891011121314151617 ((( Y ))) | |+ - -|- - - - - - - - - - - - - + xxxxxxxxxxxx +--------+| +--+-----------+ +------+ | xx x x xxx | || | | | | | xx Internet xx | || | Concentrator |&lt;---&gt;| Host |&lt;-------xx or xx--------&gt;| || | | SPI | | | xx Intranet xx | Server || +--------------+ +------+ | xxxx x xxxx | || ^ ^ | xxxxxxxx | || | PPS +-------+ NMEA | | | || +-----| GPS |-------+ | +--------+| | (opt) | || +-------+ || || Gateway |+- - - - - - - - - - - - - - - -+ 网关：无线电RX / TX板，基于Semtech多通道调制解调器（SX130x），收发器（SX135x）和/或低功耗独立调制解调器（SX127x）。 主机：运行包转发器的嵌入式计算机。通过SPI链路驱动集中器。 GPS：具有“每秒1脉冲”的GNSS（GPS，伽利略，GLONASS等）接收器 输出和到主机的串行链接，以发送包含时间和地理坐标数据的NMEA帧。可选的。 网关：由至少一个无线电集中器，主机，一些组成的设备网络连接到互联网或专用网络（以太网，3G，Wifi，微波链路），以及可选的GPS接收器进行同步。 服务器：一种抽象计算机，它将处理由网关接收和转发的RF数据包，并发出RF数据包以响应网关必须发出的数据包。 假设网关可以在NAT后面或防火墙停止任何传入连接。 假设服务器具有静态IP地址（或通过DNS服务可解决的地址），并且能够接收特定端口上的传入连接。 上行协议3.1 时序图 12345678910111213141516+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | -----------------------------------\\ | |-| When 1-N RF packets are received | | | ------------------------------------ | | | | PUSH_DATA (token X, GW MAC, JSON payload) | |-------------------------------------------------------------&gt;| | | | PUSH_ACK (token X) | |&lt;-------------------------------------------------------------| | ------------------------------\\ | | | process packets *after* ack |-| | ------------------------------- | | | PUSH_DATA 包网关使用该数据包类型主要是将所接收的RF分组和相关联的元数据转发到服务器。 字节 功能 0 协议版本2 1-2 随机凭证 3 PUSH_DATA标识0x00 4-11 网关唯一标识（MAC地址） 12-结束 JSON对象，看第4章 PUSH_ACK包服务器使用该数据包类型立即确认收到的所有PUSH_DATA数据包。 字节 功能 0 协议版本2 1-2 与PUSH_DATA包中相同的凭证，用于确认 3 PUSH_ACK标识0x01 上行JSON数据结构根对象包含名为&quot;rxpk&quot;的数组： 123&#123; &quot;rxpk&quot;:[ &#123;...&#125;, ...]&#125; 该数组包含至少一个JSON对象，每个对象包含一个RF数据包以及包含以下字段的关联元数据： 名称 类别 功能 time string UTC time of pkt RX, us precision, ISO 8601 ‘compact’ format tmst number Internal timestamp of “RX finished” event (32b unsigned) freq number RX central frequency in MHz (unsigned float, Hz precision) chan number Concentrator “IF” channel used for RX (unsigned integer) rfch number Concentrator “RF chain” used for RX (unsigned integer) stat number CRC status: 1 = OK, -1 = fail, 0 = no CRC modu string Modulation identifier “LORA” or “FSK” datr string LoRa datarate identifier (eg. SF12BW500) datr number FSK datarate (unsigned, in bits per second) codr string LoRa ECC coding rate identifier rssi number RSSI in dBm (signed integer, 1 dB precision) lsnr number Lora SNR ratio in dB (signed float, 0.1 dB precision) size number RF packet payload size in bytes (unsigned integer) data string Base64 encoded RF packet payload, padded 示例（为了便于阅读而添加了空格，缩进和换行符）： 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123;&quot;rxpk&quot;:[ &#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.528002Z&quot;, &quot;tmst&quot;:3512348611, &quot;chan&quot;:2, &quot;rfch&quot;:0, &quot;freq&quot;:866.349812, &quot;stat&quot;:1, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF7BW125&quot;, &quot;codr&quot;:&quot;4/6&quot;, &quot;rssi&quot;:-35, &quot;lsnr&quot;:5.1, &quot;size&quot;:32, &quot;data&quot;:&quot;-DS4CGaDCdG+48eJNM3Vai-zDpsR71Pn9CPA9uCON84&quot; &#125;,&#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.530974Z&quot;, &quot;tmst&quot;:3512348514, &quot;chan&quot;:9, &quot;rfch&quot;:1, &quot;freq&quot;:869.1, &quot;stat&quot;:1, &quot;modu&quot;:&quot;FSK&quot;, &quot;datr&quot;:50000, &quot;rssi&quot;:-75, &quot;size&quot;:16, &quot;data&quot;:&quot;VEVTVF9QQUNLRVRfMTIzNA==&quot; &#125;,&#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.532038Z&quot;, &quot;tmst&quot;:3316387610, &quot;chan&quot;:0, &quot;rfch&quot;:0, &quot;freq&quot;:863.00981, &quot;stat&quot;:1, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF10BW125&quot;, &quot;codr&quot;:&quot;4/7&quot;, &quot;rssi&quot;:-38, &quot;lsnr&quot;:5.5, &quot;size&quot;:32, &quot;data&quot;:&quot;ysgRl452xNLep9S1NTIg2lomKDxUgn3DJ7DE+b00Ass&quot; &#125;]&#125; 根对象还可以包含名为&quot;stat&quot;的对象： 1234&#123; &quot;rxpk&quot;:[ &#123;...&#125;, ...], &quot;stat&quot;:&#123;...&#125;&#125; 数据包可能不包含&quot;rxpk&quot;数组而是“stat”对象。 123&#123; &quot;stat&quot;:&#123;...&#125;&#125; 该对象包含网关的状态，包含以下字段： 名称 类型 功能 time string UTC ‘system’ time of the gateway, ISO 8601 ‘expanded’ format lati number GPS latitude of the gateway in degree (float, N is +) long number GPS latitude of the gateway in degree (float, E is +) alti number GPS altitude of the gateway in meter RX (integer) rxnb number Number of radio packets received (unsigned integer) rxok number Number of radio packets received with a valid PHY CRC rxfw number Number of radio packets forwarded (unsigned integer) ackr number Percentage of upstream datagrams that were acknowledged dwnb number Number of downlink datagrams received (unsigned integer) txnb number Number of packets emitted (unsigned integer) 示例（为了便于阅读而添加了空格，缩进和换行符）： 123456789101112&#123;&quot;stat&quot;:&#123; &quot;time&quot;:&quot;2014-01-12 08:59:28 GMT&quot;, &quot;lati&quot;:46.24000, &quot;long&quot;:3.25230, &quot;alti&quot;:145, &quot;rxnb&quot;:2, &quot;rxok&quot;:2, &quot;rxfw&quot;:2, &quot;ackr&quot;:100.0, &quot;dwnb&quot;:2, &quot;txnb&quot;:2&#125;&#125; 下行协议时序图1234567891011121314151617181920212223242526+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | -----------------------------------\\ | |-| Every N seconds (keepalive time) | | | ------------------------------------ | | | | PULL_DATA (token Y, MAC@) | |-------------------------------------------------------------&gt;| | | | PULL_ACK (token Y) | |&lt;-------------------------------------------------------------| | |+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | ------------------------------------------------------\\ | | | Anytime after first PULL_DATA for each packet to TX |-| | ------------------------------------------------------- | | | | PULL_RESP (token Z, JSON payload) | |&lt;-------------------------------------------------------------| | | | TX_ACK (token Z, JSON payload) | |-------------------------------------------------------------&gt;| PULL_DATA包网关使用该数据包类型来轮询来自服务器的数据。 此数据交换由网关初始化，因为如果网关位于NAT后面，服务器可能无法将数据包发送到网关。 当网关初始化交换机时，将打开通向服务器的网络路由，并允许数据包在两个方向上流动。 网关必须定期发送PULL_DATA数据包，以确保网络路由保持打开状态，以便服务器随时使用。 Bytes Function 0 protocol version = 2 1-2 random token 3 PULL_DATA identifier 0x02 4-11 Gateway unique identifier (MAC address) PULL_ACK 包服务器使用该数据包类型来确认网络路由是否已打开，以及服务器是否可以随时发送PULL_RESP数据包。 Bytes Function 0 protocol version = 2 1-2 same token as the PULL_DATA packet to acknowledge 3 PULL_ACK identifier 0x04 PULL_RESP 包服务器使用该数据包类型来发送必须由网关发出的RF数据包和相关元数据。 Bytes Function 0 protocol version = 2 1-2 random token 3 PULL_RESP identifier 0x03 4-end JSON object, starting with {, ending with }, see section 6 TX_ACK 包网关使用该分组类型向服务器发送反馈，以通知网关是否已接受或拒绝下行链路请求。 数据报可以选项包含一个JSON字符串，以提供有关acknoledge的更多详细信息。 如果没有JSON（空字符串），这意味着没有发生错误。 Bytes Function 0 protocol version = 2 1-2 same token as the PULL_RESP packet to acknowledge 3 TX_ACK identifier 0x05 4-11 Gateway unique identifier (MAC address) 12-end [optional] JSON object, starting with {, ending with }, see section 6 下行JSON数据结构 PULL_RESP数据包的根对象必须包含名为“txpk”的对象： 123&#123; &quot;txpk&quot;: &#123;...&#125;&#125; 该对象包含要发出的RF数据包以及与以下字段相关联的元数据： Name Type Function imme bool Send packet immediately (will ignore tmst &amp; time) tmst number Send packet on a certain timestamp value (will ignore time) time string Send packet at a certain time (GPS synchronization required) freq number TX central frequency in MHz (unsigned float, Hz precision) rfch number Concentrator “RF chain” used for TX (unsigned integer) powe number TX output power in dBm (unsigned integer, dBm precision) modu string Modulation identifier “LORA” or “FSK” datr string LoRa datarate identifier (eg. SF12BW500) datr number FSK datarate (unsigned, in bits per second) codr string LoRa ECC coding rate identifier fdev number FSK frequency deviation (unsigned integer, in Hz) ipol bool Lora modulation polarization inversion prea number RF preamble size (unsigned integer) size number RF packet payload size in bytes (unsigned integer) data string Base64 encoded RF packet payload, padding optional ncrc bool If true, disable the CRC of the physical layer (optional) 大多数字段都是可选的。如果省略字段，将使用默认参数。 示例（为便于阅读而添加了空格，缩进和换行符）： 1234567891011121314151617181920212223&#123;&quot;txpk&quot;:&#123; &quot;imme&quot;:true, &quot;freq&quot;:864.123456, &quot;rfch&quot;:0, &quot;powe&quot;:14, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF11BW125&quot;, &quot;codr&quot;:&quot;4/6&quot;, &quot;ipol&quot;:false, &quot;size&quot;:32, &quot;data&quot;:&quot;H3P3N2i9qc4yt7rK7ldqoeCVJGBybzPY5h1Dd7P7p8v&quot;&#125;&#125;&#123;&quot;txpk&quot;:&#123; &quot;imme&quot;:true, &quot;freq&quot;:861.3, &quot;rfch&quot;:0, &quot;powe&quot;:12, &quot;modu&quot;:&quot;FSK&quot;, &quot;datr&quot;:50000, &quot;fdev&quot;:3000, &quot;size&quot;:32, &quot;data&quot;:&quot;H3P3N2i9qc4yt7rK7ldqoeCVJGBybzPY5h1Dd7P7p8v&quot;&#125;&#125; TX_ACK数据包的根对象必须包含名为“txpk_ack”的对象： 123&#123; &quot;txpk_ack&quot;: &#123;...&#125;&#125; 该对象包含有关相关PULL_RESP数据包的状态信息。 Name Type Function error string Indication about success or type of failure that occured for downlink request. 可能的错误有： Value Definition NONE Packet has been programmed for downlink TOO_LATE Rejected because it was already too late to program this packet for downlink TOO_EARLY Rejected because downlink packet timestamp is too much in advance COLLISION_PACKET Rejected because there was already a packet programmed in requested timeframe COLLISION_BEACON Rejected because there was already a beacon planned in requested timeframe TX_FREQ Rejected because requested frequency is not supported by TX RF chain TX_POWER Rejected because requested power is not supported by gateway GPS_UNLOCKED Rejected because GPS is unlocked, so GPS timestamp cannot be used 示例（为便于阅读而添加了空格，缩进和换行符）： 123&#123;&quot;txpk_ack&quot;:&#123; &quot;error&quot;:&quot;COLLISION_PACKET&quot;&#125;&#125;","tags":[{"name":"lorawan","slug":"lorawan","permalink":"http://wumuwumu.github.io/tags/lorawan/"}]},{"title":"lorawan协议","date":"2018-12-05T13:32:24.000Z","path":"2018/12/05/lorawan协议/","text":"Introduction The protocol between the gateway and the server is purposefully very basic and for demonstration purpose only, or for use on private and reliable networks. There is no authentication of the gateway or the server, and the acknowledges are only used for network quality assessment, not to correct UDP datagrams losses (no retries). System schematic and definitions 1234567891011121314151617 ((( Y ))) | |+ - -|- - - - - - - - - - - - - + xxxxxxxxxxxx +--------+| +--+-----------+ +------+ | xx x x xxx | || | | | | | xx Internet xx | || | Concentrator |&lt;---&gt;| Host |&lt;-------xx or xx--------&gt;| || | | SPI | | | xx Intranet xx | Server || +--------------+ +------+ | xxxx x xxxx | || ^ ^ | xxxxxxxx | || | PPS +-------+ NMEA | | | || +-----| GPS |-------+ | +--------+| | (opt) | || +-------+ || || Gateway |+- - - - - - - - - - - - - - - -+ Concentrator: radio RX/TX board, based on Semtech multichannel modems (SX130x), transceivers (SX135x) and/or low-power stand-alone modems (SX127x). Host: embedded computer on which the packet forwarder is run. Drives the concentrator through a SPI link. GPS: GNSS (GPS, Galileo, GLONASS, etc) receiver with a “1 Pulse Per Second” output and a serial link to the host to send NMEA frames containing time and geographical coordinates data. Optional. Gateway: a device composed of at least one radio concentrator, a host, some network connection to the internet or a private network (Ethernet, 3G, Wifi, microwave link), and optionally a GPS receiver for synchronization. Server: an abstract computer that will process the RF packets received and forwarded by the gateway, and issue RF packets in response that the gateway will have to emit. It is assumed that the gateway can be behind a NAT or a firewall stopping any incoming connection. It is assumed that the server has an static IP address (or an address solvable through a DNS service) and is able to receive incoming connections on a specific port. Upstream protocol Sequence diagram12345678910111213141516+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | -----------------------------------\\ | |-| When 1-N RF packets are received | | | ------------------------------------ | | | | PUSH_DATA (token X, GW MAC, JSON payload) | |-------------------------------------------------------------&gt;| | | | PUSH_ACK (token X) | |&lt;-------------------------------------------------------------| | ------------------------------\\ | | | process packets *after* ack |-| | ------------------------------- | | | PUSH_DATA packetThat packet type is used by the gateway mainly to forward the RF packets received, and associated metadata, to the server. Bytes Function 0 protocol version = 2 1-2 random token 3 PUSH_DATA identifier 0x00 4-11 Gateway unique identifier (MAC address) 12-end JSON object, starting with {, ending with }, see section 4 PUSH_ACK packetThat packet type is used by the server to acknowledge immediately all the PUSH_DATA packets received. Bytes Function 0 protocol version = 2 1-2 same token as the PUSH_DATA packet to acknowledge 3 PUSH_ACK identifier 0x01 Upstream JSON data structure The root object can contain an array named “rxpk”: 123&#123; &quot;rxpk&quot;:[ &#123;...&#125;, ...]&#125; That array contains at least one JSON object, each object contain a RF packet and associated metadata with the following fields: Name Type Function time string UTC time of pkt RX, us precision, ISO 8601 ‘compact’ format tmst number Internal timestamp of “RX finished” event (32b unsigned) freq number RX central frequency in MHz (unsigned float, Hz precision) chan number Concentrator “IF” channel used for RX (unsigned integer) rfch number Concentrator “RF chain” used for RX (unsigned integer) stat number CRC status: 1 = OK, -1 = fail, 0 = no CRC modu string Modulation identifier “LORA” or “FSK” datr string LoRa datarate identifier (eg. SF12BW500) datr number FSK datarate (unsigned, in bits per second) codr string LoRa ECC coding rate identifier rssi number RSSI in dBm (signed integer, 1 dB precision) lsnr number Lora SNR ratio in dB (signed float, 0.1 dB precision) size number RF packet payload size in bytes (unsigned integer) data string Base64 encoded RF packet payload, padded Example (white-spaces, indentation and newlines added for readability): 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123;&quot;rxpk&quot;:[ &#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.528002Z&quot;, &quot;tmst&quot;:3512348611, &quot;chan&quot;:2, &quot;rfch&quot;:0, &quot;freq&quot;:866.349812, &quot;stat&quot;:1, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF7BW125&quot;, &quot;codr&quot;:&quot;4/6&quot;, &quot;rssi&quot;:-35, &quot;lsnr&quot;:5.1, &quot;size&quot;:32, &quot;data&quot;:&quot;-DS4CGaDCdG+48eJNM3Vai-zDpsR71Pn9CPA9uCON84&quot; &#125;,&#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.530974Z&quot;, &quot;tmst&quot;:3512348514, &quot;chan&quot;:9, &quot;rfch&quot;:1, &quot;freq&quot;:869.1, &quot;stat&quot;:1, &quot;modu&quot;:&quot;FSK&quot;, &quot;datr&quot;:50000, &quot;rssi&quot;:-75, &quot;size&quot;:16, &quot;data&quot;:&quot;VEVTVF9QQUNLRVRfMTIzNA==&quot; &#125;,&#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.532038Z&quot;, &quot;tmst&quot;:3316387610, &quot;chan&quot;:0, &quot;rfch&quot;:0, &quot;freq&quot;:863.00981, &quot;stat&quot;:1, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF10BW125&quot;, &quot;codr&quot;:&quot;4/7&quot;, &quot;rssi&quot;:-38, &quot;lsnr&quot;:5.5, &quot;size&quot;:32, &quot;data&quot;:&quot;ysgRl452xNLep9S1NTIg2lomKDxUgn3DJ7DE+b00Ass&quot; &#125;]&#125; The root object can also contain an object named “stat” : 1234&#123; &quot;rxpk&quot;:[ &#123;...&#125;, ...], &quot;stat&quot;:&#123;...&#125;&#125; It is possible for a packet to contain no “rxpk” array but a “stat” object. 123&#123; &quot;stat&quot;:&#123;...&#125;&#125; That object contains the status of the gateway, with the following fields: Name Type Function time string UTC ‘system’ time of the gateway, ISO 8601 ‘expanded’ format lati number GPS latitude of the gateway in degree (float, N is +) long number GPS latitude of the gateway in degree (float, E is +) alti number GPS altitude of the gateway in meter RX (integer) rxnb number Number of radio packets received (unsigned integer) rxok number Number of radio packets received with a valid PHY CRC rxfw number Number of radio packets forwarded (unsigned integer) ackr number Percentage of upstream datagrams that were acknowledged dwnb number Number of downlink datagrams received (unsigned integer) txnb number Number of packets emitted (unsigned integer) Example (white-spaces, indentation and newlines added for readability): 123456789101112&#123;&quot;stat&quot;:&#123; &quot;time&quot;:&quot;2014-01-12 08:59:28 GMT&quot;, &quot;lati&quot;:46.24000, &quot;long&quot;:3.25230, &quot;alti&quot;:145, &quot;rxnb&quot;:2, &quot;rxok&quot;:2, &quot;rxfw&quot;:2, &quot;ackr&quot;:100.0, &quot;dwnb&quot;:2, &quot;txnb&quot;:2&#125;&#125; Downstream protocol Sequence diagram1234567891011121314151617181920212223242526+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | -----------------------------------\\ | |-| Every N seconds (keepalive time) | | | ------------------------------------ | | | | PULL_DATA (token Y, MAC@) | |-------------------------------------------------------------&gt;| | | | PULL_ACK (token Y) | |&lt;-------------------------------------------------------------| | |+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | ------------------------------------------------------\\ | | | Anytime after first PULL_DATA for each packet to TX |-| | ------------------------------------------------------- | | | | PULL_RESP (token Z, JSON payload) | |&lt;-------------------------------------------------------------| | | | TX_ACK (token Z, JSON payload) | |-------------------------------------------------------------&gt;| PULL_DATA packetThat packet type is used by the gateway to poll data from the server. This data exchange is initialized by the gateway because it might be impossible for the server to send packets to the gateway if the gateway is behind a NAT. When the gateway initialize the exchange, the network route towards the server will open and will allow for packets to flow both directions. The gateway must periodically send PULL_DATA packets to be sure the network route stays open for the server to be used at any time. Bytes Function 0 protocol version = 2 1-2 random token 3 PULL_DATA identifier 0x02 4-11 Gateway unique identifier (MAC address) PULL_ACK packetThat packet type is used by the server to confirm that the network route is open and that the server can send PULL_RESP packets at any time. Bytes Function 0 protocol version = 2 1-2 same token as the PULL_DATA packet to acknowledge 3 PULL_ACK identifier 0x04 PULL_RESP packetThat packet type is used by the server to send RF packets and associated metadata that will have to be emitted by the gateway. Bytes Function 0 protocol version = 2 1-2 random token 3 PULL_RESP identifier 0x03 4-end JSON object, starting with {, ending with }, see section 6 TX_ACK packetThat packet type is used by the gateway to send a feedback to the server to inform if a downlink request has been accepted or rejected by the gateway. The datagram may optionnaly contain a JSON string to give more details on acknoledge. If no JSON is present (empty string), this means than no error occured. Bytes Function 0 protocol version = 2 1-2 same token as the PULL_RESP packet to acknowledge 3 TX_ACK identifier 0x05 4-11 Gateway unique identifier (MAC address) 12-end [optional] JSON object, starting with {, ending with }, see section 6 Downstream JSON data structure The root object of PULL_RESP packet must contain an object named “txpk”: 123&#123; &quot;txpk&quot;: &#123;...&#125;&#125; That object contain a RF packet to be emitted and associated metadata with the following fields: Name Type Function imme bool Send packet immediately (will ignore tmst &amp; time) tmst number Send packet on a certain timestamp value (will ignore time) time string Send packet at a certain time (GPS synchronization required) freq number TX central frequency in MHz (unsigned float, Hz precision) rfch number Concentrator “RF chain” used for TX (unsigned integer) powe number TX output power in dBm (unsigned integer, dBm precision) modu string Modulation identifier “LORA” or “FSK” datr string LoRa datarate identifier (eg. SF12BW500) datr number FSK datarate (unsigned, in bits per second) codr string LoRa ECC coding rate identifier fdev number FSK frequency deviation (unsigned integer, in Hz) ipol bool Lora modulation polarization inversion prea number RF preamble size (unsigned integer) size number RF packet payload size in bytes (unsigned integer) data string Base64 encoded RF packet payload, padding optional ncrc bool If true, disable the CRC of the physical layer (optional) Most fields are optional. If a field is omitted, default parameters will be used. Examples (white-spaces, indentation and newlines added for readability): 1234567891011121314151617181920212223&#123;&quot;txpk&quot;:&#123; &quot;imme&quot;:true, &quot;freq&quot;:864.123456, &quot;rfch&quot;:0, &quot;powe&quot;:14, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF11BW125&quot;, &quot;codr&quot;:&quot;4/6&quot;, &quot;ipol&quot;:false, &quot;size&quot;:32, &quot;data&quot;:&quot;H3P3N2i9qc4yt7rK7ldqoeCVJGBybzPY5h1Dd7P7p8v&quot;&#125;&#125;&#123;&quot;txpk&quot;:&#123; &quot;imme&quot;:true, &quot;freq&quot;:861.3, &quot;rfch&quot;:0, &quot;powe&quot;:12, &quot;modu&quot;:&quot;FSK&quot;, &quot;datr&quot;:50000, &quot;fdev&quot;:3000, &quot;size&quot;:32, &quot;data&quot;:&quot;H3P3N2i9qc4yt7rK7ldqoeCVJGBybzPY5h1Dd7P7p8v&quot;&#125;&#125; The root object of TX_ACK packet must contain an object named “txpk_ack”: 123&#123; &quot;txpk_ack&quot;: &#123;...&#125;&#125; That object contain status information concerning the associated PULL_RESP packet. Name Type Function error string Indication about success or type of failure that occured for downlink request. The possible values of “error” field are: Value Definition NONE Packet has been programmed for downlink TOO_LATE Rejected because it was already too late to program this packet for downlink TOO_EARLY Rejected because downlink packet timestamp is too much in advance COLLISION_PACKET Rejected because there was already a packet programmed in requested timeframe COLLISION_BEACON Rejected because there was already a beacon planned in requested timeframe TX_FREQ Rejected because requested frequency is not supported by TX RF chain TX_POWER Rejected because requested power is not supported by gateway GPS_UNLOCKED Rejected because GPS is unlocked, so GPS timestamp cannot be used Examples (white-spaces, indentation and newlines added for readability): 123&#123;&quot;txpk_ack&quot;:&#123; &quot;error&quot;:&quot;COLLISION_PACKET&quot;&#125;&#125; Revisions v1.3 Added downlink feedback from gateway to server (PULL_RESP -&gt; TX_ACK) v1.2 Added value of FSK bitrate for upstream. Added parameters for FSK bitrate and frequency deviation for downstream. v1.1 Added syntax for status report JSON object on upstream. v1.0 Initial version.","tags":[{"name":"lorawan","slug":"lorawan","permalink":"http://wumuwumu.github.io/tags/lorawan/"}]},{"title":"let-us-encrypt证书","date":"2018-12-05T12:59:59.000Z","path":"2018/12/05/运维/let-us-encrypt证书/","text":"基本知识为了实现通配符证书，Let’s Encrypt 对 ACME 协议的实现进行了升级，只有 v2 协议才能支持通配符证书。 客户在申请 Let’s Encrypt 证书的时候，需要校验域名的所有权，证明操作者有权利为该域名申请证书，目前支持三种验证方式： dns-01：给域名添加一个 DNS TXT 记录。 http-01：在域名对应的 Web 服务器下放置一个 HTTP well-known URL 资源文件。 tls-sni-01：在域名对应的 Web 服务器下放置一个 HTTPS well-known URL 资源文件。 而申请通配符证书，只能使用 dns-01 的方式 ACME v2 和 v1 协议是互相不兼容的，为了使用 v2 版本，客户端需要创建另外一个账户（代表客户端操作者），以 Certbot 客户端为例，大家可以查看： Enumerable Orders 和限制 安装12wget https://dl.eff.org/certbot-autochmod a+x ./certbot-auto 申请1./certbot-auto certonly -d *.newyingyong.cn --manual --preferred-challenges dns --server https://acme-v02.api.letsencrypt.org/directory certonly，表示安装模式，Certbot 有安装模式和验证模式两种类型的插件。 –manual 表示手动安装插件，Certbot 有很多插件，不同的插件都可以申请证书，用户可以根据需要自行选择 -d 为那些主机申请证书，如果是通配符，输入 *.newyingyong.cn（可以替换为你自己的域名） -preferred-challenges dns，使用 DNS 方式校验域名所有权 –server，Let’s Encrypt ACME v2 版本使用的服务器不同于 v1 版本，需要显示指定。 添加记录根据命令行提示，填写相关的内容，注意在添加记录的时候，要等到记录生效才确定。 123456789-------------------------------------------------------------------------------Please deploy a DNS TXT record under the name_acme-challenge.newyingyong.cn with the following value:2_8KBE_jXH8nYZ2unEViIbW52LhIqxkg6i9mcwsRvhQBefore continuing, verify the record is deployed.-------------------------------------------------------------------------------Press Enter to ContinueWaiting for verification...Cleaning up challenges 12## 检测记录生效$ dig -t txt _acme-challenge.newyingyong.cn @8.8.8.8 更新查看当前服务器所配置的证书 1certbot-auto certificates 使用申请的普通证书，使用certbot-auto renew 使用通配符证书。 添加DNS记录 1git clone https://github.com/ywdblog/certbot-letencrypt-wildcardcertificates-alydns-au.git 1./certbot-auto renew --cert-name simplehttps.com --manual-auth-hook /脚本目录/au.sh 自动更新 11 1 */1 * * root certbot-auto renew --manual --preferred-challenges dns --manual-auth-hook /脚本目录/sslupdate.sh 参考 https://www.jianshu.com/p/c5c9d071e395 https://www.jianshu.com/p/074e147b68b0 certbot工具https://segmentfault.com/a/1190000015354547","tags":[{"name":"web","slug":"web","permalink":"http://wumuwumu.github.io/tags/web/"}]},{"title":"css动画","date":"2018-12-04T13:36:49.000Z","path":"2018/12/04/css动画/","text":"transition transition-duration transition-property transition-delay transition-timing-function animation @keyframes animation animation-name animation-duration animation-timing-function animation-delay animation-iteration-count animation-fill-mode animation-direction animation-play-state(这个要写在最下面，不然不会生效) transform none translate(x,y)/translate3d(x,y,z) translateX(x)/translateY(y)/translateZ(z) materix/materix3d scale/scale3d scaleX/scaleY/scaleZ rotate/rotate3d rotateX/rotateY/rotateZ skew/skewX/skewY perspective","tags":[{"name":"css","slug":"css","permalink":"http://wumuwumu.github.io/tags/css/"}]},{"title":"清除inline-block之间的间隙","date":"2018-12-03T11:54:08.000Z","path":"2018/12/03/清除inline-block之间的间隙/","text":"原因两个inline-block之间存在间隙，这是因为html元素换行导致的（换行和元素之间的空格、tabs、多个空格，结果一样，最后都是一个空格） 移除空格如果我们使用html minimize工具，会清除html之间的空格。如果没有使用就需要我们手动去除。该方法简单但是不推荐使用，阅读不方便。 123456789101112131415&lt;!-- 方法一 --&gt;&lt;div&gt;one&lt;/div&gt;&lt;div&gt;two&lt;/div&gt;&lt;div&gt;three&lt;/div&gt;&lt;!-- 方法二 --&gt;&lt;div&gt;one&lt;/div&gt;&lt;div&gt;two&lt;/div&gt;&lt;div&gt;three&lt;/div&gt;&lt;!-- 方法三 --&gt;&lt;div&gt;one&lt;/div&gt;&lt;!----&gt;&lt;div&gt;two&lt;/div&gt;&lt;!----&gt;&lt;div&gt;three&lt;/div&gt; 负值margin不推荐使用，每个浏览器之间的间隙不同。 1234nav a &#123; display: inline-block; margin-right: -4px;&#125; 父元素font-size设置为0123456.space &#123; font-size: 0;&#125;.space a &#123; font-size: 12px;&#125; 这种方法是推荐使用的，但是在ie和Chrome浏览器(新的浏览器没有问题)上可能出现问题，因为在chrome上有最小字体限制。改进方法如下。 1234.space &#123; font-size: 0; -webkit-text-size-adjust:none;&#125; 使用letter-spacingletter-spacing用于修改字符间的间隙。 123456.space &#123; letter-spacing: -3px;&#125;.space a &#123; letter-spacing: 0;&#125; 使用word-spacingword-spacing修改单词之间的间隙 123456.space &#123; word-spacing: -6px;&#125;.space a &#123; word-spacing: 0;&#125; 使用浮动123a&#123; float:left;&#125; 参考 https://www.zhangxinxu.com/wordpress/2012/04/inline-block-space-remove-%E5%8E%BB%E9%99%A4%E9%97%B4%E8%B7%9D/ 代码 https://codepen.io/wumuwumu/pen/WYmKYX","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"}]}]