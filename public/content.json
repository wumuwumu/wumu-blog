[{"title":"odoo源码解析4-wsgi_server","date":"2019-11-16T07:35:05.000Z","path":"2019/11/16/odoo/odoo源码解析4-wsgi-server/","text":"application12345678910def application(environ, start_response): ## 是否启动代理 # FIXME: is checking for the presence of HTTP_X_FORWARDED_HOST really useful? # we're ignoring the user configuration, and that means we won't # support the standardised Forwarded header once werkzeug supports # it if config['proxy_mode'] and 'HTTP_X_FORWARDED_HOST' in environ: return ProxyFix(application_unproxied)(environ, start_response) else: return application_unproxied(environ, start_response) application_unproxied清除数据库和用户的追踪清除动作在application方法的结尾不能完成，因为werkzeu在后面还会生成有关的日志。 123456789101112131415161718192021def application_unproxied(environ, start_response): \"\"\" WSGI entry point.\"\"\" # cleanup db/uid trackers - they're set at HTTP dispatch in # web.session.OpenERPSession.send() and at RPC dispatch in # odoo.service.web_services.objects_proxy.dispatch(). # /!\\ The cleanup cannot be done at the end of this `application` # method because werkzeug still produces relevant logging afterwards if hasattr(threading.current_thread(), 'uid'): del threading.current_thread().uid if hasattr(threading.current_thread(), 'dbname'): del threading.current_thread().dbname if hasattr(threading.current_thread(), 'url'): del threading.current_thread().url with odoo.api.Environment.manage(): result = odoo.http.root(environ, start_response) if result is not None: return result # We never returned from the loop. return werkzeug.exceptions.NotFound(\"No handler found.\\n\")(environ, start_response) 参考 https://blog.csdn.net/weixin_35737303/article/details/79038982","tags":[{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"odoo源码解析-启动web服务器","date":"2019-11-16T06:41:42.000Z","path":"2019/11/16/odoo/odoo源码解析3-启动web服务器/","text":"启动12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def start(preload=None, stop=False): \"\"\" Start the odoo http server and cron processor. \"\"\" global server ## 这里加载两个模块web和web_kan，在这里加载模块才能够在用户没有登录的时候才能够访问路由 load_server_wide_modules() odoo.service.wsgi_server._patch_xmlrpc_marshaller() \"\"\" ·GeventServer ·PreforkServer ·ThreadedServer(默认) CommonServer是后面三个类的父类 Odoo服务器通过ThreadedServer.run()运行 \"\"\" if odoo.evented: server = GeventServer(odoo.service.wsgi_server.application) elif config['workers']: if config['test_enable'] or config['test_file']: _logger.warning(\"Unit testing in workers mode could fail; use --workers 0.\") server = PreforkServer(odoo.service.wsgi_server.application) # Workaround for Python issue24291, fixed in 3.6 (see Python issue26721) if sys.version_info[:2] == (3,5): # turn on buffering also for wfile, to avoid partial writes (Default buffer = 8k) werkzeug.serving.WSGIRequestHandler.wbufsize = -1 else: server = ThreadedServer(odoo.service.wsgi_server.application) watcher = None if 'reload' in config['dev_mode'] and not odoo.evented: if inotify: watcher = FSWatcherInotify() watcher.start() elif watchdog: watcher = FSWatcherWatchdog() watcher.start() else: if os.name == 'posix' and platform.system() != 'Darwin': module = 'inotify' else: module = 'watchdog' _logger.warning(\"'%s' module not installed. Code autoreload feature is disabled\", module) if 'werkzeug' in config['dev_mode']: server.app = DebuggedApplication(server.app, evalex=True) ## 启动web服务器 rc = server.run(preload, stop) if watcher: watcher.stop() # like the legend of the phoenix, all ends with beginnings if getattr(odoo, 'phoenix', False): _reexec() return rc if rc else 0 ThreadedServer(CommandServer)Run1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950\"\"\" Start the http server and the cron thread then wait for a signal. The first SIGINT or SIGTERM signal will initiate a graceful shutdown while a second one if any will force an immediate exit. \"\"\"## 启动一个系统命令监测。。。self.start(stop=stop)## 安装、更新、加载模块rc = preload_registries(preload)if stop: self.stop() return rc## 加载定时任务self.cron_spawn()# Wait for a first signal to be handled. (time.sleep will be interrupted# by the signal handler)try: while self.quit_signals_received == 0: self.process_limit() if self.limit_reached_time: has_other_valid_requests = any( not t.daemon and t not in self.limits_reached_threads for t in threading.enumerate() if getattr(t, 'type', None) == 'http') if (not has_other_valid_requests or (time.time() - self.limit_reached_time) &gt; SLEEP_INTERVAL): # We wait there is no processing requests # other than the ones exceeding the limits, up to 1 min, # before asking for a reload. _logger.info('Dumping stacktrace of limit exceeding threads before reloading') dumpstacks(thread_idents=[thread.ident for thread in self.limits_reached_threads]) self.reload() # `reload` increments `self.quit_signals_received` # and the loop will end after this iteration, # therefore leading to the server stop. # `reload` also sets the `phoenix` flag # to tell the server to restart the server after shutting down. else: time.sleep(1) else: time.sleep(SLEEP_INTERVAL) except KeyboardInterrupt: pass self.stop() start12345678910111213141516171819def start(self, stop=False): _logger.debug(\"Setting signal handlers\") set_limit_memory_hard() if os.name == 'posix': signal.signal(signal.SIGINT, self.signal_handler) signal.signal(signal.SIGTERM, self.signal_handler) signal.signal(signal.SIGCHLD, self.signal_handler) signal.signal(signal.SIGHUP, self.signal_handler) signal.signal(signal.SIGXCPU, self.signal_handler) signal.signal(signal.SIGQUIT, dumpstacks) signal.signal(signal.SIGUSR1, log_ormcache_stats) elif os.name == 'nt': import win32api win32api.SetConsoleCtrlHandler(lambda sig: self.signal_handler(sig, None), 1) test_mode = config['test_enable'] or config['test_file'] if test_mode or (config['http_enable'] and not stop): # some tests need the http deamon to be available... self.http_spawn() ThreadedWSGIServerReloadable这个服务可以不启动也能够运行程序。他的作用是debug保持端口是开启的。 Werkzeug是Python的WSGI规范的实现函数库。基于BSD协议。WSGI(Web Server Gateway Interface)WSGI服务允许重用环境提供的监听套接字，它通过自动重加载使用，用于保持当有重加载的时候监听套接字是打开状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class ThreadedWSGIServerReloadable(LoggingBaseWSGIServerMixIn, werkzeug.serving.ThreadedWSGIServer): \"\"\" werkzeug Threaded WSGI Server patched to allow reusing a listen socket given by the environement, this is used by autoreload to keep the listen socket open when a reload happens. \"\"\" def __init__(self, host, port, app): super(ThreadedWSGIServerReloadable, self).__init__(host, port, app, handler=RequestHandler) # See https://github.com/pallets/werkzeug/pull/770 # This allow the request threads to not be set as daemon # so the server waits for them when shutting down gracefully. self.daemon_threads = False def server_bind(self): SD_LISTEN_FDS_START = 3 if os.environ.get('LISTEN_FDS') == '1' and os.environ.get('LISTEN_PID') == str(os.getpid()): self.reload_socket = True self.socket = socket.fromfd(SD_LISTEN_FDS_START, socket.AF_INET, socket.SOCK_STREAM) _logger.info('HTTP service (werkzeug) running through socket activation') else: self.reload_socket = False super(ThreadedWSGIServerReloadable, self).server_bind() _logger.info('HTTP service (werkzeug) running on %s:%s', self.server_name, self.server_port) def server_activate(self): if not self.reload_socket: super(ThreadedWSGIServerReloadable, self).server_activate() def process_request(self, request, client_address): \"\"\" Start a new thread to process the request. Override the default method of class socketserver.ThreadingMixIn to be able to get the thread object which is instantiated and set its start time as an attribute \"\"\" t = threading.Thread(target = self.process_request_thread, args = (request, client_address)) t.daemon = self.daemon_threads t.type = 'http' t.start_time = time.time() t.start() # TODO: Remove this method as soon as either of the revision # - python/cpython@8b1f52b5a93403acd7d112cd1c1bc716b31a418a for Python 3.6, # - python/cpython@908082451382b8b3ba09ebba638db660edbf5d8e for Python 3.7, # is included in all Python 3 releases installed on all operating systems supported by Odoo. # These revisions are included in Python from releases 3.6.8 and Python 3.7.2 respectively. def _handle_request_noblock(self): \"\"\" In the python module `socketserver` `process_request` loop, the __shutdown_request flag is not checked between select and accept. Thus when we set it to `True` thanks to the call `httpd.shutdown`, a last request is accepted before exiting the loop. We override this function to add an additional check before the accept(). \"\"\" if self._BaseServer__shutdown_request: return super(ThreadedWSGIServerReloadable, self)._handle_request_noblock() 参考 https://blog.csdn.net/weixin_35737303/article/details/79038879","tags":[{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"hibernate使用hbm2ddl.auto=在生产环境更新.md","date":"2019-11-16T06:22:56.000Z","path":"2019/11/16/java/hibernate使用hbm2ddl.auto=在生产环境更新/","text":"https://www.codenong.com/221379/","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"odoo源码解析2-server命令","date":"2019-11-16T03:40:07.000Z","path":"2019/11/16/odoo/odoo源码解析2-server命令/","text":"默认情况下的启动命令的server，这个是将odoo运行起来的命令。核心代码如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556## 判断是否为root用户，如果为root用户就发送警告check_root_user() ## 解析命令行参数odoo.tools.config.parse_config(args)## 如果为postgres用户就停止运行check_postgres_user()report_configuration()config = odoo.tools.config# the default limit for CSV fields in the module is 128KiB, which is not# quite sufficient to import images to store in attachment. 500MiB is a# bit overkill, but better safe than sorry I guesscsv.field_size_limit(500 * 1024 * 1024)## 创建加载的数据库preload = []if config['db_name']: preload = config['db_name'].split(',') for db_name in preload: try: odoo.service.db._create_empty_database(db_name) config['init']['base'] = True except ProgrammingError as err: if err.pgcode == errorcodes.INSUFFICIENT_PRIVILEGE: # We use an INFO loglevel on purpose in order to avoid # reporting unnecessary warnings on build environment # using restricted database access. _logger.info(\"Could not determine if database %s exists, \" \"skipping auto-creation: %s\", db_name, err) else: raise err except odoo.service.db.DatabaseExists: passif config[\"translate_out\"]: export_translation() sys.exit(0)if config[\"translate_in\"]: import_translation() sys.exit(0)# This needs to be done now to ensure the use of the multiprocessing# signaling mecanism for registries loaded with -dif config['workers']: odoo.multi_process = True## 是否在启动服务后停止，用户创建更新数据库stop = config[\"stop_after_init\"]## 设置pid文件setup_pid_file()## 启动serverrc = odoo.service.server.start(preload=preload, stop=stop)sys.exit(rc) 参考 https://blog.csdn.net/weixin_35737303/article/details/79038671","tags":[{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"odoo模块加载机制","date":"2019-11-08T12:36:34.000Z","path":"2019/11/08/odoo/odoo模块加载机制/","text":"Odoo的启动通过openerp-server脚本完成，它是系统的入口。 然后加载配置文件openerp-server.conf 或者 openerp_serverrc； openerp-server.conf的主要内容： 这个文件缺省是没有的，Odoo系统会有一个默认值，但是一般情况我们都需配置这个文件。 启动http服务器，监听端口。 模块加载： 模块加载外层就是封装一个Registry(Mapping)对象:实际是一个字典，它包含对应的db，model等映射关系，一个DB对应一个Registry。后续的操作都会围绕这个Registry进行，将相关的数据赋值给相应的属性项。 初始化数据库（初次运行)1)加载base模块下的base.sql文件并执行。此时数据库表为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174CREATE TABLE ir_actions ( id serial, primary key(id));CREATE TABLE ir_act_window (primary key(id)) INHERITS (ir_actions);CREATE TABLE ir_act_report_xml (primary key(id)) INHERITS (ir_actions);CREATE TABLE ir_act_url (primary key(id)) INHERITS (ir_actions);CREATE TABLE ir_act_server (primary key(id)) INHERITS (ir_actions);CREATE TABLE ir_act_client (primary key(id)) INHERITS (ir_actions);CREATE TABLE ir_model ( id serial, model varchar NOT NULL, name varchar, state varchar, info text, primary key(id));CREATE TABLE ir_model_fields ( id serial, model varchar NOT NULL, model_id integer references ir_model on delete cascade, name varchar NOT NULL, relation varchar, select_level varchar, field_description varchar, ttype varchar, state varchar default 'base', relation_field varchar, translate boolean default False, serialization_field_id integer references ir_model_fields on delete cascade, primary key(id));CREATE TABLE res_lang ( id serial, name VARCHAR(64) NOT NULL UNIQUE, code VARCHAR(16) NOT NULL UNIQUE, primary key(id));CREATE TABLE res_users ( id serial NOT NULL, active boolean default True, login varchar(64) NOT NULL UNIQUE, password varchar(64) default null, -- No FK references below, will be added later by ORM -- (when the destination rows exist) company_id integer, -- references res_company, partner_id integer, -- references res_partner, primary key(id));create table wkf ( id serial, name varchar(64), osv varchar(64), on_create bool default false, primary key(id));CREATE TABLE ir_module_category ( id serial NOT NULL, create_uid integer, -- references res_users on delete set null, create_date timestamp without time zone, write_date timestamp without time zone, write_uid integer, -- references res_users on delete set null, parent_id integer REFERENCES ir_module_category ON DELETE SET NULL, name character varying(128) NOT NULL, primary key(id));CREATE TABLE ir_module_module ( id serial NOT NULL, create_uid integer, -- references res_users on delete set null, create_date timestamp without time zone, write_date timestamp without time zone, write_uid integer, -- references res_users on delete set null, website character varying(256), summary character varying(256), name character varying(128) NOT NULL, author character varying(128), icon varchar, state character varying(16), latest_version character varying(64), shortdesc character varying(256), category_id integer REFERENCES ir_module_category ON DELETE SET NULL, description text, application boolean default False, demo boolean default False, web boolean DEFAULT FALSE, license character varying(32), sequence integer DEFAULT 100, auto_install boolean default False, primary key(id));ALTER TABLE ir_module_module add constraint name_uniq unique (name);CREATE TABLE ir_module_module_dependency ( id serial NOT NULL, create_uid integer, -- references res_users on delete set null, create_date timestamp without time zone, write_date timestamp without time zone, write_uid integer, -- references res_users on delete set null, name character varying(128), module_id integer REFERENCES ir_module_module ON DELETE cascade, primary key(id));CREATE TABLE ir_model_data ( id serial NOT NULL, create_uid integer, create_date timestamp without time zone, write_date timestamp without time zone, write_uid integer, noupdate boolean, name varchar NOT NULL, date_init timestamp without time zone, date_update timestamp without time zone, module varchar NOT NULL, model varchar NOT NULL, res_id integer, primary key(id));-- Records foreign keys and constraints installed by a module (so they can be-- removed when the module is uninstalled):-- - for a foreign key: type is 'f',-- - for a constraint: type is 'u' (this is the convention PostgreSQL uses).CREATE TABLE ir_model_constraint ( id serial NOT NULL, date_init timestamp without time zone, date_update timestamp without time zone, module integer NOT NULL references ir_module_module on delete restrict, model integer NOT NULL references ir_model on delete restrict, type character varying(1) NOT NULL, name varchar NOT NULL, primary key(id));-- Records relation tables (i.e. implementing many2many) installed by a module-- (so they can be removed when the module is uninstalled).CREATE TABLE ir_model_relation ( id serial NOT NULL, date_init timestamp without time zone, date_update timestamp without time zone, module integer NOT NULL references ir_module_module on delete restrict, model integer NOT NULL references ir_model on delete restrict, name varchar NOT NULL, primary key(id)); CREATE TABLE res_currency ( id serial, name varchar NOT NULL, primary key(id));CREATE TABLE res_company ( id serial, name varchar NOT NULL, partner_id integer, currency_id integer, primary key(id));CREATE TABLE res_partner ( id serial, name varchar, company_id integer, primary key(id)); 这20张表是odoo系统级的，它是模块加载及系统运行的基础。后续模块生成的表及相关数据都可以在这20张中找到蛛丝马迹。 数据库表初始化后，就可以加载模块数据（addons）到数据库了，这个也是odoo作为平台灵活的原因，所有的数据都在数据库。找到addons-path下所有的模块,然后一个一个的加载到数据库中。Info就是load模块的openerp.py文件，它是一个dict。 根据openerp.py中定义的category创建分类信息：将模块信息写入ir_module_module表：将module信息写入ir_model_data表：一个module要写两次ir_model_data表，写module的dependency表： 根据依赖关系进行判断，递归更新那些需要auto_install的模块状态为“to install”。 到目前为止，模块的加载都是在数据库级别，只是将“模块文件”信息存入数据库表，但是还没有真正加载到程序中。Odoo运行时查找object是通过Registry.get()获取的，而不是通过python自己的机制来找到相应的object，所以odoo在加载模块时会把模块下包含的model全部注册到models.py的module_to_models字典中。 **下面的步骤就是加载模块到内存： 加载base模块创建一个包含model层级的节点图，第二行代码将从数据库更新数据到graph中。然后调用load_module_graph方法加载模块，最终执行加载的方法： 这个方法是odoo加载model的核心，通过 import方法加载模块，这个是python的机制，当import到某个继承了BaseModel类的class时，它的实例化将有别于python自身的实例化操作，后者说它根本不会通过python自身的new方法创建实例，所有的实例创建都是通过 _build_model 方法及元类创建，并注册到module_to_models中。通过这种方式实例化model就可以解决我们在xml中配置model时指定的继承，字段，约束等各种属性。 标记需要加载或者更新的模块（db）加载被标记的模块（加载过程与加载base模块一致）完成及清理安装清理菜单删除卸载的模块核实model的view运行post-install测试","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"},{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"odoo源码解析1-启动命令","date":"2019-11-06T12:01:48.000Z","path":"2019/11/06/odoo/odoo源码解析1-启动命令/","text":"启动命令12345678#!/usr/bin/env python3# set server timezone in UTC before time module imported__import__('os').environ['TZ'] = 'UTC'import odooif __name__ == \"__main__\": odoo.cli.main() main函数主要是进行一些初始化和启动相关的命令 解析启动命令的参数 1234567891011121314151617181920212223242526272829def main(): args = sys.argv[1:] # The only shared option is '--addons-path=' needed to discover additional # commands from modules if len(args) &gt; 1 and args[0].startswith('--addons-path=') and not args[1].startswith(\"-\"): # parse only the addons-path, do not setup the logger... odoo.tools.config._parse_config([args[0]]) args = args[1:] # Default legacy command command = \"server\" # TODO: find a way to properly discover addons subcommands without importing the world # Subcommand discovery if len(args) and not args[0].startswith(\"-\"): logging.disable(logging.CRITICAL) for module in get_modules(): if isdir(joinpath(get_module_path(module), 'cli')): __import__('odoo.addons.' + module) logging.disable(logging.NOTSET) command = args[0] args = args[1:] if command in commands: o = commands[command]() o.run(args) else: sys.exit('Unknow command %r' % (command,))","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"},{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"pm2学习","date":"2019-11-02T02:10:05.000Z","path":"2019/11/02/pm2学习/","text":"pm2基本命令123456789101112131415161718192021222324252627# 启动程序pm2 start app.jspm2 start npm --name pro -- run dev# 查看程序pm2 start listpm2 monitpm2 logs# 重启pm2 restart allpm2 reload allpm2 restartt 0# 停止pm2 stop allpm2 stop 0# 杀死pm2 delete allpm2 delete 0# 集群pm2 start app.js -i max # 根据cpu数目启动线程pm2 start app.js -i 3 # 启动3个进程pm2 start app.js -x # 使用fork模式启动pm2 start app.json 日志问题日志系统对于任意应用而言，通常都是必不可少的一个辅助功能。pm2的相关文件默认存放于$HOME/.pm2/目录下，其日志主要有两类： a. pm2自身的日志，存放于$HOME/.pm2/pm2.log； b. pm2所管理的应用的日志，存放于$HOME/.pm2/logs/目录下，标准谁出日志存放于${APP_NAME}_out.log，标准错误日志存放于${APP_NAME}_error.log； 这里之所以把日志单独说明一下是因为，如果程序开发不严谨，为了调试程序，导致应用产生大量标准输出，使服务器本身记录大量的日志，导致服务磁盘满载问题。一般而言，pm2管理的应用本身都有自己日志系统，所以对于这种不必要的输出内容需禁用日志，重定向到/dev/null。 与crontab比较，也有类似情况，crontab自身日志，与其管理的应用本身的输出。应用脚本输出一定需要重定向到/dev/null，因为该输出内容会以邮件的形式发送给用户，内容存储在邮件文件，会产生意向不到的结果，或会导致脚本压根不被执行； 开机启动12pm2 startupsystemctl enable pm2-root 参考https://pm2.keymetrics.io/docs/usage/monitoring/","tags":[{"name":"node","slug":"node","permalink":"http://wumuwumu.github.io/tags/node/"}]},{"title":"docker网络模式","date":"2019-10-29T06:26:03.000Z","path":"2019/10/29/docker网络模式/","text":"Docker的网络模式详解1、Docker的四种网络模式 img （1）docker四种网络模式如下： Bridge contauner 桥接式网络模式 Host(open) container 开放式网络模式 Container(join) container 联合挂载式网络模式，是host网络模式的延伸 None(Close) container 封闭式网络模式 （2）可以通过docker network命令查看 12345[root@along ~]# docker network lsNETWORK ID NAME DRIVER SCOPEf23b4899add1 bridge bridge local65520497f693 host host locala0c5f18e0f04 none null local复制代码 （3）docker run –network 命令可以指定使用网络模式 2、Bridge 网络模式2.1 介绍 当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上，所以有默认地址172.17.0.0/16的地址。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。 从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中。可以通过brctl show命令查看。 123[root@along ~]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.024241c45d6e no复制代码 bridge模式是docker的默认网络模式，不写–net参数，就是bridge模式。使用docker run -p时，docker实际是在iptables做了DNAT规则，实现端口转发功能。可以使用iptables -t nat -vnL查看。 1234[root@along ~]# iptables -t nat -vnLChain POSTROUTING (policy ACCEPT 20 packets, 1238 bytes) pkts bytes target prot opt in out source destination 0 0 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0复制代码 2.2 bridge模式示意图 2.3 演示bridge 网络模式；–network不指定，默认也是bridge模式 12345678910111213141516171819202122232425262728[root@along ~]# docker run --name b1 -it --network bridge --rm busybox:latest / # ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:12 errors:0 dropped:0 overruns:0 frame:0 TX packets:6 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1016 (1016.0 B) TX bytes:508 (508.0 B)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)/ # route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0/ # ping 10.11.55.5 正常访问宿主机PING 10.11.55.5 (10.11.55.5): 56 data bytes64 bytes from 10.11.55.5: seq=0 ttl=64 time=0.292 ms/ # exit复制代码 3、Host 网络模式3.1 介绍 如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。 3.2 Host模式示意图 3.3 演示12345678910111213141516171819202122232425262728[root@along ~]# docker run --name b2 -it --network host --rm busybox:latest/ # ifconfig -a 和宿主机一样docker0 Link encap:Ethernet HWaddr 02:42:41:C4:5D:6E inet addr:172.17.0.1 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:41ff:fec4:5d6e/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:90 errors:0 dropped:0 overruns:0 frame:0 TX packets:26 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:5903 (5.7 KiB) TX bytes:2381 (2.3 KiB)eth0 Link encap:Ethernet HWaddr 00:0C:29:AB:D2:DA inet addr:10.11.55.5 Bcast:10.11.55.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:feab:d2da/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:3913 errors:0 dropped:0 overruns:0 frame:0 TX packets:3327 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:354314 (346.0 KiB) TX bytes:919096 (897.5 KiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)复制代码 4、Container 网络模式4.1 介绍 这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。 4.2 Container模式示意图 4.3 演示（1）在一个终端，使用bridge网络模式启动容器b1 12345678910111213141516171819202122232425[root@along ~]# docker run --name b1 -it --rm busybox:latest / # ifconfig b1的ip为172.17.0.2eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:6 errors:0 dropped:0 overruns:0 frame:0 TX packets:6 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:508 (508.0 B) TX bytes:508 (508.0 B)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)/ # echo &quot;hello world b1&quot; &gt; /tmp/index.html/ # httpd -h /tmp/ 在b1上启动httpd服务/ # netstat -nutlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 :::80 :::* LISTEN 复制代码 （2）在另一个终端使用Container 网络模式创建容器b2 12345678910111213141516171819202122[root@along ~]# docker run --name b2 -it --network container:b1 --rm busybox:latest/ # ifconfig -a b2的ip和b1一样eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:648 (648.0 B)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)/ # wget -O - -q 127.0.0.1 b1启动的httpd服务，在b2上直接访问hello world b1/ # ls /tmp/ 但是文件系统并不共享，只共享网络复制代码 5、None 网络模式5.1 介绍 使用none模式，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息，只有lo 网络接口。需要我们自己为Docker容器添加网卡、配置IP等。 不参与网络通信，运行于此类容器中的进程仅能访问本地回环接口；仅适用于进程无须网络通信的场景中，例如：备份、进程诊断及各种离线任务等。 5.2 Node模式示意图 5.3 演示12345678910111213[root@along ~]# docker run --name b1 -it --network none --rm busybox:latest / # ifconfig lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)/ # route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface","tags":[{"name":"docker","slug":"docker","permalink":"http://wumuwumu.github.io/tags/docker/"}]},{"title":"npm版本管理","date":"2019-10-29T06:08:30.000Z","path":"2019/10/29/npm版本管理/","text":"在打包项目的时候，我们都要更新package.json的版本号，接着给给代码添加tag，最后push代码，这样的流程泰国麻烦有什么方法简化。 12345671. package.json`中修改递增`version2. git add -A3. git commit -m &quot;update version&quot;4. git push5. git tag &lt;tag version&gt;6. git push --tag7. npm publish 解决方法我们可以使用npm version命令，从文档上我们可以看到其依据semver支持了大部分alias： 1npm version [&lt;newversion&gt; | major | minor | patch | premajor | preminor | prepatch | prerelease | from-git] 例：初始版本为1.0.0 npm version prepatch //预备补丁版本号 v1.0.1-0 npm version prerelease //预发布版本号 v1.0.1-1 npm version patch //补丁版本号 v1.0.2 npm version preminor //预备次版本号 v1.1.0-0 npm version minor //次版本号 v1.1.0 npm version premajor //预备主版本号 v2.0.0-0 npm version major //主版本号 v2.0.0 当在仓库中执行npm version时，会自动提交git commit并打上git tag。 当使用-m参数时，就可以自定义发布版本的信息，其中%s可以用来代替当前版本号 123&gt; npm version patch -m &quot;upgrade to %s for reasons&quot;&gt; 复制代码&gt; 这样以后版本迭代只需要以下步骤 npm version patch | minor | major | ...etc git push git push --tag npm publish npm version会同时创建时 v版本号 形式的tag，将tag push上去就可以自动触发构建了。 也可以简化这步操作，在npm version操作后自动 push 在 package.json中加入下面的代码，即可实现npm version操作后，自动push代码及tag，也就自动触发了 npm 发布操作。 123\"scripts\": &#123; \"postversion\": \"git push --follow-tags\"&#125; 衍生问题 如何发布beta，rc，alpha版本呢？如果发布了，应该如何安装？ 解决方案首先我们要理解这些版本的含义 alpha：内部测试版本 beta： 公开测试版本 rc： 候选版本（Release Candidate） 然后将package.json的version改成x.x.x-beta 配合npm publish --tag &lt;tag&gt;，我们可以发布对应的dist-tag 举个例子： 使用npm publish --tag beta发布后，然后就可以使用npm install &lt;pkg&gt;@beta安装对应版本的包。 我们可以通过npm dist-tag ls &lt;pkg&gt;来查看包的dist-tag 1234&#123; latest: 1.0.1, // 这就是npm publish默认发布的tag beta: 1.0.1-beta&#125; 当我们的beta版本稳定后，可以使用npm dist-tag add x.x.x-beta latest设置为稳定版本。 npm version与npm dist-tag关于npm version prerelease的作用我这里不再赘述，你可以查看这个文章。我只是记录一下关于npm version与npm dist-tag的使用： 第一步：发布第一个稳定版本 1npm publish//1.0.0 第二步：修改文件继续发布第二个版本 123git add -A &amp;&amp; git commit -m &quot;c&quot;npm version patchnpm publish//1.0.1 第三步：继续修改文件发布一个prerelease版本 123git add -A &amp;&amp; git commit -m &quot;c&quot;npm version prereleasenpm publish --tag -beta//版本n-n-n-n@1.0.2-0 第四步：继续修改发布第二个prerelease版本 123git add -A &amp;&amp; git commit -m &quot;c&quot;npm version prereleasenpm publish --tag -beta//版本n-n-n-n@1.0.2-1 第五步：npm info查看我们的版本信息 1234567891011121314151617181920212223242526&#123; name: 'n-n-n-n', 'dist-tags': &#123; latest: '1.0.1', '-beta': '1.0.2-1' &#125;, versions: [ '1.0.0', '1.0.1', '1.0.2-0', '1.0.2-1' ], maintainers: [ 'liangklfang &lt;liangklfang@163.com&gt;' ], time: &#123; modified: '2017-04-01T12:17:56.755Z', created: '2017-04-01T12:15:23.605Z', '1.0.0': '2017-04-01T12:15:23.605Z', '1.0.1': '2017-04-01T12:16:24.916Z', '1.0.2-0': '2017-04-01T12:17:23.354Z', '1.0.2-1': '2017-04-01T12:17:56.755Z' &#125;, homepage: 'https://github.com/liangklfang/n#readme', repository: &#123; type: 'git', url: 'git+https://github.com/liangklfang/n.git' &#125;, bugs: &#123; url: 'https://github.com/liangklfang/n/issues' &#125;, license: 'ISC', readmeFilename: 'README.md', version: '1.0.1', description: '', main: 'index.js', scripts: &#123; test: 'echo \"Error: no test specified\" &amp;&amp; exit 1' &#125;, author: '', gitHead: '8123b8addf6fed83c4c5edead1dc2614241a4479', dist: &#123; shasum: 'a60d8b02222e4cae74e91b69b316a5b173d2ac9d', tarball: 'https://registry.npmjs.org/n-n-n-n/-/n-n-n-n-1.0.1.tgz' &#125;, directories: &#123;&#125; &#125; 我们只要注意下面者两个部分： 12&apos;dist-tags&apos;: &#123; latest: &apos;1.0.1&apos;, &apos;-beta&apos;: &apos;1.0.2-1&apos; &#125;, versions: [ &apos;1.0.0&apos;, &apos;1.0.1&apos;, &apos;1.0.2-0&apos;, &apos;1.0.2-1&apos; ], 其中最新的稳定版本和最新的beta版本可以在dist-tags中看到，而versions数组中存储的是所有的版本。 第六步：npm dist-tag命令 1npm dist-tag ls n-n-n-n 即npm dist-tag获取到所有的最新的版本，包括prerelease与稳定版本，得到下面结果： 12-beta: 1.0.2-1latest: 1.0.1 第七步：当我们的prerelease版本已经稳定了，重新设置为稳定版本 1npm dist-tag add n-n-n-n@1.0.2-1 latest 此时你通过npm info查看可以知道： 1234567891011121314151617181920212223242526&#123; name: 'n-n-n-n', 'dist-tags': &#123; latest: '1.0.2-1', '-beta': '1.0.2-1' &#125;, versions: [ '1.0.0', '1.0.1', '1.0.2-0', '1.0.2-1' ], maintainers: [ 'liangklfang &lt;liangklfang@163.com&gt;' ], time: &#123; modified: '2017-04-01T12:24:55.800Z', created: '2017-04-01T12:15:23.605Z', '1.0.0': '2017-04-01T12:15:23.605Z', '1.0.1': '2017-04-01T12:16:24.916Z', '1.0.2-0': '2017-04-01T12:17:23.354Z', '1.0.2-1': '2017-04-01T12:17:56.755Z' &#125;, homepage: 'https://github.com/liangklfang/n#readme', repository: &#123; type: 'git', url: 'git+https://github.com/liangklfang/n.git' &#125;, bugs: &#123; url: 'https://github.com/liangklfang/n/issues' &#125;, license: 'ISC', readmeFilename: 'README.md', version: '1.0.2-1', description: '', main: 'index.js', scripts: &#123; test: 'echo \"Error: no test specified\" &amp;&amp; exit 1' &#125;, author: '', gitHead: '03189d2cc61604aa05f4b93e429d3caa3b637f8c', dist: &#123; shasum: '41ea170a6b155c8d61658cd4c309f0d5d1b12ced', tarball: 'https://registry.npmjs.org/n-n-n-n/-/n-n-n-n-1.0.2-1.tgz' &#125;, directories: &#123;&#125; &#125; 主要关注如下: 12&apos;dist-tags&apos;: &#123; latest: &apos;1.0.2-1&apos;, &apos;-beta&apos;: &apos;1.0.2-1&apos; &#125;, versions: [ &apos;1.0.0&apos;, &apos;1.0.1&apos;, &apos;1.0.2-0&apos;, &apos;1.0.2-1&apos; ] 此时latest版本已经是prerelease版本”1.0.2-1”了！此时用户如果直接运行npm install就会安装我们的prerelease版本了，因为版本已经更新了！ 当然，我们的npm publish可以有很多tag的，比如上面是beta，也可以是stable, dev, canary等，比如下面你继续运行： 123git add -A &amp;&amp; git commit -m &quot;c&quot;npm version prereleasenpm publish --tag -dev 此时你运行npm info就会得到下面的信息： 123456789101112131415161718192021222324252627&#123; name: &apos;n-n-n-n&apos;, &apos;dist-tags&apos;: &#123; latest: &apos;1.0.2-1&apos;, &apos;-beta&apos;: &apos;1.0.2-1&apos;, &apos;-dev&apos;: &apos;1.0.2-2&apos; &#125;, versions: [ &apos;1.0.0&apos;, &apos;1.0.1&apos;, &apos;1.0.2-0&apos;, &apos;1.0.2-1&apos;, &apos;1.0.2-2&apos; ], maintainers: [ &apos;liangklfang &lt;liangklfang@163.com&gt;&apos; ], time: &#123; modified: &apos;2017-04-01T13:01:17.106Z&apos;, created: &apos;2017-04-01T12:15:23.605Z&apos;, &apos;1.0.0&apos;: &apos;2017-04-01T12:15:23.605Z&apos;, &apos;1.0.1&apos;: &apos;2017-04-01T12:16:24.916Z&apos;, &apos;1.0.2-0&apos;: &apos;2017-04-01T12:17:23.354Z&apos;, &apos;1.0.2-1&apos;: &apos;2017-04-01T12:17:56.755Z&apos;, &apos;1.0.2-2&apos;: &apos;2017-04-01T13:01:17.106Z&apos; &#125;, homepage: &apos;https://github.com/liangklfang/n#readme&apos;, repository: &#123; type: &apos;git&apos;, url: &apos;git+https://github.com/liangklfang/n.git&apos; &#125;, bugs: &#123; url: &apos;https://github.com/liangklfang/n/issues&apos; &#125;, license: &apos;ISC&apos;, readmeFilename: &apos;README.md&apos;, version: &apos;1.0.2-1&apos;, description: &apos;&apos;, main: &apos;index.js&apos;, scripts: &#123; test: &apos;echo &quot;Error: no test specified&quot; &amp;&amp; exit 1&apos; &#125;, author: &apos;&apos;, gitHead: &apos;03189d2cc61604aa05f4b93e429d3caa3b637f8c&apos;, dist: &#123; shasum: &apos;41ea170a6b155c8d61658cd4c309f0d5d1b12ced&apos;, tarball: &apos;https://registry.npmjs.org/n-n-n-n/-/n-n-n-n-1.0.2-1.tgz&apos; &#125;, directories: &#123;&#125; &#125; 重点关注如下内容 12&apos;dist-tags&apos;: &#123; latest: &apos;1.0.2-1&apos;, &apos;-beta&apos;: &apos;1.0.2-1&apos;, &apos;-dev&apos;: &apos;1.0.2-2&apos; &#125;, versions: [ &apos;1.0.0&apos;, &apos;1.0.1&apos;, &apos;1.0.2-0&apos;, &apos;1.0.2-1&apos;, &apos;1.0.2-2&apos; ], 此时你会看到-beta版本最新是1.0.2-1，而-dev版本最新是1.0.2-2 参考 https://github.com/liangklfangl/npm-dist-tag https://juejin.im/post/5b624d42f265da0fa1223ffa https://docs.npmjs.com/cli/version","tags":[{"name":"node","slug":"node","permalink":"http://wumuwumu.github.io/tags/node/"}]},{"title":"python中and和or用法","date":"2019-10-25T07:41:30.000Z","path":"2019/10/25/python/python中and和or用法/","text":"在Python 中，and 和 or 执行布尔逻辑演算，如你所期待的一样。但是它们并不返回布尔值，而是返回它们实际进行比较的值之一。 （类似C++里面的&amp;&amp;和||的短路求值） （ 在布尔环境中，0、”、[]、()、{}、None为假；其它任何东西都为真。但是可以在类中定义特定的方法使得类实例的演算值为假。） and实例：123456&gt;&gt;&gt; 'a' and 'b''b'&gt;&gt;&gt; '' and 'b'''&gt;&gt;&gt; 'a' and 'b' and 'c''c'12345 从左到右扫描，返回第一个为假的表达式值，无假值则返回最后一个表达式值。 or实例：123456&gt;&gt;&gt; 'a' or 'b''a'&gt;&gt;&gt; '' or 'b''b'&gt;&gt;&gt; '' or [] or&#123;&#125;&#123;&#125;12345 从左到右扫描，返回第一个为真的表达式值，无真值则返回最后一个表达式值。 and-or搭配：123456&gt;&gt;&gt; a = \"betabin\"&gt;&gt;&gt; b = \"python\"&gt;&gt;&gt; 1 and a or b'betabin'&gt;&gt;&gt; 0 and a or b'python'12345 看起来类似于于我们Ｃ＋＋中的条件运算符（bool？a：b），是的，当a为true的时候是一样的。但是，当a为false的时候，就明显不同了。 如果坚持要用and-or技巧来实现条件运算符的话，可以用种安全的方法： 1234&gt;&gt;&gt; a = \"\"&gt;&gt;&gt; b = \"betabin\"&gt;&gt;&gt; (1 and [a] or [b])[0]''123 就是万能的[]，把a为假的可能性给抹杀掉，然后通过[0]再获得（因为要通过[0]获得元素，所以b也得加上[]）。 这个and-or技巧主要在lambda中使用。","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"使用jenv对java多版本管理","date":"2019-10-25T02:42:43.000Z","path":"2019/10/25/java/使用jenv对java多版本管理/","text":"配置JDK环境变量 打开 vim ~/.bash_profile 文件 进行添加 1234export JAVA_8_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_112.jdk/Contents/Homeexport JAVA_7_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home# 默认激活 jdk8export JAVA_HOME=$JAVA_8_HOME 编辑完成，重新加载 .bash_profile 1$ source ~/.bash_profile jEnv安装 安装 1$ brew install jenv 配置 安装了zsh，配置如下 12$ echo &apos;export PATH=&quot;$HOME/.jenv/bin:$PATH&quot;&apos; &gt;&gt; ~/.zshrc$ echo &apos;eval &quot;$(jenv init -)&quot;&apos; &gt;&gt; ~/.zshrc 如果是默认的bash 12$ echo &apos;export PATH=&quot;$HOME/.jenv/bin:$PATH&quot;&apos; &gt;&gt; ~/.bash_profile$ echo &apos;eval &quot;$(jenv init -)&quot;&apos; &gt;&gt; ~/.bash_profilec jEnv配置JDK查看安装的java版本，如果我们一开始未添加jdk，执行jenv versions 应该是空的，* 号位置表示当前的jdk版本 12345678$ jenv versions system 1.7* 1.7.0.80 (set by /Users/gulj/.java-version) 1.8 1.8.0.112 oracle64-1.7.0.80 oracle64-1.8.0.112 重启下terminal，为jEnv添加java版本 1234添加jdk7$ jenv add /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home添加jdk8$ jenv add /Library/Java/JavaVirtualMachines/jdk1.8.0_112.jdk/Contents/Home 添加完jdk7和jdk8之后，再执行 jenv versions 命令就会看到我们添加的jdk jEnv常用命令 移除指定版本jdk 1$ jenv remove 1.8.0.111 选择一个jdk版本 1$ jenv local 1.8.0.111 设置默认的jdk版本 1$ jenv global 1.8.0.111 查看当前版本jdk的路径 1jenv which java","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"react-tree-walker学习","date":"2019-10-19T09:02:13.000Z","path":"2019/10/19/react-tree-walker学习/","text":"react-tree-walker这个主要用于遍历react的dom树，用于在react服务端渲染数据请求的时候。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import reactTreeWalker from 'react-tree-walker'class DataFetcher extends React.Component &#123; constructor(props) &#123; super(props) this.getData = this.getData.bind(this) &#125; getData() &#123; // Supports promises! You could call an API for example to fetch some // data, or do whatever \"bootstrapping\" you desire. return Promise.resolve(this.props.id) &#125; render() &#123; return &lt;div&gt;&#123;this.props.children&#125;&lt;/div&gt; &#125;&#125;const app = ( &lt;div&gt; &lt;h1&gt;Hello World!&lt;/h1&gt; &lt;DataFetcher id=&#123;1&#125; /&gt; &lt;DataFetcher id=&#123;2&#125;&gt; &lt;DataFetcher id=&#123;3&#125;&gt; &lt;DataFetcher id=&#123;4&#125; /&gt; &lt;/DataFetcher&gt; &lt;/DataFetcher&gt; &lt;DataFetcher id=&#123;5&#125; /&gt; &lt;/div&gt;)const values = []// You provide this! See the API docs below for full details.function visitor(element, instance) &#123; if (instance &amp;&amp; typeof instance.getData) &#123; return instance.getData().then(value =&gt; &#123; values.push(value) // Return \"false\" to indicate that we do not want to visit \"3\"'s children, // therefore we do not expect \"4\" to make it into our values array. return value !== 3 &#125;) &#125;&#125;reactTreeWalker(app, visitor) .then(() =&gt; &#123; console.log(values) // [1, 2, 3, 5]; // Now is a good time to call React's renderToString whilst exposing // whatever values you built up to your app. &#125;) // since v3.0.0 you need to do your own error handling! .catch(err =&gt; console.error(err)) react-ssr-prepass这个项目还在维护，是一个不错的选择 安装123yarn add react-ssr-prepass# ornpm install --save react-ssr-prepass 使用12345678910111213141516171819import &#123; createElement &#125; from 'react'import &#123; renderToString &#125; from 'react-dom/server'import ssrPrepass from 'react-ssr-prepass'const renderApp = async App =&gt; &#123; const element = createElement(App) await ssrPrepass(element) return renderToString(element)&#125;ssrPrepass(&lt;App /&gt;, (element, instance) =&gt; &#123; if (element.type === SomeData) &#123; return fetchData() &#125; else if (instance &amp;&amp; instance.fetchData) &#123; return instance.fetchData() &#125;&#125;)","tags":[{"name":"react","slug":"react","permalink":"http://wumuwumu.github.io/tags/react/"}]},{"title":"weboack性能优化笔记","date":"2019-10-18T03:52:51.000Z","path":"2019/10/18/weboack性能优化笔记/","text":"https://juejin.im/post/5b652b036fb9a04fa01d616b","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"}]},{"title":"lodash按需加载","date":"2019-10-18T03:48:59.000Z","path":"2019/10/18/lodash按需加载/","text":"lodash提供了很多可用的方法供我们使用，绝对是一个很好用且用起来得心应手的工具库。但是同时，lodash的体积也不小，我们项目中使用的大概522K，可能只是使用了几个方法，但是却把整个lodash库引入了。为了吃几条鱼，就承包了整个鱼塘，代价有点大呀！ 对于这个问题，有几种方案可供选择。 一.引入单个函数 lodash整个安装完之后，引用方式： lodash/function 格式，单独引入某个函数，如 let _trim= require(‘lodash/trim’) 或者 import trim from ‘lodash/trim’ 或者 lodash 中的每个函数在 NPM 都有一个单独的发布模块，单独安装并引用部分模块，然后按以下方式引用 let _trim= require(‘lodash.trim’) 或者 import trim from ‘lodash.trim’ trim(‘ 123123 ‘) 二.借助 lodash-webpack-plugin，babel-plugin-lodash插件优化 使用上述两种方式，在使用较多个lodash中方法的情况下，不太美观，且并不方便。那么我们可以借助于lodash-webpack-plugin，去除未引入的模块，需要和babel-plugin-lodash插件配合使用。类似于webpack的tree-shaking。 1）安装插件：npm i -S lodash-webpack-plugin babel-plugin-lodash 2）webpack.conf.js中 var LodashModuleReplacementPlugin = require(‘lodash-webpack-plugin’) plugins: [ new LodashModuleReplacementPlugin()] 3）.babelrc中配置 “plugins”: [“transform-runtime”,”transform-vue-jsx”,”lodash”] 或者在webpack.conf.js的rules配置 1234567&#123; test: /\\.(js|jsx)$/, loader: &apos;babel-loader&apos;, exclude: /node_modules/, include: [resolve(&apos;src&apos;), resolve(&apos;test&apos;)] options: &#123;plugins: [&apos;lodash&apos;]&#125;&#125; 三.lodash-es结合tree-shaking lodash-es 是着具备 ES6 模块化的版本，只需要直接引入就可以。 import {isEmpty,forIn, cloneDeep} from ‘lodash-es’ tree-shaking的作用，即移除上下文中未引用的代码（dead code） 只有当函数给定输入后，产生相应的输出，且不修改任何外部的东西，才可以安全做shaking的操作 如何使用tree-shaking？ 1）.确保代码是es6格式,即 export，import 2）.package.json中，设置sideEffects 3）.确保tree-shaking的函数没有副作用 4）.babelrc中设置presets [[“env”, { “modules”: false }]] 禁止转换模块，交由webpack进行模块化处理 5）.结合uglifyjs-webpack-plugin","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"}]},{"title":"babel配置","date":"2019-10-18T03:21:01.000Z","path":"2019/10/18/babel配置/","text":"Babel6Babel6 现在使用的越来越少了，但是还是做一个笔记，现在基本都使用babel-preset-env，不需要写babel-preset-esxxxx了，但是babel-preset-stage-x还是要自己去加的。 安装1npm install -D babel-cli babel-preset-env 配置文件Babel6的配置文件是.babelrc 123&#123; //https://juejin.im/post/5a79adeef265da4e93116430&#125; Babel7Babel7 相对于babel6有很大的变化，相关的模块的名字有很大的变化，官方舍弃了babel-preset-esxxxx和babel-preset-stage-x，后者的原因是提案一直在变化。 安装1npm install -D @babel/cli @babel/react @babel/plugin-transform-runtime @babel/env 配置文件Babel7有两种配置文件，一个是.babelrc，是局部的，另外一个是babel.config.js是全局的，具体的可以看下官网。7版本的配置文件解析也变得更加严格。 ###","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"},{"name":"babel","slug":"babel","permalink":"http://wumuwumu.github.io/tags/babel/"}]},{"title":"Hello World","date":"2019-10-14T09:01:07.430Z","path":"2019/10/14/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]},{"title":"php5环境搭建","date":"2019-09-02T15:14:11.000Z","path":"2019/09/02/php5环境搭建/","text":"安装nginx12yum install epel-releaseyum install nginx 安装phpremi源可以获取更高的版本，php-fpm是要启动的 123rpm -ivh http://rpms.famillecollet.com/enterprise/remi-release-7.rpmyum install --enablerepo=remi --enablerepo=remi-php56 php php-fpmyum install --enablerepo=remi --enablerepo=remi-php56 php-opcache php-mbstring php-mysql* php-gd php-redis php-mcrypt php-xml php-redis nginx配置123456789101112131415161718server &#123; listen 80; server_name www.test.com test.com; root /data/www/Public; index index.php index.html; location / &#123; try_files $uri $uri/ /index.php?$args; &#125; location ~ index.php &#123; fastcgi_connect_timeout 20s; # default of 60s is just too long fastcgi_read_timeout 20s; # default of 60s is just too long include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_pass 127.0.0.1:9000; # assumes you are running php-fpm locally on port 9000 fastcgi_param PHP_VALUE \"open_basedir=/data/www/:/data/www/Data:/tmp/\"; &#125;&#125; 开启php的日志 修改 php-fpm.conf 文件，添加（或修改）如下配置： 12345[global] error_log = log/error_log [www] catch_workers_output = yes 修改 php.ini 文件，添加（或修改）如下配置： 123log_errors = Onerror_log = &quot;/usr/local/lnmp/php/var/log/error_log&quot;error_reporting=E_ALL&amp;~E_NOTICE 重启 php-fpm","tags":[]},{"title":"linux压缩","date":"2019-09-02T14:46:45.000Z","path":"2019/09/02/linux压缩/","text":"tar12345678910111213141516171819202122232425262728# 打包tar -cvf xx.tar dirName# 解包tar -xvf xx.tar# .gz# 解压gunzip fileName.gzgzip -d fileName.gz# 压缩gzip fileName# .tar.gz 和.tgz# 解压tar zxvf fileName.tar.gz# 压缩tar zcvf filename.tar.gz dirName# bz2# 解压bzip2 -d fileName.bzbunzip2 fileName.bz# .tar.bz# 解压tar jxvf fileName.tar.bz# 压缩tar jcvf fileName.tar.bz dirName zip12345678# 安装yum install zip unzip# 解压unzip mydata.zip -d mydatabak# 压缩zip -r abc123.zip abc 123.txt rar1234567891011# 安装wget http://www.rarlab.com/rar/rarlinux-x64-5.3.0.tar.gztar -zxvf rarlinux-x64-5.3.0.tar.gz // 对应64位下载的cd rarmake# 解压rar x fileName.rar# 压缩rar fileName.rar dirName 7z1234567891011# 安装yum install p7zip p7zip-plugins# 压缩7za a 压缩包.7z 被压缩文件或目录# 解压#将压缩包解压到指定目录，注意：指定目录参数-o后面不要有空格7za x 压缩包.7z -o解压目录#将压缩包解压到当前目录7za x 压缩包.7z","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"nginx伪静态","date":"2019-09-02T14:26:40.000Z","path":"2019/09/02/nginx/nginx伪静态/","text":"伪静态伪静态是一种可以把文件后缀改成任何可能的一种方法，如果我想把PHP文件伪静态成html文件，这种相当简单的。nginx里使用伪静态是直接在nginx.conf 中写规则的，而apache要开启写模块(mod_rewrite)才能进行伪静态。nginx只需要打开nginx.conf配置文件,然后在里面写需要的规则就可以了。 1、Nginx伪静态案例：（Nginx用伪静态是不需要配置的） 找到nginx.conf配置文件：nginx.conf，然后打开，找到server {} 在里面加上： 下面加的意思是隐藏掉index.php： 1234567location / &#123; # 其他的一些规则，自己加 if(!-e $request_filename) &#123; rewrite ^(.*)$ /index.php?s=$1 last; break; &#125;&#125; 2、每个网站独立的配置文件（独立的伪静态规则）： 我们正常的时候每个网站都会有独立的配置文件，直接去改配置文件就好了。然后nginx.conf引入他们所有的配置文件就好了： 如：在nginx.conf配置文件最下面添加以下代码： 1include vhost/*.conf; 说明：引入nginx.conf配置文件所在目录下vhost目录下的所有以.conf的配置文件！ 以下就是其中一个网站的配置文件内容：规则就是隐藏掉index.php 123456789101112131415161718192021222324server &#123; listen 80; root /www/web/admin/public; server_name www.admin.com; index index.html index.php index.htm; error_page 400 /errpage/400.html; error_page 403 /errpage/403.html; error_page 404 /errpage/404.html; error_page 503 /errpage/503.html; location ~ \\.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fcgi.conf; &#125; location ~ /\\.ht &#123; deny all; &#125; location / &#123; if (!-e $request_filename) &#123; rewrite ^(.*)$ /index.php?s=$1 last; break; &#125; &#125;&#125; nginx url重写url重写是指通过配置conf文件，以让网站的url中达到某种状态时则定向/跳转到某个规则，比如常见的伪静态、301重定向、浏览器定向等 rewrite语法在配置文件的server块中写，如： 123server &#123; rewrite 规则 定向路径 重写类型;&#125; 规则：可以是字符串或者正则来表示想匹配的目标url 定向路径：表示匹配到规则后要定向的路径，如果规则里有正则，则可以使用$index来表示正则里的捕获分组 重写类型： last ：相当于Apache里德(L)标记，表示完成rewrite，浏览器地址栏URL地址不变 break；本条规则匹配完成后，终止匹配，不再匹配后面的规则，浏览器地址栏URL地址不变 redirect：返回302临时重定向，浏览器地址会显示跳转后的URL地址 permanent：返回301永久重定向，浏览器地址栏会显示跳转后的URL地址 简单例子1234567891011121314server &#123; # 访问 /last.html 的时候，页面内容重写到 /index.html 中 rewrite /last.html /index.html last; # 访问 /break.html 的时候，页面内容重写到 /index.html 中，并停止后续的匹配 rewrite /break.html /index.html break; # 访问 /redirect.html 的时候，页面直接302定向到 /index.html中 rewrite /redirect.html /index.html redirect; # 访问 /permanent.html 的时候，页面直接301定向到 /index.html中 rewrite /permanent.html /index.html permanent; # 把 /html/*.html =&gt; /post/*.html ，301定向 rewrite ^/html/(.+?).html$ /post/$1.html permanent; # 把 /search/key =&gt; /search.html?keyword=key rewrite ^/search\\/([^\\/]+?)(\\/|$) /search.html?keyword=$1 permanent;&#125; last和break的区别因为301和302不能简单的只返回状态码，还必须有重定向的URL，这就是return指令无法返回301,302的原因了。这里 last 和 break 区别有点难以理解： last一般写在server和if中，而break一般使用在location中 last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配 break和last都能组织继续执行后面的rewrite指令 在location里一旦返回break则直接生效并停止后续的匹配location 123456789server &#123; location / &#123; rewrite /last/ /q.html last; rewrite /break/ /q.html break; &#125; location = /q.html &#123; return 400; &#125;&#125; 访问/last/时重写到/q.html，然后使用新的uri再匹配，正好匹配到locatoin = /q.html然后返回了400 访问/break时重写到/q.html，由于返回了break，则直接停止了 if判断只是上面的简单重写很多时候满足不了需求，比如需要判断当文件不存在时、当路径包含xx时等条件，则需要用到if 语法1if (表达式) &#123;&#125; 当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false 直接比较变量和内容时，使用=或!= ~正则表达式匹配，~*不区分大小写的匹配，!~区分大小写的不匹配 一些内置的条件判断： -f和!-f用来判断是否存在文件 -d和!-d用来判断是否存在目录 -e和!-e用来判断是否存在文件或目录 -x和!-x用来判断文件是否可执行 内置的全局变量123456789101112131415161718192021$args ：这个变量等于请求行中的参数，同$query_string$content_length ： 请求头中的Content-length字段。$content_type ： 请求头中的Content-Type字段。$document_root ： 当前请求在root指令中指定的值。$host ： 请求主机头字段，否则为服务器名称。$http_user_agent ： 客户端agent信息$http_cookie ： 客户端cookie信息$limit_rate ： 这个变量可以限制连接速率。$request_method ： 客户端请求的动作，通常为GET或POST。$remote_addr ： 客户端的IP地址。$remote_port ： 客户端的端口。$remote_user ： 已经经过Auth Basic Module验证的用户名。$request_filename ： 当前请求的文件路径，由root或alias指令与URI请求生成。$scheme ： HTTP方法（如http，https）。$server_protocol ： 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。$server_addr ： 服务器地址，在完成一次系统调用后可以确定这个值。$server_name ： 服务器名称。$server_port ： 请求到达服务器的端口号。$request_uri ： 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。$uri ： 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。$document_uri ： 与$uri相同。 如： 12345678访问链接是：http://localhost:88/test1/test2/test.php 网站路径是：/var/www/html$host：localhost$server_port：88$request_uri：http://localhost:88/test1/test2/test.php$document_uri：/test1/test2/test.php$document_root：/var/www/html$request_filename：/var/www/html/test1/test2/test.php 例子12345678910111213141516# 如果文件不存在则返回400if (!-f $request_filename) &#123; return 400;&#125;# 如果host不是xuexb.com，则301到xuexb.com中if ( $host != 'xuexb.com' )&#123; rewrite ^/(.*)$ https://xuexb.com/$1 permanent;&#125;# 如果请求类型不是POST则返回405if ($request_method = POST) &#123; return 405;&#125;# 如果参数中有 a=1 则301到指定域名if ($args ~ a=1) &#123; rewrite ^ http://example.com/ permanent;&#125; 在某种场景下可结合location规则来使用，如： 1234567891011# 访问 /test.html 时location = /test.html &#123; # 默认值为xiaowu set $name xiaowu; # 如果参数中有 name=xx 则使用该值 if ($args ~* name=(\\w+?)(&amp;|$)) &#123; set $name $1; &#125; # 301 rewrite ^ /$name.html permanent;&#125; 上面表示： /test.html =&gt; /xiaowu.html /test.html?name=ok =&gt; /ok.html?name=ok location语法在server块中使用，如： 123server &#123; location 表达式 &#123; &#125;&#125; location表达式类型 如果直接写一个路径，则匹配该路径下的 ~ 表示执行一个正则匹配，区分大小写 ~* 表示执行一个正则匹配，不区分大小写 ^~ 表示普通字符匹配。使用前缀匹配。如果匹配成功，则不再匹配其他location。 = 进行普通字符精确匹配。也就是完全匹配。 优先级 等号类型（=）的优先级最高。一旦匹配成功，则不再查找其他匹配项。 ^~类型表达式。一旦匹配成功，则不再查找其他匹配项。 正则表达式类型（~ ~*）的优先级次之。如果有多个location的正则能匹配的话，则使用正则表达式最长的那个。 常规字符串匹配类型。按前缀匹配。 例子 - 假地址掩饰真地址1234567891011server &#123; # 用 xxoo_admin 来掩饰 admin location / &#123; # 使用break拿一旦匹配成功则忽略后续location rewrite /xxoo_admin /admin break; &#125; # 访问真实地址直接报没权限 location /admin &#123; return 403; &#125;&#125; 参考https://www.toolnb.com/tools/rewriteTools.html","tags":[{"name":"nginx","slug":"nginx","permalink":"http://wumuwumu.github.io/tags/nginx/"}]},{"title":"spring数据库事务","date":"2019-09-01T12:22:24.000Z","path":"2019/09/01/spring数据库事务/","text":"接口PlatformTransactionManagerPlatformTransactionManager接口中定义了三个方法： 123456789Public interface PlatformTransactionManager()...&#123; // Return a currently active transaction or create a new one, according to the specified propagation behavior（根据指定的传播行为，返回当前活动的事务或创建一个新事务。） TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // Commit the given transaction, with regard to its status（使用事务目前的状态提交事务） Void commit(TransactionStatus status) throws TransactionException; // Perform a rollback of the given transaction（对执行的事务进行回滚） Void rollback(TransactionStatus status) throws TransactionException; &#125; 复制代码 我们刚刚也说了Spring中PlatformTransactionManager根据不同持久层框架所对应的接口实现类,几个比较常见的如下图所示 比如我们在使用JDBC或者iBatis（就是Mybatis）进行数据持久化操作时,我们的xml配置通常如下： 123456&lt;!-- 事务管理器 --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!-- 数据源 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt;&lt;/bean&gt; TransactionDefinition事务管理器接口 PlatformTransactionManager 通过 getTransaction(TransactionDefinition definition) 方法来得到一个事务，这个方法里面的参数是 TransactionDefinition类 ，这个类就定义了一些基本的事务属性。 那么什么是事务属性呢？ 事务属性可以理解成事务的一些基本配置，描述了事务策略如何应用到方法上。事务属性包含了5个方面。 TransactionDefinition接口中的方法如下：TransactionDefinition接口中定义了5个方法以及一些表示事务属性的常量比如隔离级别、传播行为等等的常量。 我下面只是列出了TransactionDefinition接口中的方法而没有给出接口中定义的常量，该接口中的常量信息会在后面依次介绍到。 123456789101112public interface TransactionDefinition &#123; // 返回事务的传播行为 int getPropagationBehavior(); // 返回事务的隔离级别，事务管理器根据它来控制另外一个事务可以看到本事务内的哪些数据 int getIsolationLevel(); // 返回事务必须在多少秒内完成 //返回事务的名字 String getName()； int getTimeout(); // 返回是否优化为只读事务。 boolean isReadOnly();&#125; TransactionStatusPlatformTransactionManager.getTransaction(…) 方法返回一个 TransactionStatus 对象。返回的TransactionStatus 对象可能代表一个新的或已经存在的事务（如果在当前调用堆栈有一个符合条件的事务）。TransactionStatus 接口提供了一个简单的控制事务执行和查询事务状态的方法。该接口定义如清单3所示： 清单3. TransactionStatus 接口中定义的主要方法1`public interface TransactionStatus&#123;`` ``boolean isNewTransaction();`` ``void setRollbackOnly();`` ``boolean isRollbackOnly();``&#125;` 事务管理API分析事务隔离级别隔离级别是指若干个并发的事务之间的隔离程度。TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是TransactionDefinition.ISOLATION_READ_COMMITTED。 TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读和不可重复读，因此很少使用该隔离级别。 TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。 TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。即使在多次查询之间有新增的数据满足该查询，这些新增的记录也会被忽略。该级别可以防止脏读和不可重复读。 TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 事务传播行为所谓事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务性方法的执行行为。在TransactionDefinition定义中包括了如下几个表示传播行为的常量： TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 这里需要指出的是，前面的六种事务传播行为是 Spring 从 EJB 中引入的，他们共享相同的概念。而 PROPAGATION_NESTED是 Spring 所特有的。以 PROPAGATION_NESTED 启动的事务内嵌于外部事务中（如果存在外部事务的话），此时，内嵌事务并不是一个独立的事务，它依赖于外部事务的存在，只有通过外部的事务提交，才能引起内部事务的提交，嵌套的子事务不能单独提交。如果熟悉 JDBC 中的保存点（SavePoint）的概念，那嵌套事务就很容易理解了，其实嵌套的子事务就是保存点的一个应用，一个事务中可以包括多个保存点，每一个嵌套子事务。另外，外部事务的回滚也会导致嵌套子事务的回滚。 事务超时所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。 事务的只读属性事务的只读属性是指，对事务性资源进行只读操作或者是读写操作。所谓事务性资源就是指那些被事务管理的资源，比如数据源、 JMS 资源，以及自定义的事务性资源等等。如果确定只对事务性资源进行只读操作，那么我们可以将事务标志为只读的，以提高事务处理的性能。在 TransactionDefinition 中以 boolean 类型来表示该事务是否只读。 应用场合： 如果你一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持SQL执行期间的读一致性；如果你一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询SQL必须保证整体的读一致性，否则，在前条SQL查询之后，后条SQL查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持。【注意是一次执行多次查询来统计某些信息，这时为了保证数据整体的一致性，要用只读事务】 怎样设置： 对于只读查询，可以指定事务类型为readonly，即只读事务。由于只读事务不存在数据的修改，因此数据库将会为只读事务提供一些优化手段，例如Oracle对于只读事务，不启动回滚段，不记录回滚log。 （1）在JDBC中，指定只读事务的办法为： connection.setReadOnly(true); （2）在Hibernate中，指定只读事务的办法为： session.setFlushMode(FlushMode.NEVER);此时，Hibernate也会为只读事务提供Session方面的一些优化手段 （3）在Spring的Hibernate封装中，指定只读事务的办法为： bean配置文件中，prop属性增加“readOnly”或者用注解方式@Transactional(readOnly=true)【 if the transaction is marked as read-only, Spring will set the Hibernate Session’s flush mode to FLUSH_NEVER,and will set the JDBC transaction to read-only】也就是说在Spring中设置只读事务是利用上面两种方式 事务的回滚规则通常情况下，如果在事务中抛出了未检查异常（继承自 RuntimeException 的异常），则默认将回滚事务。如果没有抛出任何异常，或者抛出了已检查异常，则仍然提交事务。这通常也是大多数开发者希望的处理方式，也是 EJB 中的默认处理方式。但是，我们可以根据需要人为控制事务在抛出某些未检查异常时任然提交事务，或者在抛出某些已检查异常时回滚事务。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"},{"name":"spring","slug":"spring","permalink":"http://wumuwumu.github.io/tags/spring/"}]},{"title":"数据库事务","date":"2019-09-01T12:18:43.000Z","path":"2019/09/01/mysql/数据库事务/","text":"本篇讲诉数据库中事务的四大特性（ACID），并且将会详细地说明事务的隔离级别。 如果一个数据库声称支持事务的操作，那么该数据库必须要具备以下四个特性： ⑴ 原子性（Atomicity） 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，这和前面两篇博客介绍事务的功能是一样的概念，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 ⑵ 一致性（Consistency） 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 ⑶ 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 关于事务的隔离性数据库提供了多种隔离级别，稍后会介绍到。 ⑷ 持久性（Durability） 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 以上介绍完事务的四大特性(简称ACID)，现在重点来说明下事务的隔离性，当多个线程都开启事务操作数据库中的数据时，数据库系统要能进行隔离操作，以保证各个线程获取数据的准确性，在介绍数据库提供的各种隔离级别之前，我们先看看如果不考虑事务的隔离性，会发生的几种问题： 1，脏读 脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。 当一个事务正在多次修改某个数据，而在这个事务中这多次的修改都还未提交，这时一个并发的事务来访问该数据，就会造成两个事务得到的数据不一致。例如：用户A向用户B转账100元，对应SQL命令如下 123update account set money=money+100 where name=’B’; (此时A通知B)update account set money=money - 100 where name=’A’; 当只执行第一条SQL时，A通知B查看账户，B发现确实钱已到账（此时即发生了脏读），而之后无论第二条SQL是否执行，只要该事务不提交，则所有操作都将回滚，那么当B以后再次查看账户时就会发现钱其实并没有转。 2，不可重复读 不可重复读是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。 例如事务T1在读取某一数据，而事务T2立马修改了这个数据并且提交事务给数据库，事务T1再次读取该数据就得到了不同的结果，发送了不可重复读。 不可重复读和脏读的区别是，脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。 在某些情况下，不可重复读并不是问题，比如我们多次查询某个数据当然以最后查询得到的结果为主。但在另一些情况下就有可能发生问题，例如对于同一个数据A和B依次查询就可能不同，A和B就可能打起来了…… 3，虚读(幻读) 幻读是事务非独立执行时发生的一种现象。例如事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有一行没有修改，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。 幻读和不可重复读都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 现在来看看MySQL数据库为我们提供的四种隔离级别： ① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 ② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 ③ Read committed (读已提交)：可避免脏读的发生。 ④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。 以上四种隔离级别最高的是Serializable级别，最低的是Read uncommitted级别，当然级别越高，执行效率就越低。像Serializable这样的级别，就是以锁表的方式(类似于Java多线程中的锁)使得其他的线程只能在锁外等待，所以平时选用何种隔离级别应该根据实际情况。在MySQL数据库中默认的隔离级别为Repeatable read (可重复读)。 在MySQL数据库中，支持上面四种隔离级别，默认的为Repeatable read (可重复读)；而在Oracle数据库中，只支持Serializable (串行化)级别和Read committed (读已提交)这两种级别，其中默认的为Read committed级别。 在MySQL数据库中查看当前事务的隔离级别： 1select @@tx_isolation; 在MySQL数据库中设置事务的隔离 级别： 12set [glogal | session] transaction isolation level 隔离级别名称;set tx_isolation=’隔离级别名称;’ 后记：隔离级别的设置只对当前链接有效。对于使用MySQL命令窗口而言，一个窗口就相当于一个链接，当前窗口设置的隔离级别只对当前窗口中的事务有效；对于JDBC操作数据库来说，一个Connection对象相当于一个链接，而对于Connection对象设置的隔离级别只对该Connection对象有效，与其他链接Connection对象无关。 参考博客：http://www.zhihu.com/question/23989904http://dev.mysql.com/doc/refman/5.6/en/set-transaction.htmlhttp://www.cnblogs.com/xdp-gacl/p/3984001.htmlhttps://www.cnblogs.com/fjdingsd/p/5273008.html","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"mysql性能检测","date":"2019-08-31T15:26:45.000Z","path":"2019/08/31/mysql/mysql性能检测/","text":"性能检测蝉蛹命令 show status show processlist show variables 瓶颈分析常用命令获取mysql用户下的进程总数1ps -ef | awk '&#123;print $1&#125;' | grep \"mysql\" | grep -v \"grep\" | wc -l 主机性能状态1uptime CPU使用率12topvmstat 磁盘IO量12vmstatiostat swap进出量1free -m 数据库性能状态QPS方法一 基于 questions 计算qps,基于 com_commit com_rollback 计算tps 12345questions = show global status like 'questions';uptime = show global status like 'uptime';qps=questions/uptime 1234567com_commit = show global status like 'com_commit';com_rollback = show global status like 'com_rollback';uptime = show global status like 'uptime';tps=(com_commit + com_rollback)/uptime 方法二 基于 com_* 的status 变量计算tps ,qps 使用如下命令： 1234567891011show global status where variable_name in('com_select','com_insert','com_delete','com_update');获取间隔1s 的 com_*的值，并作差值运算del_diff = (int(mystat2['com_delete']) - int(mystat1['com_delete']) ) / diffins_diff = (int(mystat2['com_insert']) - int(mystat1['com_insert']) ) / diffsel_diff = (int(mystat2['com_select']) - int(mystat1['com_select']) ) / diffupd_diff = (int(mystat2['com_update']) - int(mystat1['com_update']) ) / diff 总结： Questions 是记录了从mysqld启动以来所有的select，dml 次数包括show 命令的查询的次数。这样多少有失准确性，比如很多数据库有监控系统在运行，每5秒对数据库进行一次show 查询来获取当前数据库的状态，而这些查询就被记录到QPS,TPS统计中，造成一定的”数据污染”. 如果数据库中存在比较多的myisam表，则计算还是questions 比较合适。 如果数据库中存在比较多的innodb表，则计算以com_*数据来源比较合适 TPSTPS = (Com_commit + Com_rollback) / seconds 12show status like 'Com_commit'; show status like 'Com_rollback'; key Buffer 命中率key_buffer_read_hits = (1-key_reads / key_read_requests) 100%key_buffer_write_hits = (1-key_writes / key_write_requests) 100% 1show status like 'Key%'; InnoDB Buffer命中率innodb_buffer_read_hits = (1 - innodb_buffer_pool_reads / innodb_buffer_pool_read_requests) * 100% 1show status like 'innodb_buffer_pool_read%'; Query Cache命中率Query_cache_hits = (Qcahce_hits / (Qcache_hits + Qcache_inserts )) * 100%; 1show status like 'Qcache%'; Table Cache状态量1show status like 'open%'; Thread Cache 命中率Thread_cache_hits = (1 - Threads_created / connections ) * 100% 12show status like 'Thread%';show status like 'Connections'; 锁定状态1show status like '%lock%'; 复制延时量1show slave status; Tmp Table 状况(临时表状况)1show status like 'Create_tmp%'; Binlog Cache 使用状况1show status like 'Binlog_cache%'; Innodb_log_waits1show status like 'innodb_log_waits'; 参考https://blog.csdn.net/li_adou/article/details/78791972","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"hibernate_Embedded和@Embeddable","date":"2019-08-10T02:57:59.000Z","path":"2019/08/10/java/hibernate-@Embedded和-Embeddable/","text":"在使用实体类生成对应的数据库表时，很多的时候都会遇到这种情况：在一个实体类中引用另外的实体类，一般遇上这种情况，我们使用@OneToOne、@OneToMany、@ManyToOne、@ManyToMany这4个注解比较多，但是好奇害死猫，除了这四个有没有别的使用情况，尤其是一个实体类要在多个不同的实体类中进行使用，而本身又不需要独立生成一个数据库表，这就是需要@Embedded、@Embeddable的时候了，下面分成4类来说明在一个实体类中引用另外的实体类的情况，具体的数据库环境是MySQL 5.7。 使用的两个实体类如下： Address类123456789public class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; private String country; private String province; private String city; private String detail; //setter、getter&#125; Person类：123456789101112131415161718@Entitypublic class Person implements Serializable&#123; private static final long serialVersionUID = 8849870114127659929L; @Id @GeneratedValue private Long id; @Column(nullable = false) private String name; @Column(nullable = false) private Integer age; private Address address; //setter、getter&#125; 两个注解全不使用当这两个注解都不使用时，那么两个实体类和上面的相同，那么生成的表结构如下： Address属性字段会映射成tinyblob类型的字段，这是用来存储不超过255字符的二进制字符串的数据类型，显然我们通常不会这么使用。 只使用@Embeddable我们在Address实体类上加上@Embeddable注解，变成如下类： 1234567891011@Embeddablepublic class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; private String country; private String province; private String city; private String detail; //setter、getter&#125; 而Person实体类不变，生成的数据库表结构如下： 可以看出这次是把Address中的字段映射成数据库列嵌入到Person表中了，而这些字段的类型和长度也使用默认值。如果我们在Address中的字段中设置列的相关属性，则会按照我们设定的值去生成，如下Address类：1234567891011121314@Embeddablepublic class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; @Column(nullable = false) private String country; @Column(length = 30) private String province; @Column(unique = true) private String city; @Column(length = 50) private String detail; //setter、getter&#125; 生成的表结构如下： 我们在Address中配置的属性全部成功映射到Person表中。 只使用@Embedded这里我们只在Person中使用@Embedded,如下：12345678910111213141516171819@Entitypublic class Person implements Serializable&#123; private static final long serialVersionUID = 8849870114127659929L; @Id @GeneratedValue private Long id; @Column(nullable = false) private String name; @Column(nullable = false) private Integer age; @Embedded private Address address; //setter、getter&#125; Adddress类和最开始的不同POJO类相同，此时生成的表结构如下： 可以看出这个表结构和在Address中只使用@Embeddable注解时相同，在进入深一步试验，我们在Address中加入列属性，但是不使用@Embeddable注解会发生什么？Address类如下：12345678910111213public class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; @Column(nullable = false) private String country; @Column(length = 30) private String province; @Column(unique = true) private String city; @Column(length = 50) private String detail; //setter、getter&#125; 生成数据表结构如下： 所以只使用@Embedded和只使用@Embeddable产生的效果是相同的。 两个注解全使用既然单独使用@Embedded或者只使用@Embeddable都会产生作用，那么这两个都使用效果也一定是一样的，我们平时也是这么用的。所以在这部分我们就不演示和上面相同的效果了，而是说两个深入的话题。 覆盖@Embeddable类中字段的列属性这里就要使用另外的两个注解@AttributeOverrides和@AttributeOverride，这两个注解是用来覆盖@Embeddable类中字段的属性的。 @AttributeOverrides：里面只包含了@AttributeOverride类型数组；@AttributeOverride：包含要覆盖的@Embeddable类中字段名name和新增的@Column字段的属性；使用如下：Person类如下：123456789101112131415161718192021@Entitypublic class Person implements Serializable&#123; private static final long serialVersionUID = 8849870114127659929L; @Id @GeneratedValue private Long id; @Column(nullable = false) private String name; @Column(nullable = false) private Integer age; @Embedded @AttributeOverrides(&#123;@AttributeOverride(name=\"country\", column=@Column(name = \"person_country\", length = 25, nullable = false)), @AttributeOverride(name=\"city\", column = @Column(name = \"person_city\", length = 15))&#125;) private Address address; //setter、getter&#125; Address类如下：1234567891011121314@Embeddablepublic class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; @Column(nullable = false) private String country; @Column(length = 30) private String province; @Column(unique = true) private String city; @Column(length = 50) private String detail; //setter、getter&#125; 生成的数据表如下： 可以看出我们的@AttributeOverrides和@AttributeOverride两个注解起作用了。 多层嵌入实体类属性上面所有的例子都是使用两层实体类嵌入，其实这种实体类的嵌入映射是可以使用多层的，具体的例子如下。我们新建立一个类Direction表示方位如下：1234567@Embeddablepublic class Direction implements Serializable&#123; @Column(nullable = false) private Integer longitude; private Integer latitude;&#125; Address如下：12345678910111213141516@Embeddablepublic class Address implements Serializable&#123; private static final long serialVersionUID = 8849870114128959929L; @Column(nullable = false) private String country; @Column(length = 30) private String province; @Column(unique = true) private String city; @Column(length = 50) private String detail; @Embedded private Direction direction;&#125; Person类如下：12345678910111213141516171819@Entitypublic class Person implements Serializable&#123; private static final long serialVersionUID = 8849870114127659929L; @Id @GeneratedValue private Long id; @Column(nullable = false) private String name; @Column(nullable = false) private Integer age; @Embedded @AttributeOverrides(&#123;@AttributeOverride(name=\"direction.latitude\", column=@Column(name = \"person_latitude\")), @AttributeOverride(name=\"direction.longitude\", column = @Column(name = \"person_longitude\"))&#125;) private Address address;&#125; 生成的数据表如下： 在上面需要注意如下几点：在Person中定义Direction中的属性时，需要用”.”将所有相关的属性连接起来；在Direction中longitude属性定义为not null，但是由于使用了@AttributeOverride注解，其中虽然没有定义null属性，但是这时使用的是默认的nullable属性，默认为true; 参考 https://blog.csdn.net/lmy86263/article/details/52108130","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"create-react-app脚手架","date":"2019-08-07T01:38:30.000Z","path":"2019/08/07/react/create-react-app脚手架/","text":"安装12345npm install -g create-react-app# 切记项目名称不能大写create-react-app firstappcd firstappnpm run start eject这是一次性的操作 1npm run eject 启动less或者sasssasscreate-react-app默认有sass的配置，只需要安装依赖就行 1npm install node-sass --save less默认没有less的配置，需要自己在webpack中配置 安装依赖 1npm install less less-loader --save 运行完成之后，打开 config 目录下的 webpack.config.js 文件，找到 // style files regexes 注释位置，仿照其解析 sass 的规则，在下面添加两行代码 1234// 添加 less 解析规则const lessRegex = /\\.less$/;const lessModuleRegex = /\\.module\\.less$/;复制代码 找到 rules 属性配置，在其中添加 less 解析配置 !!!注意： 这里有一个需要注意的地方，下面的这些 less 配置规则放在 sass 的解析规则下面即可，如果放在了 file-loader 的解析规则下面，less 文件解析不会生效。 12345678910111213141516171819202122232425// Less 解析配置&#123; test: lessRegex, exclude: lessModuleRegex, use: getStyleLoaders( &#123; importLoaders: 2, sourceMap: isEnvProduction &amp;&amp; shouldUseSourceMap, &#125;, 'less-loader' ), sideEffects: true,&#125;,&#123; test: lessModuleRegex, use: getStyleLoaders( &#123; importLoaders: 2, sourceMap: isEnvProduction &amp;&amp; shouldUseSourceMap, modules: true, getLocalIdent: getCSSModuleLocalIdent, &#125;, 'less-loader' )&#125;, css module在css的命名中使用*.module.css就可以使用css module，也可以自己修改webpack的文件。 参考 https://www.jianshu.com/p/1f054623ecac","tags":[{"name":"react","slug":"react","permalink":"http://wumuwumu.github.io/tags/react/"}]},{"title":"python-类","date":"2019-08-04T10:12:22.000Z","path":"2019/08/04/python/python-类/","text":"类中默认函数new和init区别new:创建对象时调用，会返回当前对象的一个实例 init:创建完对象后调用，对当前对象的一些实例初始化，无返回值 1、在类中，如果new和init同时存在，会优先调用new 12345678class Data(object): def __new__(self): print \"new\" def __init__(self): print \"init\" data = Data()# new 2、new方法会返回所构造的对象，init则不会。init无返回值。 12345678910111213class Data(object): def __init__(cls): cls.x = 2 print \"init\" return clsdata = Data()'''initTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: __init__() should return None, not 'Data'''' 12345678910111213141516class Data(object): def __new__(cls): print(\"new\") cls.x = 1 return clsdef __init__(self): print(\"init\")data = Data()print(data.x)# new# 1data.x =2print(data.x)# 2 If new() returns an instance of cls, then the new instance’s init() method will be invoked like init(self[, …]), where self is the new instance and the remaining arguments are the same as were passed to new(). 如果new返回一个对象的实例，会隐式调用init If new() does not return an instance of cls, then the new instance’s init() method will not be invoked. 如果new不返回一个对象的实例，init不会被调用 123456789101112131415161718192021class A(object): def __new__(Class): object = super(A,Class).__new__(Class) print \"in New\" return object def __init__(self): print \"in init\" A()# in New# in initclass A(object): def __new__(cls): print \"in New\" return cls def __init__(self): print \"in init\" a = A() # in New object.init(self[, …])Called when the instance is created. The arguments are those passed to the class constructor expression. If a base class has an init() method, the derived class’s init() method, if any, must explicitly call it to ensure proper initialization of the base class part of the instance; for example: BaseClass.init(self, [args…]). As a special constraint on constructors, no value may be returned; doing so will cause a TypeError to be raised at runtime. 在对象的实例创建完成后调用。参数被传给类的构造函数。如果基类有init方法，子类必须显示调用基类的init。 没有返回值，否则会再引发TypeError错误。","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"spring的jms事务","date":"2019-08-03T07:05:17.000Z","path":"2019/08/03/java/spring的jms事务/","text":"","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"},{"name":"spring","slug":"spring","permalink":"http://wumuwumu.github.io/tags/spring/"}]},{"title":"spring事务实现","date":"2019-08-03T06:40:33.000Z","path":"2019/08/03/java/spring事务实现/","text":"事务概念回顾 什么是事务？ 事务是逻辑上的一组操作，要么都执行，要么都不执行. 事物的特性（ACID）： 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致； 隔离性： 并发访问数据库时，一个用户的事物不被其他事物所干扰，各并发事务之间数据库是独立的； 持久性: 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 Spring事务管理接口介绍 Spring事务管理接口： PlatformTransactionManager： （平台）事务管理器 TransactionDefinition： 事务定义信息(事务隔离级别、传播行为、超时、只读、回滚规则) TransactionStatus： 事务运行状态 所谓事务管理，其实就是“按照给定的事务规则来执行提交或者回滚操作”。 PlatformTransactionManager接口介绍 Spring并不直接管理事务，而是提供了多种事务管理器 ，他们将事务管理的职责委托给Hibernate或者JTA等持久化机制所提供的相关平台框架的事务来实现。 Spring事务管理器的接口是： org.springframework.transaction.PlatformTransactionManager ，通过这个接口，Spring为各个平台如JDBC、Hibernate等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。 PlatformTransactionManager接口代码如下：PlatformTransactionManager接口中定义了三个方法： 123456789Public interface PlatformTransactionManager()...&#123; // Return a currently active transaction or create a new one, according to the specified propagation behavior（根据指定的传播行为，返回当前活动的事务或创建一个新事务。） TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // Commit the given transaction, with regard to its status（使用事务目前的状态提交事务） Void commit(TransactionStatus status) throws TransactionException; // Perform a rollback of the given transaction（对执行的事务进行回滚） Void rollback(TransactionStatus status) throws TransactionException; &#125; 复制代码 我们刚刚也说了Spring中PlatformTransactionManager根据不同持久层框架所对应的接口实现类,几个比较常见的如下图所示 比如我们在使用JDBC或者iBatis（就是Mybatis）进行数据持久化操作时,我们的xml配置通常如下： 1234567 &lt;!-- 事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- 数据源 --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;/bean&gt;复制代码 TransactionDefinition接口介绍 事务管理器接口 PlatformTransactionManager 通过 getTransaction(TransactionDefinition definition) 方法来得到一个事务，这个方法里面的参数是 TransactionDefinition类 ，这个类就定义了一些基本的事务属性。 那么什么是事务属性呢？ 事务属性可以理解成事务的一些基本配置，描述了事务策略如何应用到方法上。事务属性包含了5个方面。 TransactionDefinition接口中的方法如下：TransactionDefinition接口中定义了5个方法以及一些表示事务属性的常量比如隔离级别、传播行为等等的常量。 我下面只是列出了TransactionDefinition接口中的方法而没有给出接口中定义的常量，该接口中的常量信息会在后面依次介绍到。 12345678910111213public interface TransactionDefinition &#123; // 返回事务的传播行为 int getPropagationBehavior(); // 返回事务的隔离级别，事务管理器根据它来控制另外一个事务可以看到本事务内的哪些数据 int getIsolationLevel(); // 返回事务必须在多少秒内完成 //返回事务的名字 String getName()； int getTimeout(); // 返回是否优化为只读事务。 boolean isReadOnly();&#125; 复制代码 （1）事务隔离级别（定义了一个事务可能受其他并发事务影响的程度）：我们先来看一下 并发事务带来的问题 ，然后再来介绍一下 TransactionDefinition 接口 中定义了五个表示隔离级别的常量。 并发事务带来的问题 在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对统一数据进行操作）。并发虽然是必须的，但可能会导致一下的问题。 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复度和幻读区别： 不可重复读的重点是修改，幻读的重点在于新增或者删除。 例1（同样的条件, 你读取过的数据, 再次读取出来发现值不一样了 ）：事务1中的A先生读取自己的工资为 1000的操作还没完成，事务2中的B先生就修改了A的工资为2000，导 致A再读自己的工资时工资变为 2000；这就是不可重复读。 例2（同样的条件, 第1次和第2次读出来的记录数不一样 ）：假某工资单表中工资大于3000的有4人，事务1读取了所有工资大于3000的人，共查到4条记录，这时事务2 又插入了一条工资大于3000的记录，事务1再次读取时查到的记录就变为了5条，这样就导致了幻读。 隔离级别 TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 （2）事务传播行为（为了解决业务层方法之间互相调用的事务问题）：当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。在TransactionDefinition定义中包括了如下几个表示传播行为的常量： 支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 这里需要指出的是，前面的六种事务传播行为是 Spring 从 EJB 中引入的，他们共享相同的概念。而 PROPAGATION_NESTED 是 Spring 所特有的。以 PROPAGATION_NESTED 启动的事务内嵌于外部事务中（如果存在外部事务的话），此时，内嵌事务并不是一个独立的事务，它依赖于外部事务的存在，只有通过外部的事务提交，才能引起内部事务的提交，嵌套的子事务不能单独提交。如果熟悉 JDBC 中的保存点（SavePoint）的概念，那嵌套事务就很容易理解了，其实嵌套的子事务就是保存点的一个应用，一个事务中可以包括多个保存点，每一个嵌套子事务。另外，外部事务的回滚也会导致嵌套子事务的回滚。 (3) 事务超时属性(一个事务允许执行的最长时间)所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。 (4) 事务只读属性（对事物资源是否执行只读操作）事务的只读属性是指，对事务性资源进行只读操作或者是读写操作。所谓事务性资源就是指那些被事务管理的资源，比如数据源、 JMS 资源，以及自定义的事务性资源等等。如果确定只对事务性资源进行只读操作，那么我们可以将事务标志为只读的，以提高事务处理的性能。在 TransactionDefinition 中以 boolean 类型来表示该事务是否只读。 (5) 回滚规则（定义事务回滚规则）例子使用API下面给出一个基于底层 API 的编程式事务管理的示例，基于PlatformTransactionManager、TransactionDefinition 和 TransactionStatus 三个核心接口，我们完全可以通过编程的方式来进行事务管理。 1234567891011121314151617181920public class BankServiceImpl implements BankService &#123; private BankDao bankDao; private TransactionDefinition txDefinition; private PlatformTransactionManager txManager;public boolean transfer(Long fromId， Long toId， double amount) &#123; // 获取一个事务 TransactionStatus txStatus = txManager.getTransaction(txDefinition); boolean result = false; try &#123; result = bankDao.transfer(fromId， toId， amount); txManager.commit(txStatus); // 事务提交 &#125; catch (Exception e) &#123; result = false; txManager.rollback(txStatus); // 事务回滚 System.out.println(\"Transfer Error!\"); &#125; return result;&#125;相应的配置文件如下所示： 123456789&lt;bean id=\"bankService\" class=\"footmark.spring.core.tx.programmatic.origin.BankServiceImpl\"&gt; &lt;property name=\"bankDao\" ref=\"bankDao\"/&gt; &lt;property name=\"txManager\" ref=\"transactionManager\"/&gt; &lt;property name=\"txDefinition\"&gt; &lt;bean class=\"org.springframework.transaction.support.DefaultTransactionDefinition\"&gt; &lt;property name=\"propagationBehaviorName\" value=\"PROPAGATION_REQUIRED\"/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt;如上所示，我们在BankServiceImpl类中增加了两个属性：一个是 TransactionDefinition 类型的属性，它用于定义事务的规则；另一个是 PlatformTransactionManager 类型的属性，用于执行事务管理操作。如果一个业务方法需要添加事务，我们首先需要在方法开始执行前调用PlatformTransactionManager.getTransaction(…) 方法启动一个事务；创建并启动了事务之后，便可以开始编写业务逻辑代码，然后在适当的地方执行事务的提交或者回滚。 基于 TransactionTemplate 的编程式事务管理 当然，除了可以使用基于底层 API 的编程式事务外，还可以使用基于 TransactionTemplate 的编程式事务管理。通过上面的示例可以发现，上述事务管理的代码散落在业务逻辑代码中，破坏了原有代码的条理性，并且每一个业务方法都包含了类似的启动事务、提交/回滚事务的样板代码。Spring 也意识到了这些，并提供了简化的方法，这就是 Spring 在数据访问层非常常见的 模板回调模式。 1234567891011121314151617181920public class BankServiceImpl implements BankService &#123; private BankDao bankDao; private TransactionTemplate transactionTemplate; ...... public boolean transfer(final Long fromId， final Long toId， final double amount) &#123; return (Boolean) transactionTemplate.execute(new TransactionCallback()&#123; public Object doInTransaction(TransactionStatus status) &#123; Object result; try &#123; result = bankDao.transfer(fromId， toId， amount); &#125; catch (Exception e) &#123; status.setRollbackOnly(); result = false; System.out.println(\"Transfer Error!\"); &#125; return result; &#125; &#125;); &#125;&#125; 相应的配置文件如下所示： 1234&lt;bean id=\"bankService\" class=\"footmark.spring.core.tx.programmatic.template.BankServiceImpl\"&gt; &lt;property name=\"bankDao\" ref=\"bankDao\"/&gt; &lt;property name=\"transactionTemplate\" ref=\"transactionTemplate\"/&gt;&lt;/bean&gt; TransactionTemplate 的 execute() 方法有一个 TransactionCallback 类型的参数，该接口中定义了一个 doInTransaction() 方法，通常我们以匿名内部类的方式实现 TransactionCallback 接口，并在其 doInTransaction() 方法中书写业务逻辑代码。这里可以使用默认的事务提交和回滚规则，这样在业务代码中就不需要显式调用任何事务管理的 API。doInTransaction() 方法有一个TransactionStatus 类型的参数，我们可以在方法的任何位置调用该参数的 setRollbackOnly() 方法将事务标识为回滚的，以执行事务回滚。 ​ 此外，TransactionCallback 接口有一个子接口 TransactionCallbackWithoutResult，该接口中定义了一个 doInTransactionWithoutResult() 方法，TransactionCallbackWithoutResult 接口主要用于事务过程中不需要返回值的情况。当然，对于不需要返回值的情况，我们仍然可以使用 TransactionCallback 接口，并在方法中返回任意值即可。 基于底层 API 的编程式事务管理 下面给出一个基于底层 API 的编程式事务管理的示例，基于PlatformTransactionManager、TransactionDefinition 和 TransactionStatus 三个核心接口，我们完全可以通过编程的方式来进行事务管理。 12345678910111213141516171819public class BankServiceImpl implements BankService &#123; private BankDao bankDao; private TransactionDefinition txDefinition; private PlatformTransactionManager txManager; public boolean transfer(Long fromId， Long toId， double amount) &#123; // 获取一个事务 TransactionStatus txStatus = txManager.getTransaction(txDefinition); boolean result = false; try &#123; result = bankDao.transfer(fromId， toId， amount); txManager.commit(txStatus); // 事务提交 &#125; catch (Exception e) &#123; result = false; txManager.rollback(txStatus); // 事务回滚 System.out.println(\"Transfer Error!\"); &#125; return result;&#125;相应的配置文件如下所示： 123456789&lt;bean id=\"bankService\" class=\"footmark.spring.core.tx.programmatic.origin.BankServiceImpl\"&gt; &lt;property name=\"bankDao\" ref=\"bankDao\"/&gt; &lt;property name=\"txManager\" ref=\"transactionManager\"/&gt; &lt;property name=\"txDefinition\"&gt; &lt;bean class=\"org.springframework.transaction.support.DefaultTransactionDefinition\"&gt; &lt;property name=\"propagationBehaviorName\" value=\"PROPAGATION_REQUIRED\"/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 如上所示，我们在BankServiceImpl类中增加了两个属性：一个是 TransactionDefinition 类型的属性，它用于定义事务的规则；另一个是 PlatformTransactionManager 类型的属性，用于执行事务管理操作。如果一个业务方法需要添加事务，我们首先需要在方法开始执行前调用PlatformTransactionManager.getTransaction(…) 方法启动一个事务；创建并启动了事务之后，便可以开始编写业务逻辑代码，然后在适当的地方执行事务的提交或者回滚。 基于 TransactionTemplate 的编程式事务管理 当然，除了可以使用基于底层 API 的编程式事务外，还可以使用基于 TransactionTemplate 的编程式事务管理。通过上面的示例可以发现，上述事务管理的代码散落在业务逻辑代码中，破坏了原有代码的条理性，并且每一个业务方法都包含了类似的启动事务、提交/回滚事务的样板代码。Spring 也意识到了这些，并提供了简化的方法，这就是 Spring 在数据访问层非常常见的 模板回调模式。 1234567891011121314151617181920public class BankServiceImpl implements BankService &#123; private BankDao bankDao; private TransactionTemplate transactionTemplate; ...... public boolean transfer(final Long fromId， final Long toId， final double amount) &#123; return (Boolean) transactionTemplate.execute(new TransactionCallback()&#123; public Object doInTransaction(TransactionStatus status) &#123; Object result; try &#123; result = bankDao.transfer(fromId， toId， amount); &#125; catch (Exception e) &#123; status.setRollbackOnly(); result = false; System.out.println(\"Transfer Error!\"); &#125; return result; &#125; &#125;); &#125;&#125; 相应的配置文件如下所示： 1234&lt;bean id=\"bankService\" class=\"footmark.spring.core.tx.programmatic.template.BankServiceImpl\"&gt; &lt;property name=\"bankDao\" ref=\"bankDao\"/&gt; &lt;property name=\"transactionTemplate\" ref=\"transactionTemplate\"/&gt;&lt;/bean&gt; TransactionTemplate 的 execute() 方法有一个 TransactionCallback 类型的参数，该接口中定义了一个 doInTransaction() 方法，通常我们以匿名内部类的方式实现 TransactionCallback 接口，并在其 doInTransaction() 方法中书写业务逻辑代码。这里可以使用默认的事务提交和回滚规则，这样在业务代码中就不需要显式调用任何事务管理的 API。doInTransaction() 方法有一个TransactionStatus 类型的参数，我们可以在方法的任何位置调用该参数的 setRollbackOnly() 方法将事务标识为回滚的，以执行事务回滚。 此外，TransactionCallback 接口有一个子接口 TransactionCallbackWithoutResult，该接口中定义了一个 doInTransactionWithoutResult() 方法，TransactionCallbackWithoutResult 接口主要用于事务过程中不需要返回值的情况。当然，对于不需要返回值的情况，我们仍然可以使用 TransactionCallback 接口，并在方法中返回任意值即可。 Spring 声明式事务管理 Spring 的声明式事务管理是建立在 Spring AOP 机制之上的，其本质是对目标方法前后进行拦截，并在目标方法开始之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。 声明式事务最大的优点就是不需要通过编程的方式管理事务，这样就不需要在业务逻辑代码中掺杂事务管理的代码，只需在配置文件中作相关的事务规则声明（或通过等价的基于标注的方式），便可以将事务规则应用到业务逻辑中。总的来说，声明式事务得益于 Spring IoC容器 和 Spring AOP 机制的支持：IoC容器为声明式事务管理提供了基础设施，使得 Bean 对于 Spring 框架而言是可管理的；而由于事务管理本身就是一个典型的横切逻辑（正是 AOP 的用武之地），因此 Spring AOP 机制是声明式事务管理的直接实现者。 显然，声明式事务管理要优于编程式事务管理，这正是spring倡导的非侵入式的开发方式。声明式事务管理使业务代码不受污染，一个普通的POJO对象，只要在XML文件中配置或者添加注解就可以获得完全的事务支持。因此，通常情况下，笔者强烈建议在开发中使用声明式事务，不仅因为其简单，更主要是因为这样使得纯业务代码不被污染，极大方便后期的代码维护。 基于 命名空间的声明式事务管理 Spring 2.x 引入了 命名空间，结合使用 命名空间，带给开发人员配置声明式事务的全新体验，配置变得更加简单和灵活。总的来说，开发者只需基于和命名空间在XML中进行简答配置便可实现声明式事务管理。下面基于使用Hibernate事务管理的配置文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;!-- 配置 DataSourece --&gt;&lt;bean id=\"dataSource\" class=\"org.apache.commons.dbcp.BasicDataSource\" destroy-method=\"close\"&gt; &lt;!-- results in a setDriverClassName(String) call --&gt; &lt;property name=\"driverClassName\"&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property name=\"url\"&gt; &lt;value&gt;jdbc:mysql://localhost:3306/ssh&lt;/value&gt; &lt;/property&gt; &lt;property name=\"username\"&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property name=\"password\"&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置 sessionFactory --&gt;&lt;bean id=\"sessionFactory\" class=\"org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean\"&gt; &lt;!-- 数据源的设置 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;!-- 用于持久化的实体类类列表 --&gt; &lt;property name=\"annotatedClasses\"&gt; &lt;list&gt; &lt;value&gt;cn.edu.tju.rico.model.entity.User&lt;/value&gt; &lt;value&gt;cn.edu.tju.rico.model.entity.Log&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- Hibernate 的配置 --&gt; &lt;property name=\"hibernateProperties\"&gt; &lt;props&gt; &lt;!-- 方言设置 --&gt; &lt;prop key=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;!-- 显示sql --&gt; &lt;prop key=\"hibernate.show_sql\"&gt;true&lt;/prop&gt; &lt;!-- 格式化sql --&gt; &lt;prop key=\"hibernate.format_sql\"&gt;true&lt;/prop&gt; &lt;!-- 自动创建/更新数据表 --&gt; &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;update&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置 TransactionManager --&gt;&lt;bean id=\"txManager\" class=\"org.springframework.orm.hibernate3.HibernateTransactionManager\"&gt; &lt;property name=\"sessionFactory\" ref=\"sessionFactory\" /&gt;&lt;/bean&gt;&lt;!-- 配置事务增强处理的切入点，以保证其被恰当的织入 --&gt; &lt;aop:config&gt; &lt;!-- 切点 --&gt; &lt;aop:pointcut expression=\"execution(* cn.edu.tju.rico.service.impl.*.*(..))\" id=\"bussinessService\" /&gt; &lt;!-- 声明式事务的切入 --&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"bussinessService\" /&gt;&lt;/aop:config&gt;&lt;!-- 由txAdvice切面定义事务增强处理 --&gt;&lt;tx:advice id=\"txAdvice\" transaction-manager=\"txManager\"&gt; &lt;tx:attributes&gt; &lt;!-- get打头的方法为只读方法,因此将read-only设为 true --&gt; &lt;tx:method name=\"get*\" read-only=\"true\" /&gt; &lt;!-- 其他方法为读写方法,因此将read-only设为 false --&gt; &lt;tx:method name=\"*\" read-only=\"false\" propagation=\"REQUIRED\" isolation=\"DEFAULT\" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 事实上，Spring配置文件中关于事务的配置总是由三个部分组成，即：DataSource、TransactionManager和代理机制三部分，无论哪种配置方式，一般变化的只是代理机制这部分。其中，DataSource、TransactionManager这两部分只是会根据数据访问方式有所变化，比如使用hibernate进行数据访问时，DataSource实际为SessionFactory，TransactionManager的实现为 HibernateTransactionManager。如下图所示： 基于 @Transactional 的声明式事务管理 除了基于命名空间的事务配置方式，Spring 还引入了基于 Annotation 的方式，具体主要涉及@Transactional 标注。@Transactional 可以作用于接口、接口方法、类以及类方法上：当作用于类上时，该类的所有 public 方法将都具有该类型的事务属性；当作用于方法上时，该标注来覆盖类级别的定义。如下所示： 1234@Transactional(propagation = Propagation.REQUIRED)public boolean transfer(Long fromId， Long toId， double amount) &#123; return bankDao.transfer(fromId， toId， amount);&#125; Spring 使用 BeanPostProcessor 来处理 Bean 中的标注，因此我们需要在配置文件中作如下声明来激活该后处理 Bean，如下所示： 1&lt;tx:annotation-driven transaction-manager=\"transactionManager”/&gt; 1 与前面相似，transaction-manager、datasource 和 sessionFactory的配置不变，只需将基于和命名空间的配置更换为上述配置即可。 Spring 声明式事务的本质 就Spring 声明式事务而言，无论其基于 命名空间的实现还是基于 @Transactional 的实现，其本质都是 Spring AOP 机制的应用：即通过以@Transactional的方式或者XML配置文件的方式向业务组件中的目标业务方法插入事务增强处理并生成相应的代理对象供应用程序(客户端)使用从而达到无污染地添加事务的目的。如下图所示： 参考https://juejin.im/post/5b00c52ef265da0b95276091 https://blog.csdn.net/justloveyou_/article/details/73733278","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"},{"name":"spring","slug":"spring","permalink":"http://wumuwumu.github.io/tags/spring/"}]},{"title":"python-装饰器","date":"2019-07-31T11:44:57.000Z","path":"2019/07/31/python/python-装饰器/","text":"简单的装饰器12345678910111213141516171819import loggingdef use_logging(func): def wrapper(): logging.warning(\"%s is running\" % func.__name__) return func() # 把 foo 当做参数传递进来时，执行func()就相当于执行foo() return wrapperdef foo(): print('i am foo')foo = use_logging(foo) # 因为装饰器 use_logging(foo) 返回的时函数对象 wrapper，这条语句相当于 foo = wrapperfoo() # 执行foo()就相当于执行 wrapper()'''WARNING:root:foo is runningi am foo''' @ 语法糖123456789101112def use_logging(func): def wrapper(): logging.warn(\"%s is running\" % func.__name__) return func() return wrapper@use_loggingdef foo(): print(\"i am foo\")foo() *args、**kwargs可能有人问，如果我的业务逻辑函数 foo 需要参数怎么办？比如： 12def foo(name): print(&quot;i am %s&quot; % name) 我们可以在定义 wrapper 函数的时候指定参数： 1234def wrapper(name): logging.warn(\"%s is running\" % func.__name__) return func(name) return wrapper 这样 foo 函数定义的参数就可以定义在 wrapper 函数中。这时，又有人要问了，如果 foo 函数接收两个参数呢？三个参数呢？更有甚者，我可能传很多个。当装饰器不知道 foo 到底有多少个参数时，我们可以用 *args 来代替： 1234def wrapper(*args): logging.warn(\"%s is running\" % func.__name__) return func(*args) return wrapper 如此一来，甭管 foo 定义了多少个参数，我都可以完整地传递到 func 中去。这样就不影响 foo 的业务逻辑了。这时还有读者会问，如果 foo 函数还定义了一些关键字参数呢？比如： 12def foo(name, age=None, height=None): print(\"I am %s, age %s, height %s\" % (name, age, height)) 这时，你就可以把 wrapper 函数指定关键字函数： 12345def wrapper(*args, **kwargs): # args是一个数组，kwargs一个字典 logging.warn(\"%s is running\" % func.__name__) return func(*args, **kwargs) return wrapper 带参数的装饰器装饰器还有更大的灵活性，例如带参数的装饰器，在上面的装饰器调用中，该装饰器接收唯一的参数就是执行业务的函数 foo 。装饰器的语法允许我们在调用时，提供其它参数，比如@decorator(a)。这样，就为装饰器的编写和使用提供了更大的灵活性。比如，我们可以在装饰器中指定日志的等级，因为不同业务函数可能需要的日志级别是不一样的。 1234567891011121314151617def use_logging(level): def decorator(func): def wrapper(*args, **kwargs): if level == \"warn\": logging.warn(\"%s is running\" % func.__name__) elif level == \"info\": logging.info(\"%s is running\" % func.__name__) return func(*args) return wrapper return decorator@use_logging(level=\"warn\")def foo(name='foo'): print(\"i am %s\" % name)foo() 上面的 use_logging 是允许带参数的装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有参数的闭包。当我 们使用@use_logging(level=&quot;warn&quot;)调用的时候，Python 能够发现这一层的封装，并把参数传递到装饰器的环境中。 1@use_logging(level=\"warn\")`等价于`@decorator 类装饰器没错，装饰器不仅可以是函数，还可以是类，相比函数装饰器，类装饰器具有灵活度大、高内聚、封装性等优点。使用类装饰器主要依靠类的__call__方法，当使用 @ 形式将装饰器附加到函数上时，就会调用此方法。 1234567891011121314class Foo(object): def __init__(self, func): self._func = func def __call__(self): print ('class decorator runing') self._func() print ('class decorator ending')@Foodef bar(): print ('bar')bar() functools.wraps使用装饰器极大地复用了代码，但是他有一个缺点就是原函数的元信息不见了，比如函数的docstring、__name__、参数列表，先看例子： 123456789101112131415# 装饰器def logged(func): def with_logging(*args, **kwargs): print func.__name__ # 输出 'with_logging' print func.__doc__ # 输出 None return func(*args, **kwargs) return with_logging# 函数@loggeddef f(x): \"\"\"does some math\"\"\" return x + x * xlogged(f) 不难发现，函数 f 被with_logging取代了，当然它的docstring，__name__就是变成了with_logging函数的信息了。好在我们有functools.wraps，wraps本身也是一个装饰器，它能把原函数的元信息拷贝到装饰器里面的 func 函数中，这使得装饰器里面的 func 函数也有和原函数 foo 一样的元信息了。 12345678910111213from functools import wrapsdef logged(func): @wraps(func) def with_logging(*args, **kwargs): print func.__name__ # 输出 'f' print func.__doc__ # 输出 'does some math' return func(*args, **kwargs) return with_logging@loggeddef f(x): \"\"\"does some math\"\"\" return x + x * x 装饰器顺序一个函数还可以同时定义多个装饰器，比如： 12345@a@b@cdef f (): pass 它的执行顺序是从里到外，最先调用最里层的装饰器，最后调用最外层的装饰器，它等效于 1f = a(b(c(f))) 补充*与**区别在Python的函数定义中使用args和**kwargs可传递可变参数。args用作传递非命名键值可变长参数列表（位置参数），**kwargs用作传递键值可变长参数列表。在函数调用的时候也有解构的使用 1234567891011def test_var_args(farg, *args): print \"formal arg:\", farg for arg in args: print \"another arg:\", arg test_var_args(1, \"two\", 3)'''formal arg: 1another arg: twoanother arg: 3''' 1234567891011121314def test_var_kwargs(farg, **kwargs): print \"formal arg:\", farg for key in kwargs: print \"another keyword arg: %s: %s\" % (key, kwargs[key]) test_var_kwargs(farg=1, myarg2=\"two\", myarg3=3)'''Required argument: 1Optional argument (*args): 2Optional argument (*args): 3Optional argument (*args): 4Optional argument k2 (*kwargs): 6Optional argument k1 (*kwargs): 5''' 1234567def test_var_args_call(arg1, arg2, arg3): print \"arg1:\", arg1 print \"arg2:\", arg2 print \"arg3:\", arg3 args = (\"two\", 3)test_var_args_call(1, *args) 1234567def test_var_args_call(arg1, arg2, arg3): print \"arg1:\", arg1 print \"arg2:\", arg2 print \"arg3:\", arg3 kwargs = &#123;\"arg3\": 3, \"arg2\": \"two\"&#125;test_var_args_call(1, **kwargs) 参考 https://foofish.net/python-decorator.html https://www.biaodianfu.com/python-args-kwargs.html https://my.oschina.net/leejun2005/blog/477614 例子介绍的很详细","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"python-字符串格式","date":"2019-07-31T02:59:16.000Z","path":"2019/07/31/python/python-字符串格式/","text":"格式化操作符（%）“%”是Python风格的字符串格式化操作符，非常类似C语言里的printf()函数的字符串格式化（C语言中也是使用%）。 下面整理了一下Python中字符串格式化符合： 格式化符号 说明 %c 转换成字符（ASCII 码值，或者长度为一的字符串） %r 优先用repr()函数进行字符串转换 %s 优先用str()函数进行字符串转换 %d / %i 转成有符号十进制数 %u 转成无符号十进制数 %o 转成无符号八进制数 %x / %X 转成无符号十六进制数（x / X 代表转换后的十六进制字符的大小写） %e / %E 转成科学计数法（e / E控制输出e / E） %f / %F 转成浮点数（小数部分自然截断） %g / %G %e和%f / %E和%F 的简写 %% 输出% （格式化字符串里面包括百分号，那么必须使用%%） 这里列出的格式化符合都比较简单，唯一想要强调一下的就是”%s”和”%r”的差别。 看个简单的代码： 123456789string = \"Hello\\tWill\\n\"print(\"%s\" %string)print(\"%r\" %string)'''Hello Will'Hello\\tWill\\n'''' 补充： Python打印值的时候会保持该值在Python代码中的状态，不是用户所希望看到的状态。而使用print打印值则不一样，print打印出来的值是用户所希望看到的状态。 str和repr的区别： str 把值转换为合理形式的字符串，给用户看的。str实际上类似于int，long，是一种类型。 12345678print str(\"Hello, world!\")# Hello, world! print str(1000L)# 1000 str(\"Hello, world!\")# 'Hello, world!' # 字符串转换之后仍然是字符串str(1000L)# '1000' repr() 创建一个字符串，以合法python表达式的形式来表示值。repr()是一个函数。 12345678print repr(\"Hello, world!\")# 'Hello, world!'print repr(1000L)# 1000Lrepr(\"Hello, world!\")# \"'Hello, world!'\"repr(1000L)# '1000L' 格式化操作辅助符通过”%”可以进行字符串格式化，但是”%”经常会结合下面的辅助符一起使用。 辅助符号 说明 * 定义宽度或者小数点精度 - 用做左对齐 + 在正数前面显示加号(+) # 在八进制数前面显示零(0)，在十六进制前面显示”0x”或者”0X”（取决于用的是”x”还是”X”） 0 显示的数字前面填充”0”而不是默认的空格 (var) 映射变量（通常用来处理字段类型的参数） m.n m 是显示的最小总宽度，n 是小数点后的位数（如果可用的话） 12345678910111213141516171819202122232425262728293031323334num = 100print(\"%d to hex is %x\" %(num, num))print(\"%d to hex is %X\" %(num, num))print(\"%d to hex is %#x\" %(num, num))print(\"%d to hex is %#X\" %(num, num))# 浮点数f = 3.1415926print(\"value of f is: %.4f\" %f)# 指定宽度和对齐students = [&#123;\"name\":\"Wilber\", \"age\":27&#125;, &#123;\"name\":\"Will\", \"age\":28&#125;, &#123;\"name\":\"June\", \"age\":27&#125;]print(\"name: %10s, age: %10d\" %(students[0][\"name\"], students[0][\"age\"]))print(\"name: %-10s, age: %-10d\" %(students[1][\"name\"], students[1][\"age\"]))print(\"name: %*s, age: %0*d\" %(10, students[2][\"name\"], 10, students[2][\"age\"]))# dict参数for student in students: print(\"%(name)s is %(age)d years old\" %student) '''100 to hex is 64100 to hex is 64100 to hex is 0x64100 to hex is 0X64value of f is: 3.1416name: Wilber, age: 27name: Will , age: 28 name: June, age: 0000000027Wilber is 27 years oldWill is 28 years oldJune is 27 years old''' 字符串模板其实，在Python中进行字符串的格式化，除了格式化操作符，还可以使用string模块中的字符串模板（Template）对象。下面就主要看看Template对象的substitute()方法： 123456from string import TemplatesTemp = Template('Hi ,$name,$$ ')print(sTemp.substitute(name='wumu'))'''Hi ,wumu,$ ''' format1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 位置参数print(\"&#123;&#125; is &#123;&#125; years old\".format(\"Wilber\", 28))print(\"Hi, &#123;0&#125;! &#123;0&#125; is &#123;1&#125; years old\".format(\"Wilber\", 28))# 关键字参数print(\"&#123;name&#125; is &#123;age&#125; years old\".format(name = \"Wilber\", age = 28))# 下标参数li = [\"Wilber\", 28]print(\"&#123;0[0]&#125; is &#123;0[1]&#125; years old\".format(li))# 填充与对齐# ^、&lt;、&gt;分别是居中、左对齐、右对齐，后面带宽度# :号后面带填充的字符，只能是一个字符，不指定的话默认是用空格填充print('&#123;:&gt;8&#125;'.format('3.14'))print('&#123;:&lt;8&#125;'.format('3.14'))print('&#123;:^8&#125;'.format('3.14'))print('&#123;:0&gt;8&#125;'.format('3.14'))print('&#123;:a&gt;8&#125;'.format('3.14'))# 浮点数精度print('&#123;:.4f&#125;'.format(3.1415926))print('&#123;:0&gt;10.4f&#125;'.format(3.1415926))# 进制# b、d、o、x分别是二进制、十进制、八进制、十六进制print('&#123;:b&#125;'.format(11))print('&#123;:d&#125;'.format(11))print('&#123;:o&#125;'.format(11))print('&#123;:x&#125;'.format(11))print('&#123;:#x&#125;'.format(11))print('&#123;:#X&#125;'.format(11))# 千位分隔符print('&#123;:,&#125;'.format(15700000000))'''Wilber is 28 years oldHi, Wilber! Wilber is 28 years oldWilber is 28 years oldWilber is 28 years old 3.143.14 3.14 00003.14aaaa3.143.141600003.141610111113b0xb0XB15,700,000,000''' 参考 https://www.cnblogs.com/wilber2013/p/4641616.html","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"mysql自带的数据库","date":"2019-07-27T10:06:05.000Z","path":"2019/07/27/mysql/mysql自带的数据库/","text":"information_schema SCHEMATA表：提供了当前mysql实例中所有数据库的信息。是show databases的结果取之此表。 TABLES表：提供了关于数据库中的表的信息（包括视图）。详细表述了某个表属于哪个schema，表类型，表引擎，创建时间等信息。是show tables from schemaname的 结果取之此表。 COLUMNS表：提供了表中的列信息。详细表述了某张表的所有列以及每个列的信息。是show columns from schemaname.tablename的结果取之此表。 STATISTICS表：提供了关于表索引的信息。是show index from schemaname.tablename的结果取之此表。 USER_PRIVILEGES（用户权限）表：给出了关于全程权限的信息。该信息源自mysql.user授权表。是非标准表。 SCHEMA_PRIVILEGES（方案权限）表：给出了关于方案（数据库）权限的信息。该信息来自mysql.db授权表。是非标准表。 TABLE_PRIVILEGES（表权限）表：给出了关于表权限的信息。该信息源自mysql.tables_priv授权表。是非标准表。 COLUMN_PRIVILEGES（列权限）表：给出了关于列权限的信息。该信息源自mysql.columns_priv授权表。是非标准表。 CHARACTER_SETS（字符集）表：提供了mysql实例可用字符集的信息。是SHOW CHARACTER SET结果集取之此表。 COLLATIONS表：提供了关于各字符集的对照信息。 COLLATION_CHARACTER_SET_APPLICABILITY表：指明了可用于校对的字符集。这些列等效于SHOW COLLATION的前两个显示字段。 TABLE_CONSTRAINTS表：描述了存在约束的表。以及表的约束类型。 KEY_COLUMN_USAGE表：描述了具有约束的键列。 ROUTINES表：提供了关于存储子程序（存储程序和函数）的信息。此时，ROUTINES表不包含自定义函数（UDF）。名为“mysql.proc name”的列指明了对应于 INFORMATION_SCHEMA.ROUTINES表的mysql.proc表列。 VIEWS表：给出了关于数据库中的视图的信息。需要有show views权限，否则无法查看视图信息。 TRIGGERS表：提供了关于触发程序的信息。必须有super权限才能查看该表。 mysqlperformance_schema 需要设置参数： performance_schema 才可以启动该功能 按照相关的标准对进行的事件统计表, 表也是只读的，只能turcate events_waits_summary_by_instance events_waits_summary_by_thread_by_event_name events_waits_summary_global_by_event_name file_summary_by_event_name file_summary_by_instance setup_consumers 描述各种事件 setup_instruments 描述这个数据库下的表名以及是否开启监控。 setup_timers 描述 监控选项已经采样频率的时间间隔 events_waits_current 记录当前正在发生的等待事件，这个表是只读的表，不能update ，delete ，但是可以truncate 性能历史表 ：events_waits_history 只保留每个线程（thread） 的最近的10个事件 性能历史表 ：events_waits_history_long 记录最近的10000个事件 标准的先进先出（FIFO) 这俩表也是只读表，只能truncate sakila 这是一个MySQL的一个样本数据库，里边都是一些例子表。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"mysql修改字符集","date":"2019-07-27T08:55:17.000Z","path":"2019/07/27/mysql/mysql修改字符集/","text":"概念 字符集（character set）：定义了字符以及字符的编码。 字符序（collation）：定义了字符的比较规则。 Mysql字符集 一个字符集对应至少一种字符序（一般是1对多）。 两个不同的字符集不能有相同的字符序。 每个字符集都有默认的字符序。 12345678910-- 第一种方式SHOW CHARACTER SET;-- 第二种方式use information_schema;select * from CHARACTER_SETS;-- 例子SHOW CHARACTER SET WHERE Charset=&quot;utf8&quot;;SHOW CHARACTER SET LIKE &quot;utf8%&quot;; Mysql字符序123456-- 第一种方式SHOW COLLATION WHERE Charset = &apos;utf8&apos;;-- 第二种方式USE information_schema;SELECT * FROM COLLATIONS WHERE CHARACTER_SET_NAME=&quot;utf8&quot;; 命名规范字符序的命名，以其对应的字符集作为前缀，如下所示。比如字符序utf8_general_ci，标明它是字符集utf8的字符序。 更多规则可以参考 官方文档。 1[information_schema]&gt; SELECT CHARACTER_SET_NAME, COLLATION_NAME FROM COLLATIONS WHERE CHARACTER_SET_NAME=&quot;utf8&quot; limit 2; 设置修改 修改数据库字符集 1234ALTER DATABASE db_name DEFAULT CHARACTER SET character_name [COLLATE ...];把表默认的字符集和所有字符列（CHAR,VARCHAR,TEXT）改为新的字符集：ALTER TABLE tbl_name CONVERT TO CHARACTER SET character_name [COLLATE ...]如：ALTER TABLE logtest CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci; 修改表的默认字符集 12ALTER TABLE tbl_name DEFAULT CHARACTER SET character_name [COLLATE...];如：ALTER TABLE logtest DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 修改字段的字符集 12ALTER TABLE tbl_name CHANGE c_name c_name CHARACTER SET character_name [COLLATE ...];如：ALTER TABLE logtest CHANGE title title VARCHAR(100) CHARACTER SET utf8 COLLATE utf8_general_ci; 查看数据库编码 1SHOW CREATE DATABASE db_name; 查看表编码 1SHOW CREATE TABLE tbl_name; 查看字段编码 1SHOW FULL COLUMNS FROM tbl_name; 查看系统的编码字符 1SHOW VARIABLES WHERE Variable_name LIKE &apos;character\\_set\\_%&apos; OR Variable_name LIKE &apos;collation%&apos;; MySQL字符集设置 系统变量： 12345678910111213– character_set_server：默认的内部操作字符集– character_set_client：客户端来源数据使用的字符集– character_set_connection：连接层字符集– character_set_results：查询结果字符集– character_set_database：当前选中数据库的默认字符集– character_set_system：系统元数据(字段名等)字符集– 还有以collation_开头的同上面对应的变量，用来描述字符序。 用introducer指定文本字符串的字符集： – 格式为：[_charset] ‘string’ [COLLATE collation] – 例如： 12345• SELECT _latin1 ‘string’;• SELECT _utf8 ‘你好’ COLLATE utf8_general_ci;–- 由introducer修饰的文本字符串在请求过程中不经过多余的转码，直接转换为内部字符集处理。 MySQL中的字符集转换过程 MySQL Server收到请求时将请求数据从character_set_client转换为character_set_connection； 进行内部操作前将请求数据从character_set_connection转换为内部操作字符集，其确定方法如下： • 使用每个数据字段的CHARACTER SET设定值； • 若上述值不存在，则使用对应数据表的DEFAULT CHARACTER SET设定值(MySQL扩展，非SQL标准)； • 若上述值不存在，则使用对应数据库的DEFAULT CHARACTER SET设定值； • 若上述值不存在，则使用character_set_server设定值。 参考 https://www.cnblogs.com/chyingp/p/mysql-character-set-collation.html https://www.cnblogs.com/qiumingcheng/p/10336170.html","tags":[{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"管理Odoo服务器实例","date":"2019-06-18T06:02:38.000Z","path":"2019/06/18/odoo/Odoo 12开发者指南第二章 管理Odoo服务器实例/","text":"全书完整目录请见：Odoo 12开发者指南（Cookbook）第三版 本章中，我们将讲解如下内容： 配置插件路径 更新插件模块列表 标准化你的实例目录布局 安装并升级本地插件模块 对插件应用修改 应用及尝试建议的拉取请求 引言在第一章 安装Odoo开发环境中，我们看了如何使用与编辑器一同发布的标准核心插件来设置 Odoo 实例。本章集中讲解为 Odoo 实例添加非核心插件。Odoo中，你可以从多个目录中加载插件。此外，推荐你将第三方插件（如OCA模块）或你自定义的插件放在一个单独的文件夹中，这样可以避免与 Odoo 核心模块产生冲突。甚至Odoo 企业版也是一种类型的插件目录，你需要像普通插件目录一样加载它。 ℹ️有关用词 – 插件(add-on) vs. 模块(module) 本书中，我们使用插件或插件模块来指代 Odoo 所预期安装的 Python 包。用户界面常使用应用（app）或模块的表达 ，但我们更愿意保留模块一词来表示Python模块或包，它们不一定是 Odoo 插件，而应用（app）来表示适当定义为应用的插件模块，表示它不是Odoo主菜单中的入口。 配置插件路径通过addons_path参数的配置，你可以在 Odoo 中加载自己的插件模块。在Odoo初始化一个新数据库时，它会搜索在addons_path配置参数中给定的这些目录。addons_path会在这些目录中搜索潜在的插件模块。addons_path中所列出的目录预期应包含子目录，每个子目录是一个插件模块。在数据库初始化完成后，你将能够安装这些目录中所给出的模块。 准备工作这一部分假定你已经准备好了实例并生成了配置文件，如在第一章 安装Odoo开发环境中在一个文件中存储实例配置一节所描述。Odoo的源码存放在~/odoo-dev/odoo中，而配置文件存放在~/odoo-dev/myinstance.cfg中。 如何配置…按如下步骤在实例的addons_path中添加~/odoo-dev/local-addons目录： 编辑你的实例的配置文件，即 ~/odoo-dev/my-instance.cfg。 定位到以addons_path =开头一行，默认，你会看到如下内容： 1addons_path = ~/odoo-dev/odoo/odoo/addons,~/odoo-dev/odoo/add-ons 译者注： 当前默认生成的配置文件中为绝对路径，且仅包含xxx/odoo/addons 修改该行，添加一个逗号（英文半角），并接你想想要添加为addons_的目录名称，如以下代码所示： 1addons_path = ~/odoo-dev/odoo/odoo/addons,~/odoo-dev/odoo/addons,~/odoo-dev/local-addons 重启你的实例 1$ ~/odoo-dev/odoo/odoo-bin -c my-instance.cfg 运行原理…在重启 Odoo 时，会读取配置文件。addons_path变量的值应为一个逗号分隔的目录列表。可接受相对路径，但它们是相对于当前工作目录的，因此应在配置文件中尽量避免。 至此，~/odoo-dev/local-addons中包含的新插件尚不在该实例的可用模块列表中。为此，你需要执行一个额外的操作，在下一部分更新插件模块列表中会进行讲解。 扩展知识…在第一次调用 odoo-bin脚本来初始化新数据库时，你可以传递一个带逗号分隔目录列表的–addons-path命令行参数。这会以所提供插件路径中所找到的所有插件来初始化可用插件模块列表。这么做时，你要显式地包含基础插件目录（odoo/odoo/addons）以及核心插件目录（odoo/addons）。 与前面稍有不同的是本地插件目录不能为空（译者注：请先阅读下面的小贴士），它必须要至少包含一个子目录，并包含插件模块的最小化结构。在第四章 创建Odoo插件模块中，我们会来看如何编写你自己的模块。同时，这里有一个生成内容来满足Odoo要求的快捷版黑科技： 1$ mkdir -p ~/odoo-dev/local-addons/dummy$ touch ~/odoo-dev/local-addons/dummy/__init__.py$ echo &apos;&#123;&quot;name&quot;: &quot;dummy&quot;, &quot;installable&quot;: False&#125;&apos; &gt; \\~/odoo-dev/local-addons/dummy/__manifest__.py 你可以使用–save选项来保存配置文件的路径： 1$ odoo/odoo-bin -d mydatabase \\--add-ons-path=&quot;odoo/odoo/addons,odoo/addons,~/odoo-dev/local-addons&quot; \\--save -c ~/odoo-dev/my-instance.cfg --stop-after-init 本例中，使用相对路径不会有问题，因为它们会在配置文件中转化为绝对路径。 小贴士：因为Odoo仅当从命令行中设置路径时在插件路径的目录中查看插件，而不是在从配置文件中加载路径的时候，dummy已不再必要。因此，你可以删除它（或保留到你确定不需要新建一个配置文件时）。 更新插件模块列表我们在前面的部分已经说到，在向插件路径添加目录时，仅仅重启Odoo服务是不足以安装其中一个新插件模块的。Odoo还需要有一个指定动作来扫描路径并更新可用插件模块的列表。 准备工作启动你的实例并使用管理员账号连接它。然后，激活开发者模式（如果你不知道如何激活开发者模式，请参见第一章 安装Odoo开发环境）。 如何更新…要更新你实例中的可用插件模块列表，你需要执行如下步骤： 打开Apps菜单 点击Update Apps List：[ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050902052063.jpg) 在弹出对话框中，点击Update按钮：[ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050902070776.jpg) 在更新的最后，你可以点击Apps入口来查看已更新的可用插件模块列表。你将需要删除Apps搜索框中的默认过滤器来查看所有模块。 运行原理…在点击了Update按钮之后，Odoo会读取插件路径配置变量。对于列表中的每一个目录，它会查看包含保存在插件模块目录中名为manifest.py的插件声明文件的直接子目录。Odoo读取声明内容，并预期查找其中的Python字典。除非声明内容中包含一个键installable的值为False，插件模块的元数据就会存储在数据库中。如果模块已存在，则更新相关信息。否则，会创建一条新记录。如果此前可用的插件模块未找到，则从列表中删除该记录。 ℹ️仅在初始化数据库后添加了新的插件路径时才需要更新应用列表。如果你在初始化数据库之前在配置文件中添加了新插件路径，那么就无需手动更新模块列表。 标准化你的实例目录布局我们推荐你在开发和生产环境都使用相似的目录布局。这一标准化会在你要执行运维时体现出用处，它也会缓解你日常工作的压力。 这一部分创建将相似生命周期或相似用途的文件分组放在标准化子目录中的目录结构。请自由按照自己的需求来调整这一结构，但请确保你将这一结构在某处进行记录存档。 如何标准化…创建所推荐实例布局，你需要执行如下步骤： 译者注：读者也可直接使用 Alan 在 GitHub 上准备的安装脚本进行操作 为实例创建一个目录： 1$ mkdir ~/odoo-dev/projectname$ cd ~/odoo-dev/projectname 在名为env/的子目录中创建一个Python虚拟环境： 1$ virtualenv -p python3 env 创建一些子目录，如下： 1$ mkdir src local bin filestore logs 这些子目录的功能如下： src/：这包含Odoo本身的一个拷贝，以及一些第三方插件项目（我们在下一步中添加了Odoo源码） local/：这用于保存你针对具体实例的插件 bin/：这包含各类帮助可执行shell脚本 filestore/：这用于文件存储 logs/（可选）：这用于存储服务日志文件 克隆Odoo并安装所需依赖包（参见 第一章 安装Odoo开发环境 获取更多内容）： 12$ git clone https://github.com/odoo/odoo.git src/odoo$ env/bin/pip3 install -r src/odoo/requirements.txt 以bin/odoo保存如下shell脚本： 12345ROOT=$(dirname $0)/..PYTHON=$ROOT/env/bin/python3ODOO=$ROOT/src/odoo/odoo-bin$PYTHON $ODOO -c $ROOT/projectname.cfg \"$@\"exit $? 让该脚本可执行： 1$ chmod +x bin/odoo 创建一个空的本地模块dummy： 123$ mkdir -p local/dummy$ touch local/dummy/__init__.py$ echo &apos;&#123;&quot;name&quot;: &quot;dummy&quot;, &quot;installable&quot;: False&#125;&apos; &gt;\\local/dummy/__manifest__.py 为你的实例生成配置文件： 1$ bin/odoo --stop-after-init --save \\ --addons-path src/odoo/odoo/addons,src/odoo/addons,local \\ --data-dir filestore 添加一个.gitignore文件，用于告诉GitHub排除这些给定目录，这样Git在提交代码时就会忽略掉这些目录，例如 filestore/, env/, logs/和src/： 123456789101112# dotfiles, with exceptions:.*!.gitignore# python compiled files*.py[co]# emacs backup files*~# not tracked subdirectories/env//src//filestore//logs/ 为这个实例创建一个Git仓库并将已添加的文件添加到Git中： 123$ git init$ git add .$ git commit -m \"initial version of projectname\" 运行原理…我们生成了一个有明确标签目录和独立角色的干净的目录结构。我使用了不同的目录来存储如下内容： 由其它人所维护的代码（src/中） 本地相关的具体代码 实例的文件存储 通过为每个项目建一个virtualenv环境，我们可以确保该项目的依赖文件不会与其它项目的依赖产生冲突，这些项目你可能运行着不同的Odoo版本或使用了不同的第三方插件模块，这将需要不同版本的Python依赖。这当然也会带来一部分磁盘空间的开销。 以类似的方式，通过为我们不同的项目使用不同的Odoo拷贝以及第三方插件模块，我们可以让每个项目单独的进行推进并仅在需要时在这些实例上安装更新，因此也减少了引入回退的风险。 bin/odoo允许我们不用记住各个路径或激活虚拟环境就可以运行服务。这还为我们设置了配置文件。你可以在其中添加其它脚本来协助你的日常工作。例如，你可以添加一个脚本来检查运行实例所需的第三方项目。 有关配置文件，我们仅展示了这里需要设置的最小化选项，但很明显你可以设置更多，例如数据库名、数据库过滤器或项目所监听的端口。有关这一话题的更多信息，请参见第一章 安装Odoo开发环境。 最后，通过在Git仓库中管理所有这些，在不同的电脑上复制这一设置及在团队中分享开发内容变得相当容易。 小贴士：加速贴士 要加速项目的创建，你可以创建一个包含空结构的模板仓库，并为每个项目复制（fork）该仓库。这会省却你重新输入bin/odoo脚本、.gitignore及其它所需模板文件（持续集成配置、README.md、ChangeLog等等）所花费的时间。 参见内容如果你喜欢这种方法，我们建议你尝试第三章 服务器部署中的使用 Docker 运行 Odoo 一部分的内容。 扩展知识…复杂模块的开发要求有各类配置选项，在想要尝试任何配置选项时都会要更新配置文件。更新配置常常是一件头痛的事，避免它的一种方式是通过命令行传递所有配置选项，如下： 手动激活虚拟环境： 1$ source env/bin/activate 进行Odoo源代码目录： 1$ cd src/odoo 运行服务： 1./odoo-bin --addons-path=addons,../../local -d test-12 -i account,sale,purchase --log-level=debug 第三步中，我们直接通过命令行传递了一些参数。第一个是–addons-path，它加载Odoo的核心插件目录addons，以及你自己的插件目录local，在其中你可以放自己的插件模块。选项-d会使用test-12数据库或者在该数据库不存在时新建一个数据库。选项-i 会安装会计、销售和采购模块。接着，我们传递了log-level选项来将日志级别提升为debug，这样日志中会显示更多的信息。 ℹ️通过使用命令行，你可以快速地修改配置选项。你也可以在Terminal中查看实时日志。所有可用选项可参见第一章 安装Odoo开发环境，或使用-help命令来查看所有的选项列表及各个选项的描述。 安装并升级本地插件模块Odoo 功能的核心来自于它的插件模块。Odoo自带的插件是你所拥有的财富，同时你也可以在应用商店下载一些插件模块或者自己写。 这一部分中，我们将展示如何通过网页界面及命令行来安装并升级插件模块。 对这些操作使用命令行的主要好处包含可以同时作用于一个以上的插件以及在安装或升级的过程中可以清晰地浏览到服务端日志，对于开发模式或编写脚本安装实例时都非常有用。 准备工作确保你有一个运行中的 Odoo 实例，且数据库已初始化、插件路径已进行恰当地设置。在这一部分中，我们将安装/升级一些插件模块。 如何安装升级…安装或升级插件有两种方法-可以使用网页界面或命令行。 通过网页界面可按照如下步骤来使用网页界面安装新的插件模块到数据库中： 使用管理员账户连接实例并打开Apps菜单[ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050906002399.jpg) 使用搜索框来定位你想要安装的插件。这里有一些帮助你完成该任务的操作指南： 激活Not Installed过滤器 如果你要查找一个具体的功能插件而不是广泛的功能插件，删除Apps过滤器 在搜索框中输入模块名的一部分并使用它来作为模块过滤器 你会发现使用列表视图可以阅读到更多的信息 点击卡片中模块名下的Install按钮。 注意有些Odoo插件模块需要有外部Python依赖，如果你的系统中未安装该Python依赖，那么 Odoo 会中止安装并显示如下的对话框： [ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050906125210.jpg) 译者注：按正常安装不会出现一错误，需通过 pip uninstall pyldap 才能复现这一错误 修复这一问题，仅需在你的系统中安装相关的Python依赖即可。 要升级已安装到数据库的模块，使用如下步骤： 使用管理员账户连接到实例 打开Apps菜单 点击Apps:[ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050906203077.jpg) 使用搜索框来定位你所安装的插件。有如下的小贴士： 激活Installed过滤器 如果你要查找一个具体的功能插件而不是广泛的功能插件，删除Apps过滤器 在搜索框中输入部分插件模块的名称并按下 Enter 来使用它作为模块过滤器。例如，输入CRM并按下 Enter 来搜索CRM应用 你会发现使用列表视图可以阅读到更多的信息 点击卡片右上角的的三个点，然后点击Upgrade选项： [ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050906265357.jpg) 激活开发者模式来查看模块的技术名称。如果你不知道如何激活开发者模式，请参见第一章 安装Odoo开发环境： [ Odoo 12开发者指南第二章 管理Odoo服务器实例](https://alanhou.org/homepage/wp-content/uploads/2019/05/2019050906302261.jpg) 在激活开发者模式之后，它会以红色显示模块的技术名称。如果你使用的是Odoo社区版，会看到一些带有Upgrade的附加应用。这些是Odoo企业版的应用，要想安装/使用它们，需要购买一个证书。 通过命令行要在你的数据库中安装新插件，可按照如下步骤： 查找插件的名称。这是包含manifest.py文件的目录名，不带前面的路径。 停止实例。如果你在操作生产数据库，请进行备份。 运行如下命令： 1odoo/odoo-bin -c instance.cfg -d dbname -i addon1,addon2 --stop-after-init 译者注： 请将addon1,addon2替换为你所要安装的插件名 小贴士：你可以省略掉-d dbname，因为这在配置文件中进行了设置。 重新启动实例 运行原理…插件模块的安装和升级是两个紧密关联的操作，但有一些重要的区别，在下面两部分中进行了强调： 插件安装在你安装插件时，Odoo以提供的名称检查它的可用插件列表中未安装插件。它还会检查该插件的依赖，并且如果有依赖的话，它会在安装插件前递归安装这些依赖。 单个模块的安装包含如下步骤： 如果存在，运行插件preinit钩子 从Python源代码中加载模型定义并在必要时更新数据库结构（参见第五章 应用模型了解更多信息） 加载插件的数据文件并在必要时更新数据库内容（参见第七章 模块数据了解更多信息） 如果实例中启用了演示数据则安装插件演示数据 如果存在，运行插件postinit钩子 运行对插件视图定义的验证 如果启用了演示数据并启用了测试，运行该插件的测试（参见第十八章 自动化测试用例了解更多信息） 在数据库中更新模块状态 从插件的翻译文件中更新数据库中的翻译（参见第十二章 国际化了解更多信息） ℹ️preinit和postinit钩子分别使用pre_init_hook和post_init_hook键名在manifest.py文件中定义。这些钩子用于在插件模块的安装之前及之后触发Python函数。参见第四章 创建Odoo插件模块了解更多有关 init 钩子的知识。 插件升级升级插件时，Odoo以给定的名称在可用的插件模块列表中检查已安装插件。它还会检查该插件的反向依赖（即依赖于所升级插件的那些插件）。如果存在，则也会对它们进行递归升级。 单个插件模块的升级过程包含如下步骤： 如果有的话，先运行插件模块的预迁移步骤（参见第七章 模块数据了解更多信息） 从Python源码中加载模型定义并在必要时更新数据库结构（参见第五章 应用模型了解更多信息） 加载插件的数据文件并在必要时更新数据库内容（参见第七章 模块数据了解更多信息） 如果实例中启用了演示数据更新插件演示数据 如果模块有任何迁移方法的话，先运行插件模块的后置迁移步骤（参见第七章 模块数据了解更多信息） 运行对插件视图定义的验证 如果启用了演示数据并启用了测试，运行该插件的测试（参见第十八章 自动化测试用例了解更多信息） 在数据库中更新模块状态 从插件的翻译文件中更新数据库中的翻译（参见第十二章 国际化了解更多信息） ℹ️注意更新未安装的插件模块什么也不会做。但是安装已安装的插件模块会重新安装该插件，这会通过一些包含数据的数据文件产生一些预期外的问题，这些文件可能应由用户进行更新而非在常规的模块升级处理时进行更新（参见第七章 模块数据中使用noupdate和forcecreate标记部分的内容）。通过用户界面不存在错误的风险，但通过命令行时则有可能发生。 扩展知识…要当心依赖的处理。假定有一个实例你想要安装sale、sale_stock和sale_specific插件，sale_specific依赖于sale_stock，而sale_stock依赖于sale。要安装这三者，你只需要安装sale_specific，因为它会递归安装sale_stock和sale这两个依赖。要升级这两者，你需要升级sale，因为这样会递归升级其反向依赖，sale_stock和sale_specific。 管理依赖另一个比较搞的地方是在你向已经有一个版本安装了的插件添加依赖的时候。我们继续通过前例来理解这一问题。想像一下你在sale_specific中添加了一个对stock_dropshipping的依赖。更新sale_specific插件不会自动安装新的依赖，也会要求安装sale_specific。在这种情况下，你会收到非常糟糕的错误消息，因为插件的Python代码没有成功的加载，而插件的数据和模型表则存在于数据库中。要解决这一问题，你需要停止该实例并手动安装新的依赖。 从GitHub安装插件模块GitHub是第三方插件的一个很好的来源。很多Odoo合作伙伴使用GitHub来分享他们内部维护的插件，而Odoo社区联盟（OCA）在GitHub上共同维护着几百个插件。在你开始编写自己的插件之前，确保查看是否已有可用的插件或者作为初始以继续扩展插件。 这一部分向你展示如何从GitHub上克隆OCA的partner-contact项目并让其中所包含的插件模块在我们实例中可用。 准备工作假设你想要改变你的实例中地址的处理方式，你的客户需要在Odoo两个字段（街道和街道2）之外的第三个字段来存储地址。你肯定是可以编写自己的插件来为res.partne添加一个字段的，但如果想要让地址在发票上以合适的格式显示，问题就要比看上去麻烦一些了。所幸，你邮件列表上的某个人告诉了你partner_address_street3插件，由OCA作为partner-contact项目的一部分进行维护。 本部分中所使用的路径反映了我们在标准化你的实例目录布局一节中所推荐的布局。 如何安装…按照如下步骤来安装partner_address_street3： 进入你的项目目录： 1$ cd ~/odoo-dev/my-odoo/src 在src/目录中克隆partner-contact项目的12.0分支： 1$ git clone --branch 12.0 \\https://github.com/OCA/partner-contact.git src/partner-contact 修改插件路径来包含该目录并更新你的实例中的插件列表（参见本章中的配置插件路径和更新插件模块列表一节）。instance.cfg中的addons_path一行应该是这样的： 1addons_path = ~/odoo-dev/my-odoo/src/odoo/odoo/addons, \\~/odoo-dev/my-odoo/src/odoo/addons, \\~/odoo-dev/my-odoo/src/, \\~/odoo-dev/local-addons 安装partner_address_street3插件（如果你不知道如何安装该模块，参见前面一节，安装并升级本地插件模块） 运行原理…所有 Odoo社区联盟的代码仓库都将他们自己的插件放在单独的目录中，这与Odoo对插件路径中目录的预期是相一致的。因此，只需复制某处的仓库并将其添加到插件路径中就够了。 扩展知识…有些维护者遵循不同的方法，每个插件模块一个仓库，放在仓库的根目录下。这种情况下，你需要创建一个新的目录，在这个目录中添加插件路径并克隆你所需的维护者的插件到该目录中。记住在每次添加一个新仓库拷贝时要更新插件模块列表。 对插件应用修改GitHub上可用的大部分插件需要进行修改并且不遵循Odoo对其稳定发行版所强制的规则。它们可能收到漏洞修复或改善，包含你提交的问题或功能请求，这些修改可能会引入数据库模式的修改或数据文件和视图中的更新。这一部分讲解如何安装升级后的版本。 准备工作假定你对partner_address_street3报告了一个问题并收到通知说该问题已在partner-contact项目12.0分支的最近一次修订中得以解决。这种情况下，你可以使用最新版本来更新你的实例。 如何修改…要对GitHub的插件进行源的变更，需执行如下步骤： 停止使用该插件的实例。 如果是生产实例请做一个备份（参见第一章 安装Odoo开发环境中管理Odoo服务端数据库一节）。 进入克隆了partner-contact的目录： 1$ cd ~/odoo-dev/my-odoo/src/partner-contact 为该项目创建一个本地标签，这样万一出现了崩溃你可以进行回退： 1$ git checkout 12.0$ git tag 12.0-before-update-$(date --iso) 获取源码的最新版本： 1$ git pull --ff-only 在你的数据库中更新partner_address_street3插件（参见安装并升级本地插件模块一节） 重启实例 运行原理…通常，插件模块的开发者有时会发布插件的最新版本。这一更新一般包含漏洞修复及新功能。这里，我们将获取一个插件的新版本并在我们的实例中更新它。 如果git pull –ff-only失败的话，你可以使用如下命令回退到前一个版本： 1$ git reset --hard 12.0-before-update-$(date --iso) 然后，你可以尝试git pull（不添加–ff-only），它会产生一个合并，但这表示你对插件做了本地修改。 扩展知识…如果更新这一步崩溃了，参见第一章 安装Odoo开发环境从源码更新Odoo一节获取恢复的操作指南。记住要总是在一个生产数据库的拷贝上先进行测试。 应用及尝试建议的拉取请求在GitHub的世界中，拉取请求（PR）是由开发者所提交的请求，这样项目维护人员可以添加一些新的开发。比如一个 PR 可能包含漏洞修复或新功能。这里请求在拉取到主分支之前会进行审核和测试。 这一部分讲解如何对你的 Odoo 项目应用一个PR来测试漏洞修复的改进。 准备工作在前一节中，假定你对partner_address_street3 报告了一个问题并收到一条通知在拉取请求中问题已修复，尚未合并到项目的12.0分支中。开发人员要求你验证PR #123中的修复状况。你需要使用这一分支更新一个测试实例。 你不应在生产数据库直接使用该分支，因此先创建一个带有生产数据库拷贝的测试环境（参见第一章 安装Odoo开发环境和第三章 服务器部署）。 如何操作…应用并测试一个插件的GitHub拉取请求，你需要执行如下步骤： 停止实例 进入partner-contact所被克隆的目录： 1$ cd ~/odoo-dev/my-odoo/src/partner-contact 为该项目创建一个本地标签，这样万一出现崩溃时你可以回退： 1$ git checkout 12.0$ git tag 12.0-before-update-$(date --iso 拉取pull请求的分支。这么做最容易的方式是使用PR编号，在开发者与你沟通时你应该可以看到。在本例中，这个拉取请求编号是123： 1$ git pull origin pull/123/head 在你的数据库中更新partner_address_street3插件模块并重启该实例（如果你不知道如何更新该模块的话请参见安装并升级本地插件模块一节） 测试该更新 – 尝试重现问题，或测试你想要的功能。 如果这不能运行，在GitHub的PR页面进行评论，说明你做了什么以及什么不能运行，这样开发者可以更新这个拉取请求。 如果它没有问题，也在PR页面说下；这是PR验证流程中非常重要的一部分；这会加速主分支中的合并。 运行原理…我们在使用一个GitHub功能，使用pull/nnnn/head分支名称来通过编号进行拉取请求的拉取，其中nnnn是PR的编号。Git pull命令合并远程分支到我们的分支，在我们基础代码中应用修改。在这之后，我们更新插件模块、对其测试并向作者报回修改是成功或是失败。 扩展知识…如果你想要同步测试它们，你可以针对相同仓库的不同拉取请求重复本节中的第4步。如果你对结果很满意，你可以创建一个分支来保留对应用了改变的结果的引用： 1$ git checkout -b 12.0-custom 使用一个不同的分支会帮助你记住你没有从GitHub使用该版本，而是一个自定义的版本。 ℹ️git branch命令可用于列出你仓库中的所有本地分支。 从这开始，如果你需要应用来自GitHub中12.0分支的最近一个审核版本，你需要不使用–ff-only来拉取它： 1$ git pull origin 12.0","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"},{"name":"odoo","slug":"odoo","permalink":"http://wumuwumu.github.io/tags/odoo/"}]},{"title":"__import__在python中的区别","date":"2019-06-01T07:49:16.000Z","path":"2019/06/01/import-在python中的区别/","text":"import作用：导入/引入一个python标准模块，其中包括.py文件、带有init.py文件的目录(自定义模块)。 import module_name[,module1,…] from module import *|child[,child1,…] 注意：多次重复使用import语句时，不会重新加载被指定的模块，只是把对该模块的内存地址给引用到本地变量环境。 实例： pythontab.py 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `os ``print` `'in pythontab'``,``id``(os)` test.py 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `pythontab ``#第一次会打印pythontab里面的语句 ``import` `os ``#再次导入os后，其内存地址和pythontab里面的是一样的，因此这里只是对os的本地引用 ``print` `&apos;in c&apos;``,``id``(os) ``import` `pythontab ``#第二次不会打印pythontab里面的语句，因为没有重新加载` reload作用：对已经加载的模块进行重新加载，一般用于原模块有变化等特殊情况，reload前该模块必须已经import过。 import os reload(os) 说明： reload会重新加载已加载的模块，但原来已经使用的实例还是会使用旧的模块，而新生产的实例会使用新的模块；reload后还是用原来的内存地址；不能支持from。。import。。格式的模块进行重新加载。 实例： pythontab.py 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `os ``print` `'in pythontab'``,``id``(os)` test.py 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `pythontab ``#第一次import会打印pythontab里面的语句 ``print` `id``(pythontab) ``#原来pythontab的内存地址 ``reload``(pythontab) ``#第二次reload还会打印pythontab里面的语句，因为有重新加载 ``print` `id``(pythontab) ``#reload后pythontab的内存地址，和原来一样` 扩展： 上面说了，在特殊情况的下才会使用reload函数；除了原来模块文件有修改外，还有哪些情况需要使用reload函数呢，这里举个例子。 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `sys ``#引用sys模块进来，并不是进行sys的第一次加载 ``reload``(sys) ``#重新加载sys ``sys.setdefaultencoding(``'utf8'``) ``##调用setdefaultencoding函数` 上面的代码是正确的，再测试下面的代码 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `sys ``sys.setdefaultencoding(``'utf8'``)` 上面的测试会失败，那么为什么要在调用setdefaultencoding时必须要先reload一次sys模块呢？因为这里的import语句其实并不是sys的第一次导入语句，也就是说这里其实可能是第二、三次进行sys模块的import，这里只是一个对sys的引用，只能reload才能进行重新加载；那么为什么要重新加载，而直接引用过来则不能调用该函数呢？因为setdefaultencoding函数在被系统调用后被删除了，所以通过import引用进来时其实已经没有了，所以必须reload一次sys模块，这样setdefaultencoding才会为可用，才能在代码里修改解释器当前的字符编码。试试下面的代码，同样会报错： 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `sys ``reload``(sys) ``sys.setdefaultencoding(``'utf8'``) ``del` `sys.setdefaultencoding ``##删除原来的setdefaultencoding函数 ``sys.setdefaultencoding(``'gb2312'``)` 那么到底是谁在之前就导入sys并且调用了setdefaultencoding函数呢？答案就在python安装目录的Lib文件夹下，有一个叫site.py的文件【python2.6】，在里面可以找到main() –&gt; setencoding()–&gt;sys.setdefaultencoding(encoding),因为这个site.py每次启动python解释器时会自动加载，所以main函数每次都会被执行，setdefaultencoding函数一出来就已经被删除了。 import作用： 同import语句同样的功能，但import是一个函数，并且只接收字符串作为参数，所以它的作用就可想而知了。其实import语句就是调用这个函数进行导入工作的，import sys &lt;==&gt;sys = import(‘sys’) 使用： import(module_name[, globals[, locals[, fromlist]]]) #可选参数默认为globals(),locals(),[] import(‘os’) import(‘os’,globals(),locals(),[‘path’,’pip’]) #等价于from os import path, pip 说明： 通常在动态加载时可以使用到这个函数，比如你希望加载某个文件夹下的所用模块，但是其下的模块名称又会经常变化时，就可以使用这个函数动态加载所有模块了，最常见的场景就是插件功能的支持。 扩展： 既然可以通过字符串来动态导入模块，那么是否可以通过字符串动态重新加载模块吗？试试reload(‘os’)直接报错，是不是没有其他方式呢?虽然不能直接reload但是可以先unimport一个模块，然后再import来重新加载模块。现在看看unimport操作如何实现，在Python解释里可以通过globals(),locals(),vars(),dir()等函数查看到当前环境下加载的模块及其位置，但是这些都只能看不能删除，所以无法unimport；不过除此之外还有一个地方是专门存放模块的，这就是sys.modules，通过sys.modules可以查看所有的已加载并且成功的模块，而且比globals要多，说明默认会加载一些额外的模块，接下来就是unimport了。 1`#!/usr/bin/env python ``#encoding: utf-8 ``import` `sys ``__import__``(``'a'``) ``#第一次导入会打印消息 ``del` `sys.modules[``'a'``] ``#unimport ``__import__``(``'a'``) ``#再次导入还是会打印消息，因为已经unimport一次了 ``__import__``(``'a'``) ``#这次就不会打印消息了`","tags":[{"name":"python","slug":"python","permalink":"http://wumuwumu.github.io/tags/python/"}]},{"title":"java多线程自问","date":"2019-04-15T02:31:35.000Z","path":"2019/04/15/java/java多线程自问/","text":"java创建线程的方式 java的线程的类型 Timer与TimerTask的区别 怎么启动、停止、加入、礼让线程 线程的生命周期以及其切换 CountDownLatch、CyclicBarrier和Semaphore 什么是线程安全？Vector是一个线程安全类吗？","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"redis分布式锁","date":"2019-04-11T09:55:56.000Z","path":"2019/04/11/redis分布式锁/","text":"单机Redis实现分布式锁获取锁获取锁的过程很简单，客户端向Redis发送命令： 12SET resource_name my_random_value NX PX 30000复制代码 my_random_value是由客户端生成的一个随机字符串，它要保证在足够长的一段时间内在所有客户端的所有获取锁的请求中都是唯一的。 NX表示只有当resource_name对应的key值不存在的时候才能SET成功。这保证了只有第一个请求的客户端才能获得锁，而其它客户端在锁被释放之前都无法获得锁。 PX 30000表示这个锁有一个30秒的自动过期时间。 释放锁123456if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1])else return 0end复制代码 之前获取锁的时候生成的my_random_value 作为参数传到Lua脚本里面，作为：ARGV[1],而 resource_name作为KEYS[1]。Lua脚本可以保证操作的原子性。 关于单点Redis实现分布式锁的讨论网络上有文章说用如下命令获取锁: 123SETNX resource_name my_random_valueEXPIRE resource_name 30复制代码 由于这两个命令不是原子的。如果客户端在执行完SETNX后crash了，那么就没有机会执行EXPIRE了，导致它一直持有这个锁，其他的客户端就永远获取不到这个锁了。 为什么my_random_value 要设置成随机值? 保证了一个客户端释放的锁是自己持有的那个锁。如若不然，可能出现锁不安全的情况。 123456客户端1获取锁成功。客户端1在某个操作上阻塞了很长时间。过期时间到了，锁自动释放了。客户端2获取到了对应同一个资源的锁。客户端1从阻塞中恢复过来，释放掉了客户端2持有的锁。复制代码 用 SETNX获取锁 网上大量文章说用如下命令获取锁： 12SETNX lock.foo &lt;current Unix time + lock timeout + 1&gt;复制代码 原文在Redis对SETNX的官网说明，Redis官网文档建议用Set命令来代替，主要原因是SETNX不支持超时时间的设置。 redis.io/commands/se… Redis集群实现分布式锁上面的讨论中我们有一个非常重要的假设：Redis是单点的。如果Redis是集群模式，我们考虑如下场景: 123456客户端1从Master获取了锁。Master宕机了，存储锁的key还没有来得及同步到Slave上。Slave升级为Master。客户端2从新的Master获取到了对应同一个资源的锁。客户端1和客户端2同时持有了同一个资源的锁，锁不再具有安全性。复制代码 就此问题，Redis作者antirez写了RedLock算法来解决这种问题。 RedLock获取锁 获取当前时间。 按顺序依次向N个Redis节点执行获取锁的操作。这个获取操作跟前面基于单Redis节点的获取锁的过程相同，包含随机字符串my_random_value，也包含过期时间(比如PX 30000，即锁的有效时间)。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个Redis节点获取锁失败以后，应该立即尝试下一个Redis节点。 计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。如果客户端从大多数Redis节点（&gt;= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。 如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。 如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有Redis节点发起释放锁的操作（即前面介绍的单机Redis Lua脚本释放锁的方法）。 RedLock释放锁客户端向所有Redis节点发起释放锁的操作，不管这些节点当时在获取锁的时候成功与否。 关于RedLock的问题讨论 如果有节点发生崩溃重启 假设一共有5个Redis节点：A, B, C, D, E。设想发生了如下的事件序列： 12345客户端1成功锁住了A, B, C，获取锁成功（但D和E没有锁住）。节点C崩溃重启了，但客户端1在C上加的锁没有持久化下来，丢失了。节点C重启后，客户端2锁住了C, D, E，获取锁成功。客户端1和客户端2同时获得了锁。复制代码 为了应对这一问题，antirez又提出了延迟重启(delayed restarts)的概念。也就是说，一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，这段时间应该大于锁的有效时间(lock validity time)。这样的话，这个节点在重启前所参与的锁都会过期，它在重启后就不会对现有的锁造成影响。 如果客户端长期阻塞导致锁过期 解释一下这个时序图，客户端1在获得锁之后发生了很长时间的GC pause，在此期间，它获得的锁过期了，而客户端2获得了锁。当客户端1从GC pause中恢复过来的时候，它不知道自己持有的锁已经过期了，它依然向共享资源（上图中是一个存储服务）发起了写数据请求，而这时锁实际上被客户端2持有，因此两个客户端的写请求就有可能冲突（锁的互斥作用失效了）。 如何解决这个问题呢?引入了fencing token的概念： 客户端1先获取到的锁，因此有一个较小的fencing token，等于33，而客户端2后获取到的锁，有一个较大的fencing token，等于34。客户端1从GC pause中恢复过来之后，依然是向存储服务发送访问请求，但是带了fencing token = 33。存储服务发现它之前已经处理过34的请求，所以会拒绝掉这次33的请求。这样就避免了冲突。 但是其实这已经超出了Redis实现分布式锁的范围，单纯用Redis没有命令来实现生成Token。 时钟跳跃问题 假设有5个Redis节点A, B, C, D, E。 123456客户端1从Redis节点A, B, C成功获取了锁（多数节点）。由于网络问题，与D和E通信失败。节点C上的时钟发生了向前跳跃，导致它上面维护的锁快速过期。客户端2从Redis节点C, D, E成功获取了同一个资源的锁（多数节点）。客户端1和客户端2现在都认为自己持有了锁。这个问题用Redis实现分布式锁暂时无解。而生产环境这种情况是存在的。复制代码 结论 Redis并不能实现严格意义上的分布式锁。但是这并不意味着上面讨论的方案一无是处。如果你的应用场景为了效率(efficiency)，协调各个客户端避免做重复的工作，即使锁失效了，只是可能把某些操作多做一遍而已，不会产生其它的不良后果。但是如果你的应用场景是为了正确性(correctness)，那么用Redis实现分布式锁并不合适，会存在各种各样的问题，且解决起来就很复杂，为了正确性，需要使用zab、raft共识算法，或者使用带有事务的数据库来实现严格意义上的分布式锁。 参考资料 Distributed locks with Redis 基于Redis的分布式锁到底安全吗（上）？ - 铁蕾的个人博客 martin.kleppmann.com/2016/02/08/…","tags":[{"name":"redis","slug":"redis","permalink":"http://wumuwumu.github.io/tags/redis/"}]},{"title":"protobuf使用","date":"2019-04-10T02:31:04.000Z","path":"2019/04/10/protobuf使用/","text":"安装1234wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.zipunzop protobuf-all-3.6.1.zipcd protobuf-all-3.6.1./configure &amp;&amp; make &amp;&amp; make install 语法规则123456789101112131415161718192021222324252627282930313233343536// 声明版本，默认是proto2syntax = &quot;proto3&quot;;// 声明包名package tutorialoption java_package = &quot;com.example.tutorial&quot;;// java类名option java_outer_classname = &quot;AddressBookProtos&quot;;message Person &#123; required string name =1; required int32 id = 2; optional string email = 3; enum PhoneType &#123; MOBILE = 0; HOME = 1; WORK = 2; &#125; message PhoneNumber &#123; required string number = 1; optional PhoneType type = 2[default = HOME]; &#125; repeated PhoneNumber phones = 4;&#125;message AddressBook &#123; repreated Person people = 1;&#125;// 保留字段，编程过程中某些功能没有想好，可以先把该tag 进行保留，以备以后使用。message Foo &#123; reserved 2, 15, 9 to 11; reserved &quot;foo&quot;, &quot;bar&quot;;&#125; 编码 https://blog.csdn.net/zxhoo/article/details/53228303 方法 Standard Message Methods isInitialized(): checks if all the required fields have been set. toString(): returns a human-readable representation of the message, particularly useful for debugging. mergeFrom(Message other): (builder only) merges the contents of other into this message, overwriting singular scalar fields, merging composite fields, and concatenating repeated fields. clear(): (builder only) clears all the fields back to the empty state. Parsing and Serialization byte[] toByteArray();: serializes the message and returns a byte array containing its raw bytes. static Person parseFrom(byte[] data);: parses a message from the given byte array. void writeTo(OutputStream output);: serializes the message and writes it to an OutputStream. static Person parseFrom(InputStream input);: reads and parses a message from an InputStream. 编译注意 升级协议 you must not change the tag numbers of any existing fields. you must not add or delete any required fields. you may delete optional or repeated fields. you may add new optional or repeated fields but you must use fresh tag numbers (i.e. tag numbers that were never used in this protocol buffer, not even by deleted fields). protobuf对repeated压缩不够好，所以尽量在后面加上[packed = true]。 不要让protobuf对象成为全局变量或者类成员，因为其clear方法只会把占用的内存空间清零，而不会释放，使得进程空间越来越大，可参考《Protobuf使用不当导致的程序内存上涨问题》。 https://www.jianshu.com/p/27fdf44dd63b","tags":[{"name":"go","slug":"go","permalink":"http://wumuwumu.github.io/tags/go/"}]},{"title":"go基本语法","date":"2019-04-10T02:29:55.000Z","path":"2019/04/10/go基本语法/","text":"接口 duck typing了解 在程序设计中，鸭子类型（英语：duck typing）是动态类型的一种风格。在这种风格中，一个对象有效的语义，不是由继承自特定的类或实现特定的接口，而是由“当前方法)和属性的集合”决定。 flagSync1. WaitGroup123Add()Done()Wait() 2. Context12 Regexp https://www.cnblogs.com/golove/p/3269099.html 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// MatchStringmatched, err := regexp.MatchString(&quot;foo.*&quot;, &quot;seafood&quot;)fmt.Println(matched, err)matched, err = regexp.MatchString(&quot;bar.*&quot;, &quot;seafood&quot;)fmt.Println(matched, err)// false error parsing regexp: missing closing ): `a(b`matched, err = regexp.MatchString(&quot;a(b&quot;, &quot;seafood&quot;)fmt.Println(matched, err)// true &lt;nil&gt;matched, err = regexp.MatchString(`a\\(b`, &quot;a(b&quot;)fmt.Println(matched, err)// false error parsing regexp: missing closing ): `a(b`matched, err = regexp.MatchString(`a(b`, &quot;a(b&quot;)fmt.Println(matched, err)// true &lt;nil&gt;matched, err = regexp.MatchString(&quot;a\\\\(b&quot;, &quot;a(b&quot;)fmt.Println(matched, err)// 将所有特殊字符进行转义fmt.Println(regexp.QuoteMeta(&quot;Escaping symbols like: .+*?()|[]&#123;&#125;^$&quot;))// ExpandStringcontent := ` # comment line option1: value1 option2: value2 # another comment line option3: value3`// Regex pattern captures &quot;key: value&quot; pair from the content.pattern := regexp.MustCompile(`(?m)(?P&lt;key&gt;\\w+):\\s+(?P&lt;value&gt;\\w+)$`)// Template to convert &quot;key: value&quot; to &quot;key=value&quot; by// referencing the values captured by the regex pattern.template := &quot;$key=$value\\n&quot;result := []byte&#123;&#125; // For each match of the regex in the content.for _, submatches := range pattern.FindAllStringSubmatchIndex(content, -1) &#123; // Apply the captured submatches to the template and append the output // to the result. result = pattern.ExpandString(result, template, content, submatches)&#125;fmt.Println(string(result))// findAllStringre := regexp.MustCompile(&quot;a.&quot;)fmt.Println(re.FindAllString(&quot;paranormal&quot;, -1))fmt.Println(re.FindAllString(&quot;paranormal&quot;, 2))fmt.Println(re.FindAllString(&quot;graal&quot;, -1))fmt.Println(re.FindAllString(&quot;none&quot;, -1))// FindAllStringSubmatchre := regexp.MustCompile(&quot;a(x*)b&quot;)fmt.Printf(&quot;%q\\n&quot;, re.FindAllStringSubmatch(&quot;-ab-&quot;, -1))fmt.Printf(&quot;%q\\n&quot;, re.FindAllStringSubmatch(&quot;-axxb-&quot;, -1))fmt.Printf(&quot;%q\\n&quot;, re.FindAllStringSubmatch(&quot;-ab-axb-&quot;, -1))fmt.Printf(&quot;%q\\n&quot;, re.FindAllStringSubmatch(&quot;-axxb-ab-&quot;, -1))// findStringSubmatch，只查找第一个re := regexp.MustCompile(&quot;a(x*)b(y|z)c&quot;)fmt.Printf(&quot;%q\\n&quot;, re.FindStringSubmatch(&quot;-axxxbyc-&quot;))fmt.Printf(&quot;%q\\n&quot;, re.FindStringSubmatch(&quot;-abzc-&quot;))","tags":[{"name":"go","slug":"go","permalink":"http://wumuwumu.github.io/tags/go/"}]},{"title":"makefile编写","date":"2019-04-10T02:27:53.000Z","path":"2019/04/10/makefile编写/","text":"例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091.PHONY: build clean test package package-deb ui api statics requirements ui-requirements serve update-vendor internal/statics internal/migrations static/swagger/api.swagger.jsonPKGS := $(shell go list ./... | grep -v /vendor |grep -v lora-app-server/api | grep -v /migrations | grep -v /static | grep -v /ui)VERSION := $(shell git describe --always |sed -e &quot;s/^v//&quot;)build: ui/build internal/statics internal/migrations mkdir -p build go build $(GO_EXTRA_BUILD_ARGS) -ldflags &quot;-s -w -X main.version=$(VERSION)&quot; -o build/lora-app-server cmd/lora-app-server/main.goclean: @echo &quot;Cleaning up workspace&quot; @rm -rf build dist internal/migrations internal/static ui/build static/static @rm -f static/index.html static/icon.png static/manifest.json static/asset-manifest.json static/service-worker.js @rm -rf static/logo @rm -rf docs/public @rm -rf disttest: internal/statics internal/migrations @echo &quot;Running tests&quot; @for pkg in $(PKGS) ; do \\ golint $$pkg ; \\ done @go vet $(PKGS) @go test -p 1 -v $(PKGS)documentation: @echo &quot;Building documentation&quot; @mkdir -p dist/docs @cd docs &amp;&amp; hugo @cd docs/public/ &amp;&amp; tar -pczf ../../dist/lora-app-server-documentation.tar.gz .dist: ui/build internal/statics internal/migrations @goreleaserbuild-snapshot: ui/build internal/statics internal/migrations @goreleaser --snapshotpackage-deb: package @echo &quot;Building deb package&quot; @cd packaging &amp;&amp; TARGET=deb ./package.shui/build: @echo &quot;Building ui&quot; @cd ui &amp;&amp; npm run build @mv ui/build/* staticapi: @echo &quot;Generating API code from .proto files&quot; @go generate api/api.gointernal/statics internal/migrations: static/swagger/api.swagger.json @echo &quot;Generating static files&quot; @go generate cmd/lora-app-server/main.gostatic/swagger/api.swagger.json: @echo &quot;Generating combined Swagger JSON&quot; @GOOS=&quot;&quot; GOARCH=&quot;&quot; go run api/swagger/main.go api/swagger &gt; static/swagger/api.swagger.json @cp api/swagger/*.json static/swagger# shortcuts for developmentrequirements: echo &quot;Installing development tools&quot; go get -u github.com/golang/lint/golint go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway go get -u github.com/grpc-ecosystem/grpc-gateway/protoc-gen-swagger go get -u github.com/golang/protobuf/protoc-gen-go go get -u github.com/elazarl/go-bindata-assetfs/... go get -u github.com/jteeuwen/go-bindata/... go get -u github.com/kisielk/errcheck go get -u github.com/smartystreets/goconvey go get -u golang.org/x/tools/cmd/stringer go get -u github.com/golang/dep/cmd/dep go get -u github.com/goreleaser/goreleaser dep ensure -vui-requirements: @echo &quot;Installing UI requirements&quot; @cd ui &amp;&amp; npm installserve: build @echo &quot;Starting Lora App Server&quot; ./build/lora-app-serverupdate-vendor: @echo &quot;Updating vendored packages&quot; @govendor update +externalrun-compose-test: docker-compose run --rm appserver make test 文件格式12&lt;target&gt; : &lt;prerequisites&gt; [tab] &lt;commands&gt; target：执行的命令或者文件名。如果只是执行的命令这是伪指令，在大部分时候使用.PHONY声明伪指令，这样不仅仅提供效率，同时也避免和文件名冲突。 prerequisites：前置条件。 commands：需要执行的命令， 前面需要添加[tab]，如果想要换成其他的，使用.RECIPEPREFIX = ？换成你喜欢的。 执行命令的时候会打印出相关的命令内容，这个叫做回显，如果不想显示出来可以在命令前面添加@。 命令执行的时候，每行命令在不同一个shell中执行，如果想在同一个shell中执行，有下面几个办法。 将命令写在同一行 在命令后面添加\\，实现命令多行 使用.ONESHELL: 内置变量makefile可以通过=、:=、?=、+=给变量赋值，同时Make命令提供一系列内置变量，比如，((CC)指向当前使用的编译器，)(MAKE) 指向当前使用的Make工具。这主要是为了跨平台的兼容性，详细的内置变量清单见手册。 参考https://blog.csdn.net/u010230971/article/details/80335613 https://www.cnblogs.com/wang_yb/p/3990952.html http://www.ruanyifeng.com/blog/2015/02/make.html","tags":[{"name":"go","slug":"go","permalink":"http://wumuwumu.github.io/tags/go/"}]},{"title":"mybatis-sessions","date":"2019-04-10T02:25:51.000Z","path":"2019/04/10/mybatis-sessions/","text":"SqlSessionFactorysqlSessionFactory是工厂类的接口，默认实现是DefaultSqlSessionFactory，通过sqlSessionFactoryBuilder创建，我们不具体讨论配置文件的具体解析，主要分析mybatis的运行流程。 SqlSessionFactory主要是用来创建SqlSession，SqlSession是线程不安全的，因此每次操作都要重新创建。 12345678910111213141516171819202122// 通过数据源创建SqlSession，是我们比较常用的一种方式private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); //通过事务工厂来产生一个事务 tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); //生成一个执行器(事务包含在执行器里) final Executor executor = configuration.newExecutor(tx, execType); //然后产生一个DefaultSqlSession return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; //如果打开事务出错，则关闭它 closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(&quot;Error opening session. Cause: &quot; + e, e); &#125; finally &#123; //最后清空错误上下文 ErrorContext.instance().reset(); &#125; &#125;SqlSession SqlSession有两方式调用方法，第一种方式是通过命名空间调用，第二种方式是JavaBean调用，也就是通过我们常用的Mapper接口进行调用。现在Myabtis3我们基本使用第二种方式。 通过Mapper接口进行调用，核心是 获取Mapper接口，并通过动态代理，进行方法拦截。 SqlSession通过getMapper获取相应的Mapper接口。SqlSession的的数据库操作是调用Executor的相关方法。 在getMapper调用的时候，有几个核心的类 MapperProxyFactory:用于创建MapperProxyd的工厂方法 MapperProxy:动态代理的InvocationHandler的实现，实际中就是执行sql语句 MapperRegistry MapperMethood:调用SqlSession的方法","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"},{"name":"mybatis","slug":"mybatis","permalink":"http://wumuwumu.github.io/tags/mybatis/"}]},{"title":"git基本操作","date":"2019-04-09T05:59:25.000Z","path":"2019/04/09/git基本操作/","text":"简介在实际开发中，会使用git作为版本控制工具来完成团队协作。因此，对基本的git操作指令进行总结是十分有必要的，本文对一些术语或者理论基础，不重新码字，可以参考廖雪峰老师的博文，本文只对命令做归纳总结。 git的通用操作流程如下图（来源于网络） 主要涉及到四个关键点： 工作区：本地电脑存放项目文件的地方，比如learnGitProject文件夹； 暂存区（Index/Stage）：在使用git管理项目文件的时候，其本地的项目文件会多出一个.git的文件夹，将这个.git文件夹称之为版本库。其中.git文件夹中包含了两个部分，一个是暂存区（Index或者Stage）,顾名思义就是暂时存放文件的地方，通常使用add命令将工作区的文件添加到暂存区里； 本地仓库：.git文件夹里还包括git自动创建的master分支，并且将HEAD指针指向master分支。使用commit命令可以将暂存区中的文件添加到本地仓库中； 远程仓库：不是在本地仓库中，项目代码在远程git服务器上，比如项目放在github上，就是一个远程仓库，通常使用clone命令将远程仓库拷贝到本地仓库中，开发后推送到远程仓库中即可； 更细节的来看： 日常开发时代码实际上放置在工作区中，也就是本地的XXX.java这些文件，通过add等这些命令将代码文教提交给暂存区（Index/Stage），也就意味着代码全权交给了git进行管理，之后通过commit等命令将暂存区提交给master分支上，也就是意味打了一个版本，也可以说代码提交到了本地仓库中。另外，团队协作过程中自然而然还涉及到与远程仓库的交互。 因此，经过这样的分析，git命令可以分为这样的逻辑进行理解和记忆： git管理配置的命令； 几个核心存储区的交互命令： 工作区与暂存区的交互； 暂存区与本地仓库（分支）上的交互； 本地仓库与远程仓库的交互。 安装git安装 https://git-scm.com/ 配置123456$ git config --global user.name \"Your Name\"$ git config --global user.email \"email@example.com\"$ git config --global core.editor emacs$ git config --list$ git config user.name 快速开始1234$ git init # 初始化工程$ git add * # 将文件添加到暂存区$ git commit -m # 提交$ git clone https://github.com/libgit2/libgit2 常用命令add git add -A 保存所有的修改 git add . 保存新的添加和修改，但是不包括删除 git add -u 保存修改和删除，但是不包括新建文件。 commit git commit -m git commit -ma // -a是添加全部修改 git commit –amend checkout git checkout — //使用暂缓区替换工作区 git checkout 切换分支 git checkout head — //直接使用本地参考的文件覆盖工作区文件 rm git rm // 删除工作区，并且提交 git rm —cached // 只删除暂存区 git rm -f // 暂存区和工作区都删除 reset谨慎使用！！！！！ –soft – 缓存区和工作目录都不会被改变 –mixed – 默认选项。缓存区和你指定的提交同步，但工作目录不受影响 –hard – 缓存区和工作目录都同步到你指定的提交 revert前提是已经提交，缺点：一次回滚过个记录会出现冲突。","tags":[{"name":"git","slug":"git","permalink":"http://wumuwumu.github.io/tags/git/"}]},{"title":"go工程搭建","date":"2019-04-09T01:26:21.000Z","path":"2019/04/09/go/go工程搭建/","text":"工程基本结构","tags":[{"name":"go","slug":"go","permalink":"http://wumuwumu.github.io/tags/go/"}]},{"title":"mysql权限管理","date":"2019-03-29T08:55:22.000Z","path":"2019/03/29/mysql/mysql权限管理/","text":"用户管理基本操作12345678create user zhangsan identified by 'zhangsan';SELECT current_user(); ← 查看当前用户SELECT user,host FROM mysql.user; ← 查看用户信息SHOW GRANTS; ← 当前用户权限，会生成SQL语句CREATE USER 'user'@'host' IDENTIFIED BY 'password'; ← 创建用户DROP USER 'user'@'host'; ← 删除用户RENAME USER 'user'@'host' TO 'fool'@'host'; 修改密码12345678mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'new-password'; ← 修改密码(recommand)mysql&gt; SET PASSWORD FOR 'root'@'localhost'=PASSWORD('new-password'); ← 修改密码mysql&gt; UPDATE mysql.user SET password=PASSWORD('new-password') WHERE USER='root' AND Host='127.0.0.1';mysql&gt; UPDATE mysql.user SET password='' WHERE user='root'; ← 清除密码mysql&gt; FLUSH PRIVILEGES;$ mysqladmin -uROOT -pOLD_PASSWD password NEW_PASSWD ← 通过mysqladmin修改$ mysqladmin -uROOT -p flush-privileges 权限管理1234567891011mysql&gt; GRANT ALL ON *.* TO 'user'@'%' [IDENTIFIED BY 'password'];mysql&gt; GRANT ALL PRIVILIGES ON [TABLE | DATABASE] student,course TO user1,user2;mysql&gt; GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, CREATE TEMPORARY, ALTER, DROP, REFERENCES, INDEX, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, EXECUTE ON db.tbl TO 'user'@'host' [IDENTIFIED BY 'password'];mysql&gt; GRANT ALL ON sampdb.* TO PUBLIC WITH GRANT OPTION; ← 所有人，可以授权给其他人mysql&gt; GRANT UPDATE(col),SELECT ON TABLE tbl TO user; ← 针对列赋值mysql&gt; SHOW GRANTS [FOR 'user'@'host']; ← 查看权限信息mysql&gt; REVOKE ALL ON *.* FROM 'user'@'host'; ← 撤销权限mysql&gt; REVOKE SELECT(user, host), UPDATE(host) ON db.tbl FROM 'user'@'%'; 权限admin12345mysql&gt; CREATE USER &apos;admin&apos;@&apos;IP&apos; IDENTIFIED BY &apos;password&apos;;mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;admin&apos;@&apos;IP&apos;;mysql&gt; REVOKE ALL PRIVILEGES ON *.* FROM &apos;admin&apos;@&apos;IP&apos;;mysql&gt; DROP USER &apos;admin&apos;@&apos;IP&apos;; root1mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'localhost' WITH GRANT OPTION; 其他重置root密码123456789101112131415161718----- 1. 停止mysql服务器# systemctl stop mysqld# /opt/mysql-5.7/bin/mysqladmin -uroot -p'init-password' shutdownShutting down MySQL.. done----- 2. 获取跳过认证的启动参数# mysqld --help --verbose | grep 'skip-grant-tables' -A1 --skip-grant-tables Start without grant tables. This gives all users FULL ACCESS to all tables.----- 3. 启动服务器，跳过认证# mysqld --skip-grant-tables --user=mysql &amp;[1] 10209----- 4. 取消密码mysql&gt; UPDATE mysql.user SET password='' WHERE user='root';Query OK, 2 rows affected (0.00 sec)Rows matched: 2 Changed: 2 Warnings: 0 MySQL 中 localhost 127.0.0.1 区别% 是一个通配符，用以匹配所有的 IP 地址，但是不能匹配到 locahost 这个特殊的域名。 也就是说，如果要允许本地登录，单纯只配置一个 % 是不够的 (应该是说对这种方式是不够的)，需要同时配置一个 locahost 的账号。 123456789101112mysql&gt; GRANT ALL ON *.* TO 'foobar'@'%' IDENTIFIED BY '123456';Query OK, 0 rows affected (0.01 sec)mysql&gt; SELECT user, host, password FROM mysql.user WHERE user like 'foobar%';+--------+------+-------------------------------------------+| user | host | password |+--------+------+-------------------------------------------+| foobar | % | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |+--------+------+-------------------------------------------+1 row in set (0.00 sec)$ mysql -ufoobar -h127.0.0.1 -P3307 -p'123456'ERROR 1045 (28000): Access denied for user 'foobar'@'localhost' (using password: YES) https://jin-yang.github.io/post/mysql-localhost-vs-127.0.0.1-introduce.html 参考https://jin-yang.github.io/post/mysql-users.html https://www.cnblogs.com/Richardzhu/p/3318595.html","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"centos安装mysql","date":"2019-03-29T07:45:32.000Z","path":"2019/03/29/mysql/mysql安装/","text":"添加 MySQL YUM 源123456$wget &apos;https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm&apos;$sudo rpm -Uvh mysql57-community-release-el7-11.noarch.rpm$yum repolist all | grep mysqlmysql-connectors-community/x86_64 MySQL Connectors Community 36mysql-tools-community/x86_64 MySQL Tools Community 47mysql57-community/x86_64 MySQL 5.7 Community Server 187 安装MySQL12345678910111213141516171819202122232425## 安装最新版$sudo yum install mysql-community-server$ sudo yum install mysql ## 安装客户端## 安装老版本## 1. yum-config-manager$ sudo dnf config-manager --disable mysql57-community$ sudo dnf config-manager --enable mysql56-community$ yum repolist | grep mysqlmysql-connectors-community/x86_64 MySQL Connectors Community 36mysql-tools-community/x86_64 MySQL Tools Community 47mysql56-community/x86_64 MySQL 5.6 Community Server 327## 2. 直接修改 /etc/yum.repos.d/mysql-community.repo# Enable to use MySQL 5.6[mysql56-community]name=MySQL 5.6 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.6-community/el/7/$basearch/enabled=1 #表示当前版本是安装gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql57-community]name=MySQL 5.7 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/7/$basearch/enabled=0 #默认这个是 1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql 启动Mysql123456789101112$sudo service mysqld start $sudo systemctl start mysqld #CentOS 7$sudo systemctl status mysqld● mysqld.service - MySQL Community Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2017-05-27 12:56:26 CST; 15s ago Process: 2482 ExecStartPost=/usr/bin/mysql-systemd-start post (code=exited, status=0/SUCCESS) Process: 2421 ExecStartPre=/usr/bin/mysql-systemd-start pre (code=exited, status=0/SUCCESS) Main PID: 2481 (mysqld_safe) CGroup: /system.slice/mysqld.service ├─2481 /bin/sh /usr/bin/mysqld_safe --basedir=/usr └─2647 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/... 修改密码1234## 获取临时密码sudo grep &apos;temporary password&apos; /var/log/mysqld.log$ mysql -uroot -p #输入查看到的密码mysql&gt; ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;MyNewPass4!&apos;; mysql的密码存在安全等级 1shell&gt; mysql_secure_installation 1mysql&gt; SHOW VARIABLES LIKE &apos;validate_password%&apos;; validate_password_number_count参数是密码中至少含有的数字个数，当密码策略是MEDIUM或以上时生效。 validate_password_special_char_count参数是密码中非英文数字等特殊字符的个数，当密码策略是MEDIUM或以上时生效。 validate_password_mixed_case_count参数是密码中英文字符大小写的个数，当密码策略是MEDIUM或以上时生效。 validate_password_length参数是密码的长度，这个参数由下面的公式生成 validate_password_number_count+ validate_password_special_char_count+ (2 * validate_password_mixed_case_count) validate_password_dictionary_file参数是指定密码验证的字典文件路径。 validate_password_policy这个参数可以设为0、1、2，分别代表从低到高的密码强度，此参数的默认值为1，如果想将密码强度改弱，则更改此参数为0。 修改密码策略更改密码策略为LOW 1mysql&gt; set global validate_password_policy=0; 更改密码长度 1mysql&gt; set global validate_password_length=0; 安全设置1234567## 会提示设置5个关键位置## 设置 root 密码## 禁止 root 账号远程登录## 禁止匿名账号（anonymous）登录## 删除测试库## 是否确认修改$ mysql_secure_installation 安装三方插件1yum --disablerepo=\\* --enablerepo=&apos;mysql*-community*&apos; list available 修改编码123456789## /etc/my.cnf[client]default-character-set = utf8[mysqld]default-storage-engine = INNODBcharacter-set-server = utf8collation-server = utf8_general_ci #不区分大小写collation-server = utf8_bin #区分大小写collation-server = utf8_unicode_ci #比 utf8_general_ci 更准确 修改服务器时间123456789101112131415## mysql 中默认的时间戳是 UTC 时间，需要改为服务器时间的话官网提供了 3 种方式$ mysql_tzinfo_to_sql tz_dir$ mysql_tzinfo_to_sql tz_file tz_name$ mysql_tzinfo_to_sql --leap tz_file## tz_dir 代表服务器时间数据库，CentOS 7 中默认的目录为 /usr/share/zoneinfo ，tz_name 为具体的时区。如果设置的时区需要闰秒，则使用 --leap，具体的用法如下：$ mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root -p mysql$ mysql_tzinfo_to_sql tz_file tz_name | mysql -u root mysql$ mysql_tzinfo_to_sql --leap tz_file | mysql -u root mysql&gt; set global time_zone = &apos;+8:00&apos;; ##修改mysql全局时区为北京时间，即我们所在的东8区&gt; set time_zone = &apos;+8:00&apos;; ##修改当前会话时区&gt; flush privileges; #立即生效## 通过修改my.cnf配置文件来修改时区# vim /etc/my.cnf ##在[mysqld]区域中加上default-time_zone = &apos;+8:00&apos;# /etc/init.d/mysqld restart ##重启mysql使新时区生效","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"ngrok环境搭建","date":"2019-03-29T01:28:43.000Z","path":"2019/03/29/ngrok环境搭建/","text":"下载安装 配置golang环境 安装go 1yum install golang 配置GOPATH 安装git2 1234sudo yum remove gitsudo yum install epel-releasesudo yum install https://centos7.iuscommunity.org/ius-release.rpmsudo yum install git2u 下载ngrok 1go get github.com/inconshreveable/ngrok 生成证书 使用let’s encrypt证书 申请证书（具体看申请证书，主要通配符证书和三级域名） 修改证书 客户端证书 12cd ngrokcp /etc/letsencrypt/live/xncoding.com/chain.pem assets/client/tls/ngrokroot.crt 服务端证书 12cp /etc/letsencrypt/live/xncoding.com/cert.pem assets/server/tls/snakeoil.crtcp /etc/letsencrypt/live/xncoding.com/privkey.pem assets/server/tls/snakeoil.key 编译 编译服务端 1make release-server 编译客户端 不同平台的客户端需要分开编译。不同平台使用不同的 GOOS 和 GOARCH，GOOS为go编译出来的操作系统 (windows,linux,darwin)，GOARCH, 对应的构架 (386,amd64,arm) 123GOOS=linux GOARCH=amd64 make release-clientGOOS=windows GOARCH=amd64 make release-clientGOOS=linux GOARCH=arm make release-client 启动服务器在开启之前，请主要端口是否开放 1./ngrokd -domain=ngrok.sciento.top -httpAddr=:9580 -httpsAddr=:9443 -tunnelAddr=\":9444\" 启动客户端 配置文件,具体看官方文档 123456789101112server_addr: &quot;ngrok.sciento.top:9444&quot;trust_host_root_certs: falsetunnels: http: subdomain: &quot;demo&quot; proto: http: &quot;9000&quot; https: subdomain: &quot;demo&quot; proto: https: &quot;9000&quot; 启动 1./ngrok -config=ngrok.cfg start http https nginx配置 安装nginx 配置 123456789101112131415161718192021222324252627server &#123; listen 80; server_name demo.ngrok.xncoding.com; return 301 https://demo.ngrok.xncoding.com$request_uri;&#125;server &#123; listen 443 ssl http2; server_name demo.ngrok.xncoding.com; charset utf-8; ssl_certificate /etc/letsencrypt/live/demo.ngrok.xncoding.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/demo.ngrok.xncoding.com/privkey.pem; ssl_trusted_certificate /etc/letsencrypt/live/demo.ngrok.xncoding.com/chain.pem; access_log /var/log/nginx/ngrok.log main; error_log /var/log/nginx/ngrok_error.log error; location / &#123; proxy_pass http://127.0.0.1:5442; proxy_redirect off; proxy_set_header Host $http_host:5442; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; 参考https://www.xncoding.com/2017/12/29/web/ngrok.html https://www.coldawn.com/how-to-issue-acmev2-wildcard-certificates-with-certbot-on-centos-7/ https://www.jianshu.com/p/c5c9d071e395 http://ngrok.cn/docs.html#tcp","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"Druid初始化","date":"2019-03-25T10:17:33.000Z","path":"2019/03/25/Druid初始化/","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236public void init() throws SQLException &#123; if (inited) &#123; return; &#125; // bug fixed for dead lock, for issue #2980 DruidDriver.getInstance(); final ReentrantLock lock = this.lock; try &#123; lock.lockInterruptibly(); &#125; catch (InterruptedException e) &#123; throw new SQLException(\"interrupt\", e); &#125; boolean init = false; try &#123; //双重检查 if (inited) &#123; return; &#125; initStackTrace = Utils.toString(Thread.currentThread().getStackTrace()); this.id = DruidDriver.createDataSourceId(); if (this.id &gt; 1) &#123; long delta = (this.id - 1) * 100000; this.connectionIdSeedUpdater.addAndGet(this, delta); this.statementIdSeedUpdater.addAndGet(this, delta); this.resultSetIdSeedUpdater.addAndGet(this, delta); this.transactionIdSeedUpdater.addAndGet(this, delta); &#125; if (this.jdbcUrl != null) &#123; this.jdbcUrl = this.jdbcUrl.trim(); initFromWrapDriverUrl(); &#125; for (Filter filter : filters) &#123; filter.init(this); &#125; if (this.dbType == null || this.dbType.length() == 0) &#123; this.dbType = JdbcUtils.getDbType(jdbcUrl, null); &#125; if (JdbcConstants.MYSQL.equals(this.dbType) || JdbcConstants.MARIADB.equals(this.dbType) || JdbcConstants.ALIYUN_ADS.equals(this.dbType)) &#123; boolean cacheServerConfigurationSet = false; if (this.connectProperties.containsKey(\"cacheServerConfiguration\")) &#123; cacheServerConfigurationSet = true; &#125; else if (this.jdbcUrl.indexOf(\"cacheServerConfiguration\") != -1) &#123; cacheServerConfigurationSet = true; &#125; if (cacheServerConfigurationSet) &#123; this.connectProperties.put(\"cacheServerConfiguration\", \"true\"); &#125; &#125; if (maxActive &lt;= 0) &#123; throw new IllegalArgumentException(\"illegal maxActive \" + maxActive); &#125; if (maxActive &lt; minIdle) &#123; throw new IllegalArgumentException(\"illegal maxActive \" + maxActive); &#125; if (getInitialSize() &gt; maxActive) &#123; throw new IllegalArgumentException(\"illegal initialSize \" + this.initialSize + \", maxActive \" + maxActive); &#125; if (timeBetweenLogStatsMillis &gt; 0 &amp;&amp; useGlobalDataSourceStat) &#123; throw new IllegalArgumentException(\"timeBetweenLogStatsMillis not support useGlobalDataSourceStat=true\"); &#125; if (maxEvictableIdleTimeMillis &lt; minEvictableIdleTimeMillis) &#123; throw new SQLException(\"maxEvictableIdleTimeMillis must be grater than minEvictableIdleTimeMillis\"); &#125; if (this.driverClass != null) &#123; this.driverClass = driverClass.trim(); &#125; initFromSPIServiceLoader(); // 处理驱动 if (this.driver == null) &#123; if (this.driverClass == null || this.driverClass.isEmpty()) &#123; this.driverClass = JdbcUtils.getDriverClassName(this.jdbcUrl); &#125; if (MockDriver.class.getName().equals(driverClass)) &#123; driver = MockDriver.instance; &#125; else &#123; if (jdbcUrl == null &amp;&amp; (driverClass == null || driverClass.length() == 0)) &#123; throw new SQLException(\"url not set\"); &#125; driver = JdbcUtils.createDriver(driverClassLoader, driverClass); &#125; &#125; else &#123; if (this.driverClass == null) &#123; this.driverClass = driver.getClass().getName(); &#125; &#125; // 进行参数的核对，没有什么逻辑 initCheck(); // 为不同的数据库处理异常，这个可以借鉴 initExceptionSorter(); initValidConnectionChecker(); // 做了一些检查，不知道 validationQueryCheck(); // 创建数据统计对象 if (isUseGlobalDataSourceStat()) &#123; dataSourceStat = JdbcDataSourceStat.getGlobal(); if (dataSourceStat == null) &#123; dataSourceStat = new JdbcDataSourceStat(\"Global\", \"Global\", this.dbType); JdbcDataSourceStat.setGlobal(dataSourceStat); &#125; if (dataSourceStat.getDbType() == null) &#123; dataSourceStat.setDbType(this.dbType); &#125; &#125; else &#123; dataSourceStat = new JdbcDataSourceStat(this.name, this.jdbcUrl, this.dbType, this.connectProperties); &#125; dataSourceStat.setResetStatEnable(this.resetStatEnable); // 创建连接池 connections = new DruidConnectionHolder[maxActive]; evictConnections = new DruidConnectionHolder[maxActive]; keepAliveConnections = new DruidConnectionHolder[maxActive]; SQLException connectError = null; // 同步或者异步创建线程池 if (createScheduler != null &amp;&amp; asyncInit) &#123; for (int i = 0; i &lt; initialSize; ++i) &#123; createTaskCount++; CreateConnectionTask task = new CreateConnectionTask(true); this.createSchedulerFuture = createScheduler.submit(task); &#125; &#125; else if (!asyncInit) &#123; // init connections while (poolingCount &lt; initialSize) &#123; try &#123; PhysicalConnectionInfo pyConnectInfo = createPhysicalConnection(); DruidConnectionHolder holder = new DruidConnectionHolder(this, pyConnectInfo); connections[poolingCount++] = holder; &#125; catch (SQLException ex) &#123; LOG.error(\"init datasource error, url: \" + this.getUrl(), ex); if (initExceptionThrow) &#123; connectError = ex; break; &#125; else &#123; Thread.sleep(3000); &#125; &#125; &#125; if (poolingCount &gt; 0) &#123; poolingPeak = poolingCount; poolingPeakTime = System.currentTimeMillis(); &#125; &#125; // 用来打印线程池 createAndLogThread(); createAndStartCreatorThread(); // 停止 createAndStartDestroyThread(); // 等待线程创建完成 initedLatch.await(); init = true; initedTime = new Date(); // 注册mbean registerMbean(); if (connectError != null &amp;&amp; poolingCount == 0) &#123; throw connectError; &#125; // 检查连接池，防止连接池超出最大连接池 if (keepAlive) &#123; // async fill to minIdle if (createScheduler != null) &#123; for (int i = 0; i &lt; minIdle; ++i) &#123; createTaskCount++; CreateConnectionTask task = new CreateConnectionTask(true); this.createSchedulerFuture = createScheduler.submit(task); &#125; &#125; else &#123; this.emptySignal(); &#125; &#125; &#125; catch (SQLException e) &#123; LOG.error(\"&#123;dataSource-\" + this.getID() + \"&#125; init error\", e); throw e; &#125; catch (InterruptedException e) &#123; throw new SQLException(e.getMessage(), e); &#125; catch (RuntimeException e)&#123; LOG.error(\"&#123;dataSource-\" + this.getID() + \"&#125; init error\", e); throw e; &#125; catch (Error e)&#123; LOG.error(\"&#123;dataSource-\" + this.getID() + \"&#125; init error\", e); throw e; &#125; finally &#123; // 初始化成功 inited = true; // 解锁 lock.unlock(); if (init &amp;&amp; LOG.isInfoEnabled()) &#123; String msg = \"&#123;dataSource-\" + this.getID(); if (this.name != null &amp;&amp; !this.name.isEmpty()) &#123; msg += \",\"; msg += this.name; &#125; msg += \"&#125; inited\"; LOG.info(msg); &#125; &#125; &#125;","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"java多线程","date":"2019-02-15T08:37:30.000Z","path":"2019/02/15/java/java多线程/","text":"相关的类 Runnable Thread Callable:比Runnable有个返回值 Future FutureTask","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"sqlx基本使用","date":"2019-02-13T08:15:58.000Z","path":"2019/02/13/go/sqlx基本使用/","text":"安装1go get github.com/jmoiron/sqlx 连接数据库12345678910var db *sqlx.DB // exactly the same as the built-indb = sqlx.Open(\"sqlite3\", \":memory:\") // from a pre-existing sql.DB; note the required driverNamedb = sqlx.NewDb(sql.Open(\"sqlite3\", \":memory:\"), \"sqlite3\") // force a connection and test that it workederr = db.Ping() 查询Exec直接执行，适合add,update,delete 1234567891011121314schema := `CREATE TABLE place ( country text, city text NULL, telcode integer);` // execute a query on the serverresult, err := db.Exec(schema) // or, you can use MustExec, which panics on errorcityState := `INSERT INTO place (country, telcode) VALUES (?, ?)`countryCity := `INSERT INTO place (country, city, telcode) VALUES (?, ?, ?)`db.MustExec(cityState, \"Hong Kong\", 852)db.MustExec(cityState, \"Singapore\", 65)db.MustExec(countryCity, \"South Africa\", \"Johannesburg\", 27) Query查询数据库，适合select 12345678910111213141516171819202122// fetch all places from the dbrows, err := db.Query(\"SELECT country, city, telcode FROM place\") // iterate over each rowfor rows.Next() &#123; var country string // note that city can be NULL, so we use the NullString type var city sql.NullString var telcode int err = rows.Scan(&amp;country, &amp;city, &amp;telcode)&#125;// queryx 可以对结果转换成结构体var person2 User rowxs,err :=db.Queryx(\"SELECT * FROM sys_user LIMIT 1\") if err != nil&#123; panic(err) &#125; for rowxs.Next()&#123; rowxs.StructScan(&amp;person2) fmt.Println(person2) &#125; Select12345678910111213141516p := Place&#123;&#125;pp := []Place&#123;&#125; // this will pull the first place directly into perr = db.Get(&amp;p, \"SELECT * FROM place LIMIT 1\") // this will pull places with telcode &gt; 50 into the slice pperr = db.Select(&amp;pp, \"SELECT * FROM place WHERE telcode &gt; ?\", 50) // they work with regular types as wellvar id interr = db.Get(&amp;id, \"SELECT count(*) FROM place\") // fetch at most 10 place namesvar names []stringerr = db.Select(&amp;names, \"SELECT name FROM place LIMIT 10\") 事务1234// this will not work if connection pool &gt; 1db.MustExec(\"BEGIN;\")db.MustExec(...)db.MustExec(\"COMMIT;\") 预编译123456stmt, err := db.Prepare(`SELECT * FROM place WHERE telcode=?`)row = stmt.QueryRow(65) tx, err := db.Begin()txStmt, err := tx.Prepare(`SELECT * FROM place WHERE telcode=?`)row = txStmt.QueryRow(852) Named Queries12345678910111213141516171819202122232425// named query with a structp := Place&#123;Country: \"South Africa\"&#125;rows, err := db.NamedQuery(`SELECT * FROM place WHERE country=:country`, p) // named query with a mapm := map[string]interface&#123;&#125;&#123;\"city\": \"Johannesburg\"&#125;result, err := db.NamedExec(`SELECT * FROM place WHERE city=:city`, m)p := Place&#123;TelephoneCode: 50&#125;pp := []Place&#123;&#125; // select all telcodes &gt; 50nstmt, err := db.PrepareNamed(`SELECT * FROM place WHERE telcode &gt; :telcode`)err = nstmt.Select(&amp;pp, p)arg := map[string]interface&#123;&#125;&#123; \"published\": true, \"authors\": []&#123;8, 19, 32, 44&#125;,&#125;query, args, err := sqlx.Named(\"SELECT * FROM articles WHERE published=:published AND author_id IN (:authors)\", arg)query, args, err := sqlx.In(query, args...)query = db.Rebind(query)db.Query(query, args...) 参考 http://jmoiron.github.io/sqlx/","tags":[{"name":"go","slug":"go","permalink":"http://wumuwumu.github.io/tags/go/"}]},{"title":"jquery基本操作","date":"2019-01-12T08:20:45.000Z","path":"2019/01/12/jquery基本操作/","text":"选择器123456789101112// 基本选择器$('#id')$('.class')$('element')$('*')$('select1 ,select2')//可以使用css选择器// 层次选择器$('ancestor descendant')$('parent &gt;child')$('prev+next')$('prev~siblings')//获取所有同辈元素 DOM操作基本操作123456789101112131415161718192021222324252627282930313233343536// attr$('div').attr(\"background\")//获取属性$('div').attr(\"background\",\"white\")$('div').attr(&#123;\"background\":\"white\",\"height\":\"200px\"&#125;)// css$(\"div\").css('background')$('div').css(\"background\",\"white\")$('div').css(&#123;'background':'blue',\"height\":'200px'&#125;)// width heightwidth()height()// addClass$('div').addClass('className');// removeAttr$('div').removeAttr('background')// removeClass 没参数删除所有// hasClass// 创建节点var p $('&lt;p&gt;hello&lt;/p&gt;')// append() 添加内容// appendTo()// prepend() 向元素内部前面添加内容// prependTo()​``` html&lt;p&gt;hello&lt;/p&gt;​ $(‘hi!‘).prependTo(“p”)​12&lt;p&gt;&lt;i&gt;hi!&lt;/i&gt;hello&lt;/p&gt;​ // 在相应位置添加元素，是在元素的外面// after// insertAfter// before//insertBefore // remove()// detach()：和remove()几乎一样，不同的是detach方法不会删除节点所绑定的事件和附加的数据// empty() 清空内容 // clone()复制节点，可以有参数true，当有true参数时，将同时复制节点所绑定的事件// replaceWith 将匹配的节点替换成指定的节点// replaceAll() 只是用一个 // wrap 包裹节点// wrapAll// wrapInner 将匹配的节点内部的节点或者文本内容用指定的节点包裹起来​12&lt;p&gt;我是内容&lt;/p&gt;​ $(“p”).wrapInner(““);​12&lt;p&gt;&lt;span&gt;我是内容&lt;/span&gt;&lt;/p&gt;​ // html()// text()// val() // children()// next()// prev()// siblings()// closest() 获取最近的符合匹配的一个父元素​123456&lt;div&gt;&lt;div class=&quot;div2&quot;&gt;&lt;p&gt;我是内容&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;​ var $div=$(“p”).closest();//返回class为div2的div元素 // parent()// parents() // offset()// position() // scrollTop()// scrollLeft() 1234567891011121314151617181920212223242526272829303132# 事件与动画```js$().ready()$(&apos;&apos;).bind(type,func)$(&apos;&apos;).click()$(&apos;&apos;).mouseover// 合成事件hover(enter,leave)toggle(fn1,fn2) // 阻止事件event.stopPropagation();event.preventDefault();// unbind 移除事件// trigger 触发事件// 动画hide();show(time);fadeLn();fadeOut();slideUp();slideDown();slideToggle();fadeTo();fadeToggle();animate();delay(); 参考 jQuery简明参考手册——30分钟快速入门jQuery","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"},{"name":"jquery","slug":"jquery","permalink":"http://wumuwumu.github.io/tags/jquery/"}]},{"title":"springcloud-eureka","date":"2019-01-06T10:27:25.000Z","path":"2019/01/06/springcloud-eureka/","text":"建立工程 添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt;&lt;/dependency&gt; 添加Application 1234567@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication &#123; public static void main(String[] arg)&#123; SpringApplication.run(EurekaApplication.class,arg); &#125;&#125; 添加配置文件 123456789101112131415server: port: 8761eureka: instance: hostname: localhost client: registerWithEureka: false ## 是否注册到eureka server fetchRegistry: false ## 是否获取Eureka server 注册信息，单机可以设置为false serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ ## 默认http://localhost:8761/eurekaspring: application: name: eurka-server 运行工程，访问127.0.0.1:9761可以看到web界面。 安全 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; 添加配置 老版本 123456security: basic: true user: name: wumu password: wumu 新版本 1234security: user: name: wumu password: wumu 问题 在依赖包中同时添加的spring-cloud-starter-netflix-eureka-server与springb-boot-starter-web两个依赖会导致tomcat的依赖问题，应用不能启动。","tags":[{"name":"spring-cloud","slug":"spring-cloud","permalink":"http://wumuwumu.github.io/tags/spring-cloud/"}]},{"title":"Tiemline设计方案","date":"2019-01-04T12:33:08.000Z","path":"2019/01/04/Tiemline设计方案/","text":"参考 朋友圈式的TIMELINE设计方案 朋友圈的设计及实现 几个大型网站的Feeds(Timeline)设计简单对比","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"查找附近的人","date":"2019-01-04T11:46:12.000Z","path":"2019/01/04/查找附近的人/","text":"GeoHash比较原始的方法，简单方便 Mysql计算公式 12C = sin(MLatA)*sin(MLatB)*cos(MLonA-MLonB) + cos(MLatA)*cos(MLatB)Distance = R*Arccos(C)*Pi/180 在经纬度小节中我们了解了两个公式用来计算两个位置之间的距离, 该小节我们以测试数据说明如何实现.测试需要的表结构和数据: 1234567891011121314151617表结构:CREATE TABLE `geotest` ( `userid` int(10) NOT NULL, `longitude` decimal(9,6) NOT NULL, `latitude` decimal(9,6) NOT NULL, `create_time` datetime DEFAULT NULL, UNIQUE KEY `unq_uid` (`userid`), KEY `idx_lat_lng` (`longitude`,`latitude`)) ENGINE=InnoDB DEFAULT CHARSET=utf8测试数据:insert geotest values(10000, 116.417480, 40.003033, now());insert geotest values(10001, 116.437480, 40.004033, now());insert geotest values(10002, 116.457480, 40.005033, now());insert geotest values(10003, 116.477480, 40.006033, now());............ 第一种公式中, google 为我们介绍了如何使用 sql 来获取附近的点, 如下所示, 我们选用 6371km 作为地球的半径,根据上述小节的计算公式推断: 12C = sin(MLatA)*sin(MLatB)*cos(MLonA-MLonB) + cos(MLatA)*cos(MLatB)Distance = R*Arccos(C)*Pi/180 google 地图的计算公式可以参考 geo_search 两个位置之间的距离则可以换算成以下公式: 1R*arccos( cos( radians(latA)*cos( radians(latB) ) * cos( radians(lonA - lonB) )) + sin( radians(latA)*cos(latB) )) radians 函数计算出相应的弧度信息, 得到下面的 sql: 1234567891011121314SELECT user_id, ( 6371 * acos ( cos ( radians(40.003033) ) * cos( radians( latitude ) ) * cos( radians( longitude ) - radians(116.417481) ) + sin ( radians(40.003033) ) * sin( radians( latitude ) ) ) ) AS distanceFROM geotestHAVING distance &lt; 1ORDER BY distanceLIMIT 0 , 20; 上面的 sql 从 geotest 中从 geotest 表中获取到经度(116.417481) 和纬度(40.003033) 位置附近 1km 所有的user_id 信息.观察这个 sql, 可以预见到在表数据较大的时候仅建立复合索引 idx_lat_lng 肯定会遇到性能瓶颈, 因为每行记录都需要做相关的运算, 才能跑出最后的结果. 所以要提高该 sql 的性能就需要尽量过滤不需要的 longitude 和 latitude 两列的值. 参考 geo_search 和 fastest-way-to-find-distance, 在近距离的情况下我们可以认为当前区域内的所有位置都在一个平面内, 虽然有点误差, 但是比起地球这么大的椭球, 我们完全可以忽略其中的误差. 以经纬度来讲, 1 纬度约等于 69 英里, 大约 111044.736 米, 其中的换算公式为: 121°latitude ~= 69 miles1°longitude ~= cos(latitude)*69 miles 所以对于位置信息(lng, lat), 我们可以计算出以其为中心周边指定距离的四个点, 如下图所示: 1234567+-------------+| || || + || || |+-------------+ 计算公式如下: 1234lng1 = lon - dist/abs(cos(radians(lat))*69)lng2 = lon + dist/abs(cos(radians(lat))*69)lat1 = lat - (dist/69);lat2 = lat + (dist/69); 四个点的坐标就分别为 (lng1, lat1), (lng1, lat2), (lng2, lat1), (lng2, lat2), 所以存在于该四个点组成的平面之间的点即可以被认为在(lng, lat) 的 dist 距离内. 基于上述的规则, 修改 sql 为以下: 12345678910111213141516SELECT user_id, ( 6371 * acos ( cos ( radians(40.003033) ) * cos( radians( latitude ) ) * cos( radians( longitude ) - radians(116.417481) ) + sin ( radians(40.003033) ) * sin( radians( latitude ) ) ) ) AS distanceFROM geotestWHERE longitude BETWEEN lng1 AND lng2AND latitude BETWEEN lat1 AND lat2HAVING distance &lt; 1ORDER BY distanceLIMIT 0 , 20; 这样就能很好的使用索引, 如果还想增加超时设置, 可以在 sql 里加上 create_time 条件进行过滤, 比如只查找最近一天的附近的用户. 另外开发者也可以结合使用 sphinx 或 elasticsearch 得到更好的性能. 下面为根据上面介绍的规则整理成存储过程, 方便开发者调用访问. 这里我们将地球半径的公里数转换为米即为 6371392.89m, 69英里则转为 111044.736m, 如下存储过程返回 user_id 和 距离(米): 12345678910111213141516171819202122232425262728293031DELIMITER $$drop procedure if exists geo_dist$$create procedure geo_dist(IN lng decimal(9, 6), IN lat decimal(9, 6), IN dist int)begin declare lng1 decimal(9, 6); declare lng2 decimal(16, 13); declare lat1 decimal(9, 6); declare lat1 decimal(16, 13); -- calculate lng and lat for the rectangle, in meters unit set lng1 = lng - dist/abs(cos(radians(lat))*111044.736); set lng2 = lng + dist/abs(cos(radians(lat))*111044.736); set lat1 = lat - (dist/111044.736); set lat2 = lat + (dist/111044.736); -- run the query select user_id, round(( 6371392.89 * acos ( cos ( radians(lat) ) * cos( radians( latitude ) ) * cos( radians( longitude ) - radians(lng) ) + sin ( radians(lat) ) * sin( radians( latitude ) ) ) ), 0) AS distance from user_position where lng between lng1 and lng2 and lat between lat1 and lat2 having distance &lt; dist ORDER BY distance LIMIT 0 , 20;END$$DELIMITER ; 运行存储过程, 取出该经纬度下附近 5km 的用户和距离(m): 1234567891011mysql &gt; call geo_dist(116.4174800000000, 40.0030330000000, 5000);+---------+----------+| user_id | distance |+---------+----------+| 10000 | 0 || 10001 | 1707 || 10002 | 3414 |+---------+----------+3 rows in set (0.00 sec)Query OK, 0 rows affected (0.01 sec) 10001 用户和指定的经纬度距离为1707米, 我们在 redis 3.2 版本中进行简单测试, 可以看到结果都很相近: 123456127.0.0.1:6380&gt; geoadd tttt 116.417480 40.003033 t1(integer) 0127.0.0.1:6380&gt; geoadd tttt 116.437481 40.004034 t2(integer) 0127.0.0.1:6380&gt; GEODIST tttt t1 t2&quot;1707.5093&quot; mongodb创建位置索引 参考 使用 MySQL 实现搜索附近的人 GeoHash算法学习讲解、解析及原理分析","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"},{"name":"mongodb","slug":"mongodb","permalink":"http://wumuwumu.github.io/tags/mongodb/"},{"name":"mysql","slug":"mysql","permalink":"http://wumuwumu.github.io/tags/mysql/"}]},{"title":"springboot-mongodb使用","date":"2019-01-04T01:43:06.000Z","path":"2019/01/04/springboot-mongodb使用/","text":"基本注解 @id @Document @DBRef $Indexed @CompoundIndex @GenSpatialIndexed @Transient @PersistenceConstructor","tags":[{"name":"springboot","slug":"springboot","permalink":"http://wumuwumu.github.io/tags/springboot/"},{"name":"mongodb","slug":"mongodb","permalink":"http://wumuwumu.github.io/tags/mongodb/"}]},{"title":"cordova打包vue","date":"2019-01-02T09:12:09.000Z","path":"2019/01/02/cordova打包vue/","text":"https://segmentfault.com/a/1190000013159076","tags":[{"name":"vue","slug":"vue","permalink":"http://wumuwumu.github.io/tags/vue/"}]},{"title":"Oracle表管理","date":"2018-12-29T13:45:47.000Z","path":"2018/12/29/oracle/Oracle表管理/","text":"数据类型123456789101112131415## 字符型char 定长，后面空格补全varchar2() 变长clob 字符型大对象## 数字类型numbernumber(5，2) 标识5位有效数，2位小数-999.99-999.99number(5) 5位整数## 日期类型datetimestramp## 图片blob 二进制4g,为了安全可以放入数据库 表操作12345678910create table table_name()drop table table_name;rename table_name to other_table_name;alter table table_name add ...;alter table table_name modify ...;alter table table_name drop column ...;","tags":[{"name":"oracle","slug":"oracle","permalink":"http://wumuwumu.github.io/tags/oracle/"}]},{"title":"Oracle基本管理","date":"2018-12-29T12:37:37.000Z","path":"2018/12/29/oracle/Oracle基本管理/","text":"用户管理12345678910111213141516171819202122232425## 创建用户create user test identified by test;show user;## 删除用户delete test (cascade);## 修改用户alter user test identified by wumu;alter user test expire;## 用户口令## 密码输错三次就密码锁定2天create profile lock_account limit failed_login_attempts 3 password_lock_time 2;alter user tea profile lock_account;## 解锁alter user tea account unlock;## 每10天需要修改密码，宽限期为两天create profile myprofile limit password_life_time 10 password_grace_time 2;alter user tea profile myprofile;## 口令10天后可以重用create profile password_history limit password_lift_time 10 password_grace_time 2 password_reuse_time 10## 撤销profiledrop profile my_profile CASCADE； 权限管理12345678910111213141516171819202122232425## 授权grant system_privilege|all privileges to &#123;user identified by password |role|&#125;[with admin option]grant object_privileage | Allon schema.objectto user | role[with admin option][with the grant any object]grant select on test to wumu with grant option;grant connect tp wumu with admin option;## create session 用于登录## dba 管路员## resource 可以建表## desc table_name## 撤销权限## 如果授权者的权限被撤回，那么它的被授予者也会失去相关的权限invoke system_privilege from user|roleinvoke object_privilege|All on scheme.object from user|role [cascade contraints]## 查询权限## 系统权限放在DBA_SYS_PRIVS## 对象权限放在数据字典DBA_TAB_PRIVS","tags":[{"name":"oracle","slug":"oracle","permalink":"http://wumuwumu.github.io/tags/oracle/"}]},{"title":"Vue插件开发","date":"2018-12-17T12:52:34.000Z","path":"2018/12/17/Vue插件开发/","text":"基本结构插件的功能包括全局方法和属性、指令、mixin、实例方法。插件都有一个install方法，第一个参数是Vue，第二个参数是options。 1234567891011121314151617181920MyPlugin.install = function (Vue, options) &#123; Vue.myGlobalMethod = function () &#123; // 1. 添加全局方法或属性，如: vue-custom-element // 逻辑... &#125; Vue.directive('my-directive', &#123; // 2. 添加全局资源：指令/过滤器/过渡等，如 vue-touch bind (el, binding, vnode, oldVnode) &#123; // 逻辑... &#125; ... &#125;) Vue.mixin(&#123; created: function () &#123; // 3. 通过全局 mixin方法添加一些组件选项，如: vuex // 逻辑... &#125; ... &#125;) Vue.prototype.$myMethod = function (options) &#123; // 4. 添加实例方法，通过把它们添加到 Vue.prototype 上实现 // 逻辑... &#125;&#125; vue-toast","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"},{"name":"vue","slug":"vue","permalink":"http://wumuwumu.github.io/tags/vue/"}]},{"title":"JSqlParser教程","date":"2018-12-05T13:58:31.000Z","path":"2018/12/05/JSqlParser教程/","text":"解析获取表名12345//获取所有使用过的表Statement statement = CCJSqlParserUtil.parse(\"SELECT * FROM MY_TABLE1\"); Select selectStatement = (Select) statement; TablesNamesFinder tablesNamesFinder = new TablesNamesFinder(); List&lt;String&gt; tableList = tablesNamesFinder.getTableList(selectStatement); 应用别名1234// SELECT a AS A1, b AS A2, c AS A3 FROM testSelect select = (Select) CCJSqlParserUtil.parse(\"select a,b,c from test\"); final AddAliasesVisitor instance = new AddAliasesVisitor(); select.getSelectBody().accept(instance); 添加一列或者表达式123// SELECT a, b FROM mytableSelect select = (Select) CCJSqlParserUtil.parse(\"select a from mytable\");SelectUtils.addExpression(select, new Column(\"b\")); 添加where语句新建where12345678Select select = (Select) CCJSqlParserUtil.parse(\"select name from user\"); PlainSelect plainSelect = (PlainSelect) select.getSelectBody(); if (plainSelect.getWhere() == null) &#123; EqualsTo equalsTo = new EqualsTo(); equalsTo.setLeftExpression(new Column(\"id\")); equalsTo.setRightExpression(new LongValue(1000L)); plainSelect.setWhere(equalsTo); &#125; 添加where12345678910111213Select select = (Select) CCJSqlParserUtil.parse(\"select name from user where id = 1000\"); PlainSelect plainSelect = (PlainSelect) select.getSelectBody(); // 原where表达式 Expression where = plainSelect.getWhere(); // 新增的查询条件表达式 EqualsTo equalsTo = new EqualsTo(); equalsTo.setLeftExpression(new Column(\"name\")); equalsTo.setRightExpression(new StringValue(\"'张三'\")); // 用and链接条件 AndExpression and = new AndExpression(where, equalsTo); // 设置新的where条件 plainSelect.setWhere(and); 添加null12345678910111213Select select = (Select) CCJSqlParserUtil.parse(\"select name from user where id = 1000\"); PlainSelect plainSelect = (PlainSelect) select.getSelectBody(); // 原where表达式 Expression where = plainSelect.getWhere(); // 新增的null判断条件 IsNullExpression isNullExpression = new IsNullExpression(); isNullExpression.setLeftExpression(new Column(\"name\")); isNullExpression.setNot(true); // 用and链接条件 AndExpression and = new AndExpression(where, isNullExpression); // 设置新的where条件 plainSelect.setWhere(and); 生成扩展插入123456789101112131415161718192021222324// INSERT INTO mytable (col1) VALUES (1)// INSERT INTO mytable (col1, col2) VALUES (1, 5)// INSERT INTO mytable (col1, col2, col3) VALUES (1, 5, 10)Insert insert = (Insert) CCJSqlParserUtil.parse(\"insert into mytable (col1) values (1)\"); System.out.println(insert.toString()); insert.getColumns().add(new Column(\"col2\")); insert.getItemsList().accept(new ItemsListVisitor() &#123; public void visit(SubSelect subSelect) &#123; throw new UnsupportedOperationException(\"Not supported yet.\"); &#125; public void visit(ExpressionList expressionList) &#123; expressionList.getExpressions().add(new LongValue(5)); &#125; public void visit(MultiExpressionList multiExprList) &#123; throw new UnsupportedOperationException(\"Not supported yet.\"); &#125; &#125;); System.out.println(insert.toString()); insert.getColumns().add(new Column(\"col3\")); ((ExpressionList) insert.getItemsList()).getExpressions().add(new LongValue(10)); 建立select12345Select select = SelectUtils.buildSelectFromTable(new Table(\"mytable\"));Select select = SelectUtils.buildSelectFromTableAndExpressions(new Table(\"mytable\"), new Column(\"a\"), new Column(\"b\"));Select select = SelectUtils.buildSelectFromTableAndExpressions(new Table(\"mytable\"), \"a+b\", \"test\"); 代替字符串的值1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677String sql =\"SELECT NAME, ADDRESS, COL1 FROM USER WHERE SSN IN ('11111111111111', '22222222222222');\";Select select = (Select) CCJSqlParserUtil.parse(sql);//Start of value modificationStringBuilder buffer = new StringBuilder();ExpressionDeParser expressionDeParser = new ExpressionDeParser() &#123; @Override public void visit(StringValue stringValue) &#123; this.getBuffer().append(\"XXXX\"); &#125; &#125;;SelectDeParser deparser = new SelectDeParser(expressionDeParser,buffer );expressionDeParser.setSelectVisitor(deparser);expressionDeParser.setBuffer(buffer);select.getSelectBody().accept(deparser);//End of value modificationSystem.out.println(buffer.toString());//Result is: SELECT NAME, ADDRESS, COL1 FROM USER WHERE SSN IN (XXXX, XXXX)import net.sf.jsqlparser.JSQLParserException;import net.sf.jsqlparser.expression.LongValue;import net.sf.jsqlparser.expression.StringValue;import net.sf.jsqlparser.parser.CCJSqlParserUtil;import net.sf.jsqlparser.statement.Statement;import net.sf.jsqlparser.util.deparser.ExpressionDeParser;import net.sf.jsqlparser.util.deparser.SelectDeParser;import net.sf.jsqlparser.util.deparser.StatementDeParser;public class ReplaceColumnValues &#123; static class ReplaceColumnAndLongValues extends ExpressionDeParser &#123; @Override public void visit(StringValue stringValue) &#123; this.getBuffer().append(\"?\"); &#125; @Override public void visit(LongValue longValue) &#123; this.getBuffer().append(\"?\"); &#125; &#125; public static String cleanStatement(String sql) throws JSQLParserException &#123; StringBuilder buffer = new StringBuilder(); ExpressionDeParser expr = new ReplaceColumnAndLongValues(); SelectDeParser selectDeparser = new SelectDeParser(expr, buffer); expr.setSelectVisitor(selectDeparser); expr.setBuffer(buffer); StatementDeParser stmtDeparser = new StatementDeParser(expr, selectDeparser, buffer); Statement stmt = CCJSqlParserUtil.parse(sql); stmt.accept(stmtDeparser); return stmtDeparser.getBuffer().toString(); &#125; public static void main(String[] args) throws JSQLParserException &#123; System.out.println(cleanStatement(\"SELECT 'abc', 5 FROM mytable WHERE col='test'\")); System.out.println(cleanStatement(\"UPDATE table1 A SET A.columna = 'XXX' WHERE A.cod_table = 'YYY'\")); System.out.println(cleanStatement(\"INSERT INTO example (num, name, address, tel) VALUES (1, 'name', 'test ', '1234-1234')\")); System.out.println(cleanStatement(\"DELETE FROM table1 where col=5 and col2=4\")); &#125;&#125;/*SELECT ?, ? FROM mytable WHERE col = ?UPDATE table1 A SET A.columna = ? WHERE A.cod_table = ?INSERT INTO example (num, name, address, tel) VALUES (?, ?, ?, ?)DELETE FROM table1 WHERE col = ? AND col2 = ?*/ 参考 https://github.com/JSQLParser/JSqlParser/wiki","tags":[{"name":"java","slug":"java","permalink":"http://wumuwumu.github.io/tags/java/"}]},{"title":"react入门教程","date":"2018-12-05T13:56:22.000Z","path":"2018/12/05/react入门教程/","text":"webpack4初始化12cnpm i -D webpackcnpm i -D webpack-cli //相关的命令 相应包的安装 react 专门用于创建组件和虚拟DOM，同事组件的生命周期在这个包中。 react-dom 专门进行dom操作的，最主要的应用场景，就是ReactDom.render() babel babel-node 一个命令行工具 babel-register 可以实现动态转换 babel-core 核心包 babel-preset-env 一个套餐 jsx使用安装babel插件123cnpm i babel-core babel-loader babel-plugin-transform-runtime -Dcnpm i babel-preset-env babel-preset-stage-0 -Dcnpm i babel-preset-react -D 添加.babelrc配置文件1234&#123; &quot;presets&quot;:[&quot;env&quot;,&quot;stage-0&quot;,&quot;react&quot;], &quot;plugins&quot;:[&quot;transform-runtime&quot;]&#125; ##添加babel-loader配置项 12345module：&#123; rules:[ &#123;test:/\\.js|jsx/,use:&apos;babel-loader&apos;,exclude:/node_modules/&#125; ]&#125;","tags":[{"name":"react","slug":"react","permalink":"http://wumuwumu.github.io/tags/react/"}]},{"title":"nginx配置","date":"2018-12-05T13:47:32.000Z","path":"2018/12/05/nginx/nginx配置/","text":"配置web服务器1234567891011server &#123; listen 80; server_name api.lufficc.com *.lufficc.com; location /images/ &#123; root /data; &#125; location / &#123; proxy_pass https://lufficc.com; &#125;&#125; 反向代理1234567server&#123; listen 80; server_name search.lufficc.com; location / &#123; proxy_pass https://www.baidu.com; &#125;&#125; 参考 https://lufficc.com/blog/configure-nginx-as-a-web-server https://blog.csdn.net/hj7jay/article/details/53905943 http://www.nginx.cn/76.html","tags":[{"name":"web","slug":"web","permalink":"http://wumuwumu.github.io/tags/web/"},{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"centos7修改网卡","date":"2018-12-05T13:40:23.000Z","path":"2018/12/05/centos7修改网卡/","text":"修改mac使用virtualbox导入一个虚拟机时mac地址是一样的，此时需要修改。 修改mac地址直接在virtualBox的setting&gt;network配置中进行修改。 修改网卡名称修改网卡的配置文件123vim /etc/sysconfig/network-scripts/ifcfg-eno16777736 //修改NAME，DEVICE 成希望的（不要加ifcfg）mv ifcfg-eno16777736 ifcfg-eth0 //修改配置文件的名字 禁用可预测命名规则1vim /etc/default/grub 添加内核参数： net.ifnames=0 biosdevname=0 12345678[root@ansheng network-scripts]# vi /etc/default/grubGRUB_TIMEOUT=5GRUB_DISTRIBUTOR=&quot;$(sed &apos;s, release .*$,,g&apos; /etc/system-release)&quot;GRUB_DEFAULT=savedGRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT=&quot;console&quot;GRUB_CMDLINE_LINUX=&quot;rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet net.ifnames=0 biosdevname=0&quot;GRUB_DISABLE_RECOVERY=&quot;true&quot; 用 grub2-mkconfig 命令重新生成GRUB配置并更新内核1234567[root@ansheng network-scripts]# grub2-mkconfig -o /boot/grub2/grub.cfgGenerating grub configuration file ...Found linux image: /boot/vmlinuz-3.10.0-327.el7.x86_64Found initrd image: /boot/initramfs-3.10.0-327.el7.x86_64.imgFound linux image: /boot/vmlinuz-0-rescue-4dd6b54f74c94bff9e92c61d669fc195Found initrd image: /boot/initramfs-0-rescue-4dd6b54f74c94bff9e92c61d669fc195.imgdone 重启系统","tags":[{"name":"linux","slug":"linux","permalink":"http://wumuwumu.github.io/tags/linux/"}]},{"title":"nvc-server安装","date":"2018-12-05T13:39:00.000Z","path":"2018/12/05/nvc-server安装/","text":"centos 安装 vnc server没有实现多用户配置 1234567891011121314151617181920yum install tigervnc-server -ycp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver@:1.service## 然后打开这个配置文件/etc/systemd/system/vncserver@:1.service替换掉默认用户名 ## ExecStart=/sbin/runuser -l &lt;USER&gt; -c &quot;/usr/bin/vncserver %i&quot;## PIDFile=/home/&lt;USER&gt;/.vnc/%H%i.pid## 这里我直接用root 用户登录，所以我替换成ExecStart=/sbin/runuser -l root -c &quot;/usr/bin/vncserver %i&quot;PIDFile=/root/.vnc/%H%i.pid## 如果是其他用户的话比如linoxide替换如下ExecStart=/sbin/runuser -l linoxide -c &quot;/usr/bin/vncserver %i&quot;PIDFile=/home/linoxide/.vnc/%H%i.pidsystemctl daemon-reloadvncpasswd## 开放端口## 重启服务systemctl enable vncserver@:1.servicesystemctl start vncserver@:1.service ubuntu 安装 vnc viewer1sudo dpkg -i VNC-Viewer-6.17.1113-Linux-x64.deb 参考 https://my.oschina.net/huhaoren/blog/497394","tags":[{"name":"web","slug":"web","permalink":"http://wumuwumu.github.io/tags/web/"}]},{"title":"VirtualBox磁盘扩容","date":"2018-12-05T13:36:46.000Z","path":"2018/12/05/VirtualBox磁盘扩容/","text":"扩展磁盘文件VDI1VBoxManage modifyhd centos.vdi --resize 16000 # 单位M VMDK123VBoxManage clonehd &quot;centos.vmdk&quot; &quot;centos.vdi&quot; --format vdi # vmdk是转换前的文件，vdi是转换之后的文件VBoxManage modifyhd &quot;centos.vdi&quot; --resize 16000 # 这里的单位是MVBoxManage clonehd &quot;centos.vdi&quot; &quot;resized.vmdk&quot; --format vmdk #可以再转回来 使用克隆本人在使用的时候，前面两种方式不能实现，采用第三种方式 12VBoxManage createhd -filename centos7-main-64g -size 65536 -format VDI -variant Standard # 创建一个新的磁盘，磁盘大小为想要的大小VBoxManage clonemedium ../centos7-main\\ Clone/centos7-main\\ Clone.vdi centos7-main-64g.vdi --existing # 将原有的磁盘复制到新磁盘上 磁盘扩容这里可以使用gparted进行磁盘的扩容 下载gparted-live镜像 设置iso镜像开机启动 进行分区的修改 LVM扩容如果你没有使用逻辑卷就可以跳过这节。如果使用逻辑卷也可以通过添加新磁盘的形式对文件系统进行扩容，这种方式更加简单方便。 创建PE、VG扩展LV12sudo vgextend VolGroup /dev/sda4 # 通过新卷的方式扩展到卷组lvresize -l +122 /dev/centos/root # 直接扩容 刷新逻辑分区容量1xfs_growfs /devices/centos/root # resize2fs是不能成功的","tags":[{"name":"web","slug":"web","permalink":"http://wumuwumu.github.io/tags/web/"}]},{"title":"lorawan协议（中文版）","date":"2018-12-05T13:34:09.000Z","path":"2018/12/05/lorawan协议（中文版）/","text":"介绍网关和服务器之间的协议是有目的的非常基本的，仅用于演示目的，或用于私有和可靠的网络。 这里没有网关或服务器的认证，并且确认仅用于网络质量评估，而不是 纠正UDP数据报丢失（无重试）。 系统原理和相关定义1234567891011121314151617 ((( Y ))) | |+ - -|- - - - - - - - - - - - - + xxxxxxxxxxxx +--------+| +--+-----------+ +------+ | xx x x xxx | || | | | | | xx Internet xx | || | Concentrator |&lt;---&gt;| Host |&lt;-------xx or xx--------&gt;| || | | SPI | | | xx Intranet xx | Server || +--------------+ +------+ | xxxx x xxxx | || ^ ^ | xxxxxxxx | || | PPS +-------+ NMEA | | | || +-----| GPS |-------+ | +--------+| | (opt) | || +-------+ || || Gateway |+- - - - - - - - - - - - - - - -+ 网关：无线电RX / TX板，基于Semtech多通道调制解调器（SX130x），收发器（SX135x）和/或低功耗独立调制解调器（SX127x）。 主机：运行包转发器的嵌入式计算机。通过SPI链路驱动集中器。 GPS：具有“每秒1脉冲”的GNSS（GPS，伽利略，GLONASS等）接收器 输出和到主机的串行链接，以发送包含时间和地理坐标数据的NMEA帧。可选的。 网关：由至少一个无线电集中器，主机，一些组成的设备网络连接到互联网或专用网络（以太网，3G，Wifi，微波链路），以及可选的GPS接收器进行同步。 服务器：一种抽象计算机，它将处理由网关接收和转发的RF数据包，并发出RF数据包以响应网关必须发出的数据包。 假设网关可以在NAT后面或防火墙停止任何传入连接。 假设服务器具有静态IP地址（或通过DNS服务可解决的地址），并且能够接收特定端口上的传入连接。 上行协议3.1 时序图 12345678910111213141516+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | -----------------------------------\\ | |-| When 1-N RF packets are received | | | ------------------------------------ | | | | PUSH_DATA (token X, GW MAC, JSON payload) | |-------------------------------------------------------------&gt;| | | | PUSH_ACK (token X) | |&lt;-------------------------------------------------------------| | ------------------------------\\ | | | process packets *after* ack |-| | ------------------------------- | | | PUSH_DATA 包网关使用该数据包类型主要是将所接收的RF分组和相关联的元数据转发到服务器。 字节 功能 0 协议版本2 1-2 随机凭证 3 PUSH_DATA标识0x00 4-11 网关唯一标识（MAC地址） 12-结束 JSON对象，看第4章 PUSH_ACK包服务器使用该数据包类型立即确认收到的所有PUSH_DATA数据包。 字节 功能 0 协议版本2 1-2 与PUSH_DATA包中相同的凭证，用于确认 3 PUSH_ACK标识0x01 上行JSON数据结构根对象包含名为&quot;rxpk&quot;的数组： 123&#123; &quot;rxpk&quot;:[ &#123;...&#125;, ...]&#125; 该数组包含至少一个JSON对象，每个对象包含一个RF数据包以及包含以下字段的关联元数据： 名称 类别 功能 time string UTC time of pkt RX, us precision, ISO 8601 ‘compact’ format tmst number Internal timestamp of “RX finished” event (32b unsigned) freq number RX central frequency in MHz (unsigned float, Hz precision) chan number Concentrator “IF” channel used for RX (unsigned integer) rfch number Concentrator “RF chain” used for RX (unsigned integer) stat number CRC status: 1 = OK, -1 = fail, 0 = no CRC modu string Modulation identifier “LORA” or “FSK” datr string LoRa datarate identifier (eg. SF12BW500) datr number FSK datarate (unsigned, in bits per second) codr string LoRa ECC coding rate identifier rssi number RSSI in dBm (signed integer, 1 dB precision) lsnr number Lora SNR ratio in dB (signed float, 0.1 dB precision) size number RF packet payload size in bytes (unsigned integer) data string Base64 encoded RF packet payload, padded 示例（为了便于阅读而添加了空格，缩进和换行符）： 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123;&quot;rxpk&quot;:[ &#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.528002Z&quot;, &quot;tmst&quot;:3512348611, &quot;chan&quot;:2, &quot;rfch&quot;:0, &quot;freq&quot;:866.349812, &quot;stat&quot;:1, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF7BW125&quot;, &quot;codr&quot;:&quot;4/6&quot;, &quot;rssi&quot;:-35, &quot;lsnr&quot;:5.1, &quot;size&quot;:32, &quot;data&quot;:&quot;-DS4CGaDCdG+48eJNM3Vai-zDpsR71Pn9CPA9uCON84&quot; &#125;,&#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.530974Z&quot;, &quot;tmst&quot;:3512348514, &quot;chan&quot;:9, &quot;rfch&quot;:1, &quot;freq&quot;:869.1, &quot;stat&quot;:1, &quot;modu&quot;:&quot;FSK&quot;, &quot;datr&quot;:50000, &quot;rssi&quot;:-75, &quot;size&quot;:16, &quot;data&quot;:&quot;VEVTVF9QQUNLRVRfMTIzNA==&quot; &#125;,&#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.532038Z&quot;, &quot;tmst&quot;:3316387610, &quot;chan&quot;:0, &quot;rfch&quot;:0, &quot;freq&quot;:863.00981, &quot;stat&quot;:1, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF10BW125&quot;, &quot;codr&quot;:&quot;4/7&quot;, &quot;rssi&quot;:-38, &quot;lsnr&quot;:5.5, &quot;size&quot;:32, &quot;data&quot;:&quot;ysgRl452xNLep9S1NTIg2lomKDxUgn3DJ7DE+b00Ass&quot; &#125;]&#125; 根对象还可以包含名为&quot;stat&quot;的对象： 1234&#123; &quot;rxpk&quot;:[ &#123;...&#125;, ...], &quot;stat&quot;:&#123;...&#125;&#125; 数据包可能不包含&quot;rxpk&quot;数组而是“stat”对象。 123&#123; &quot;stat&quot;:&#123;...&#125;&#125; 该对象包含网关的状态，包含以下字段： 名称 类型 功能 time string UTC ‘system’ time of the gateway, ISO 8601 ‘expanded’ format lati number GPS latitude of the gateway in degree (float, N is +) long number GPS latitude of the gateway in degree (float, E is +) alti number GPS altitude of the gateway in meter RX (integer) rxnb number Number of radio packets received (unsigned integer) rxok number Number of radio packets received with a valid PHY CRC rxfw number Number of radio packets forwarded (unsigned integer) ackr number Percentage of upstream datagrams that were acknowledged dwnb number Number of downlink datagrams received (unsigned integer) txnb number Number of packets emitted (unsigned integer) 示例（为了便于阅读而添加了空格，缩进和换行符）： 123456789101112&#123;&quot;stat&quot;:&#123; &quot;time&quot;:&quot;2014-01-12 08:59:28 GMT&quot;, &quot;lati&quot;:46.24000, &quot;long&quot;:3.25230, &quot;alti&quot;:145, &quot;rxnb&quot;:2, &quot;rxok&quot;:2, &quot;rxfw&quot;:2, &quot;ackr&quot;:100.0, &quot;dwnb&quot;:2, &quot;txnb&quot;:2&#125;&#125; 下行协议时序图1234567891011121314151617181920212223242526+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | -----------------------------------\\ | |-| Every N seconds (keepalive time) | | | ------------------------------------ | | | | PULL_DATA (token Y, MAC@) | |-------------------------------------------------------------&gt;| | | | PULL_ACK (token Y) | |&lt;-------------------------------------------------------------| | |+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | ------------------------------------------------------\\ | | | Anytime after first PULL_DATA for each packet to TX |-| | ------------------------------------------------------- | | | | PULL_RESP (token Z, JSON payload) | |&lt;-------------------------------------------------------------| | | | TX_ACK (token Z, JSON payload) | |-------------------------------------------------------------&gt;| PULL_DATA包网关使用该数据包类型来轮询来自服务器的数据。 此数据交换由网关初始化，因为如果网关位于NAT后面，服务器可能无法将数据包发送到网关。 当网关初始化交换机时，将打开通向服务器的网络路由，并允许数据包在两个方向上流动。 网关必须定期发送PULL_DATA数据包，以确保网络路由保持打开状态，以便服务器随时使用。 Bytes Function 0 protocol version = 2 1-2 random token 3 PULL_DATA identifier 0x02 4-11 Gateway unique identifier (MAC address) PULL_ACK 包服务器使用该数据包类型来确认网络路由是否已打开，以及服务器是否可以随时发送PULL_RESP数据包。 Bytes Function 0 protocol version = 2 1-2 same token as the PULL_DATA packet to acknowledge 3 PULL_ACK identifier 0x04 PULL_RESP 包服务器使用该数据包类型来发送必须由网关发出的RF数据包和相关元数据。 Bytes Function 0 protocol version = 2 1-2 random token 3 PULL_RESP identifier 0x03 4-end JSON object, starting with {, ending with }, see section 6 TX_ACK 包网关使用该分组类型向服务器发送反馈，以通知网关是否已接受或拒绝下行链路请求。 数据报可以选项包含一个JSON字符串，以提供有关acknoledge的更多详细信息。 如果没有JSON（空字符串），这意味着没有发生错误。 Bytes Function 0 protocol version = 2 1-2 same token as the PULL_RESP packet to acknowledge 3 TX_ACK identifier 0x05 4-11 Gateway unique identifier (MAC address) 12-end [optional] JSON object, starting with {, ending with }, see section 6 下行JSON数据结构 PULL_RESP数据包的根对象必须包含名为“txpk”的对象： 123&#123; &quot;txpk&quot;: &#123;...&#125;&#125; 该对象包含要发出的RF数据包以及与以下字段相关联的元数据： Name Type Function imme bool Send packet immediately (will ignore tmst &amp; time) tmst number Send packet on a certain timestamp value (will ignore time) time string Send packet at a certain time (GPS synchronization required) freq number TX central frequency in MHz (unsigned float, Hz precision) rfch number Concentrator “RF chain” used for TX (unsigned integer) powe number TX output power in dBm (unsigned integer, dBm precision) modu string Modulation identifier “LORA” or “FSK” datr string LoRa datarate identifier (eg. SF12BW500) datr number FSK datarate (unsigned, in bits per second) codr string LoRa ECC coding rate identifier fdev number FSK frequency deviation (unsigned integer, in Hz) ipol bool Lora modulation polarization inversion prea number RF preamble size (unsigned integer) size number RF packet payload size in bytes (unsigned integer) data string Base64 encoded RF packet payload, padding optional ncrc bool If true, disable the CRC of the physical layer (optional) 大多数字段都是可选的。如果省略字段，将使用默认参数。 示例（为便于阅读而添加了空格，缩进和换行符）： 1234567891011121314151617181920212223&#123;&quot;txpk&quot;:&#123; &quot;imme&quot;:true, &quot;freq&quot;:864.123456, &quot;rfch&quot;:0, &quot;powe&quot;:14, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF11BW125&quot;, &quot;codr&quot;:&quot;4/6&quot;, &quot;ipol&quot;:false, &quot;size&quot;:32, &quot;data&quot;:&quot;H3P3N2i9qc4yt7rK7ldqoeCVJGBybzPY5h1Dd7P7p8v&quot;&#125;&#125;&#123;&quot;txpk&quot;:&#123; &quot;imme&quot;:true, &quot;freq&quot;:861.3, &quot;rfch&quot;:0, &quot;powe&quot;:12, &quot;modu&quot;:&quot;FSK&quot;, &quot;datr&quot;:50000, &quot;fdev&quot;:3000, &quot;size&quot;:32, &quot;data&quot;:&quot;H3P3N2i9qc4yt7rK7ldqoeCVJGBybzPY5h1Dd7P7p8v&quot;&#125;&#125; TX_ACK数据包的根对象必须包含名为“txpk_ack”的对象： 123&#123; &quot;txpk_ack&quot;: &#123;...&#125;&#125; 该对象包含有关相关PULL_RESP数据包的状态信息。 Name Type Function error string Indication about success or type of failure that occured for downlink request. 可能的错误有： Value Definition NONE Packet has been programmed for downlink TOO_LATE Rejected because it was already too late to program this packet for downlink TOO_EARLY Rejected because downlink packet timestamp is too much in advance COLLISION_PACKET Rejected because there was already a packet programmed in requested timeframe COLLISION_BEACON Rejected because there was already a beacon planned in requested timeframe TX_FREQ Rejected because requested frequency is not supported by TX RF chain TX_POWER Rejected because requested power is not supported by gateway GPS_UNLOCKED Rejected because GPS is unlocked, so GPS timestamp cannot be used 示例（为便于阅读而添加了空格，缩进和换行符）： 123&#123;&quot;txpk_ack&quot;:&#123; &quot;error&quot;:&quot;COLLISION_PACKET&quot;&#125;&#125;","tags":[{"name":"lorawan","slug":"lorawan","permalink":"http://wumuwumu.github.io/tags/lorawan/"}]},{"title":"lorawan协议","date":"2018-12-05T13:32:24.000Z","path":"2018/12/05/lorawan协议/","text":"Introduction The protocol between the gateway and the server is purposefully very basic and for demonstration purpose only, or for use on private and reliable networks. There is no authentication of the gateway or the server, and the acknowledges are only used for network quality assessment, not to correct UDP datagrams losses (no retries). System schematic and definitions 1234567891011121314151617 ((( Y ))) | |+ - -|- - - - - - - - - - - - - + xxxxxxxxxxxx +--------+| +--+-----------+ +------+ | xx x x xxx | || | | | | | xx Internet xx | || | Concentrator |&lt;---&gt;| Host |&lt;-------xx or xx--------&gt;| || | | SPI | | | xx Intranet xx | Server || +--------------+ +------+ | xxxx x xxxx | || ^ ^ | xxxxxxxx | || | PPS +-------+ NMEA | | | || +-----| GPS |-------+ | +--------+| | (opt) | || +-------+ || || Gateway |+- - - - - - - - - - - - - - - -+ Concentrator: radio RX/TX board, based on Semtech multichannel modems (SX130x), transceivers (SX135x) and/or low-power stand-alone modems (SX127x). Host: embedded computer on which the packet forwarder is run. Drives the concentrator through a SPI link. GPS: GNSS (GPS, Galileo, GLONASS, etc) receiver with a “1 Pulse Per Second” output and a serial link to the host to send NMEA frames containing time and geographical coordinates data. Optional. Gateway: a device composed of at least one radio concentrator, a host, some network connection to the internet or a private network (Ethernet, 3G, Wifi, microwave link), and optionally a GPS receiver for synchronization. Server: an abstract computer that will process the RF packets received and forwarded by the gateway, and issue RF packets in response that the gateway will have to emit. It is assumed that the gateway can be behind a NAT or a firewall stopping any incoming connection. It is assumed that the server has an static IP address (or an address solvable through a DNS service) and is able to receive incoming connections on a specific port. Upstream protocol Sequence diagram12345678910111213141516+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | -----------------------------------\\ | |-| When 1-N RF packets are received | | | ------------------------------------ | | | | PUSH_DATA (token X, GW MAC, JSON payload) | |-------------------------------------------------------------&gt;| | | | PUSH_ACK (token X) | |&lt;-------------------------------------------------------------| | ------------------------------\\ | | | process packets *after* ack |-| | ------------------------------- | | | PUSH_DATA packetThat packet type is used by the gateway mainly to forward the RF packets received, and associated metadata, to the server. Bytes Function 0 protocol version = 2 1-2 random token 3 PUSH_DATA identifier 0x00 4-11 Gateway unique identifier (MAC address) 12-end JSON object, starting with {, ending with }, see section 4 PUSH_ACK packetThat packet type is used by the server to acknowledge immediately all the PUSH_DATA packets received. Bytes Function 0 protocol version = 2 1-2 same token as the PUSH_DATA packet to acknowledge 3 PUSH_ACK identifier 0x01 Upstream JSON data structure The root object can contain an array named “rxpk”: 123&#123; &quot;rxpk&quot;:[ &#123;...&#125;, ...]&#125; That array contains at least one JSON object, each object contain a RF packet and associated metadata with the following fields: Name Type Function time string UTC time of pkt RX, us precision, ISO 8601 ‘compact’ format tmst number Internal timestamp of “RX finished” event (32b unsigned) freq number RX central frequency in MHz (unsigned float, Hz precision) chan number Concentrator “IF” channel used for RX (unsigned integer) rfch number Concentrator “RF chain” used for RX (unsigned integer) stat number CRC status: 1 = OK, -1 = fail, 0 = no CRC modu string Modulation identifier “LORA” or “FSK” datr string LoRa datarate identifier (eg. SF12BW500) datr number FSK datarate (unsigned, in bits per second) codr string LoRa ECC coding rate identifier rssi number RSSI in dBm (signed integer, 1 dB precision) lsnr number Lora SNR ratio in dB (signed float, 0.1 dB precision) size number RF packet payload size in bytes (unsigned integer) data string Base64 encoded RF packet payload, padded Example (white-spaces, indentation and newlines added for readability): 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123;&quot;rxpk&quot;:[ &#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.528002Z&quot;, &quot;tmst&quot;:3512348611, &quot;chan&quot;:2, &quot;rfch&quot;:0, &quot;freq&quot;:866.349812, &quot;stat&quot;:1, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF7BW125&quot;, &quot;codr&quot;:&quot;4/6&quot;, &quot;rssi&quot;:-35, &quot;lsnr&quot;:5.1, &quot;size&quot;:32, &quot;data&quot;:&quot;-DS4CGaDCdG+48eJNM3Vai-zDpsR71Pn9CPA9uCON84&quot; &#125;,&#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.530974Z&quot;, &quot;tmst&quot;:3512348514, &quot;chan&quot;:9, &quot;rfch&quot;:1, &quot;freq&quot;:869.1, &quot;stat&quot;:1, &quot;modu&quot;:&quot;FSK&quot;, &quot;datr&quot;:50000, &quot;rssi&quot;:-75, &quot;size&quot;:16, &quot;data&quot;:&quot;VEVTVF9QQUNLRVRfMTIzNA==&quot; &#125;,&#123; &quot;time&quot;:&quot;2013-03-31T16:21:17.532038Z&quot;, &quot;tmst&quot;:3316387610, &quot;chan&quot;:0, &quot;rfch&quot;:0, &quot;freq&quot;:863.00981, &quot;stat&quot;:1, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF10BW125&quot;, &quot;codr&quot;:&quot;4/7&quot;, &quot;rssi&quot;:-38, &quot;lsnr&quot;:5.5, &quot;size&quot;:32, &quot;data&quot;:&quot;ysgRl452xNLep9S1NTIg2lomKDxUgn3DJ7DE+b00Ass&quot; &#125;]&#125; The root object can also contain an object named “stat” : 1234&#123; &quot;rxpk&quot;:[ &#123;...&#125;, ...], &quot;stat&quot;:&#123;...&#125;&#125; It is possible for a packet to contain no “rxpk” array but a “stat” object. 123&#123; &quot;stat&quot;:&#123;...&#125;&#125; That object contains the status of the gateway, with the following fields: Name Type Function time string UTC ‘system’ time of the gateway, ISO 8601 ‘expanded’ format lati number GPS latitude of the gateway in degree (float, N is +) long number GPS latitude of the gateway in degree (float, E is +) alti number GPS altitude of the gateway in meter RX (integer) rxnb number Number of radio packets received (unsigned integer) rxok number Number of radio packets received with a valid PHY CRC rxfw number Number of radio packets forwarded (unsigned integer) ackr number Percentage of upstream datagrams that were acknowledged dwnb number Number of downlink datagrams received (unsigned integer) txnb number Number of packets emitted (unsigned integer) Example (white-spaces, indentation and newlines added for readability): 123456789101112&#123;&quot;stat&quot;:&#123; &quot;time&quot;:&quot;2014-01-12 08:59:28 GMT&quot;, &quot;lati&quot;:46.24000, &quot;long&quot;:3.25230, &quot;alti&quot;:145, &quot;rxnb&quot;:2, &quot;rxok&quot;:2, &quot;rxfw&quot;:2, &quot;ackr&quot;:100.0, &quot;dwnb&quot;:2, &quot;txnb&quot;:2&#125;&#125; Downstream protocol Sequence diagram1234567891011121314151617181920212223242526+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | -----------------------------------\\ | |-| Every N seconds (keepalive time) | | | ------------------------------------ | | | | PULL_DATA (token Y, MAC@) | |-------------------------------------------------------------&gt;| | | | PULL_ACK (token Y) | |&lt;-------------------------------------------------------------| | |+---------+ +---------+| Gateway | | Server |+---------+ +---------+ | ------------------------------------------------------\\ | | | Anytime after first PULL_DATA for each packet to TX |-| | ------------------------------------------------------- | | | | PULL_RESP (token Z, JSON payload) | |&lt;-------------------------------------------------------------| | | | TX_ACK (token Z, JSON payload) | |-------------------------------------------------------------&gt;| PULL_DATA packetThat packet type is used by the gateway to poll data from the server. This data exchange is initialized by the gateway because it might be impossible for the server to send packets to the gateway if the gateway is behind a NAT. When the gateway initialize the exchange, the network route towards the server will open and will allow for packets to flow both directions. The gateway must periodically send PULL_DATA packets to be sure the network route stays open for the server to be used at any time. Bytes Function 0 protocol version = 2 1-2 random token 3 PULL_DATA identifier 0x02 4-11 Gateway unique identifier (MAC address) PULL_ACK packetThat packet type is used by the server to confirm that the network route is open and that the server can send PULL_RESP packets at any time. Bytes Function 0 protocol version = 2 1-2 same token as the PULL_DATA packet to acknowledge 3 PULL_ACK identifier 0x04 PULL_RESP packetThat packet type is used by the server to send RF packets and associated metadata that will have to be emitted by the gateway. Bytes Function 0 protocol version = 2 1-2 random token 3 PULL_RESP identifier 0x03 4-end JSON object, starting with {, ending with }, see section 6 TX_ACK packetThat packet type is used by the gateway to send a feedback to the server to inform if a downlink request has been accepted or rejected by the gateway. The datagram may optionnaly contain a JSON string to give more details on acknoledge. If no JSON is present (empty string), this means than no error occured. Bytes Function 0 protocol version = 2 1-2 same token as the PULL_RESP packet to acknowledge 3 TX_ACK identifier 0x05 4-11 Gateway unique identifier (MAC address) 12-end [optional] JSON object, starting with {, ending with }, see section 6 Downstream JSON data structure The root object of PULL_RESP packet must contain an object named “txpk”: 123&#123; &quot;txpk&quot;: &#123;...&#125;&#125; That object contain a RF packet to be emitted and associated metadata with the following fields: Name Type Function imme bool Send packet immediately (will ignore tmst &amp; time) tmst number Send packet on a certain timestamp value (will ignore time) time string Send packet at a certain time (GPS synchronization required) freq number TX central frequency in MHz (unsigned float, Hz precision) rfch number Concentrator “RF chain” used for TX (unsigned integer) powe number TX output power in dBm (unsigned integer, dBm precision) modu string Modulation identifier “LORA” or “FSK” datr string LoRa datarate identifier (eg. SF12BW500) datr number FSK datarate (unsigned, in bits per second) codr string LoRa ECC coding rate identifier fdev number FSK frequency deviation (unsigned integer, in Hz) ipol bool Lora modulation polarization inversion prea number RF preamble size (unsigned integer) size number RF packet payload size in bytes (unsigned integer) data string Base64 encoded RF packet payload, padding optional ncrc bool If true, disable the CRC of the physical layer (optional) Most fields are optional. If a field is omitted, default parameters will be used. Examples (white-spaces, indentation and newlines added for readability): 1234567891011121314151617181920212223&#123;&quot;txpk&quot;:&#123; &quot;imme&quot;:true, &quot;freq&quot;:864.123456, &quot;rfch&quot;:0, &quot;powe&quot;:14, &quot;modu&quot;:&quot;LORA&quot;, &quot;datr&quot;:&quot;SF11BW125&quot;, &quot;codr&quot;:&quot;4/6&quot;, &quot;ipol&quot;:false, &quot;size&quot;:32, &quot;data&quot;:&quot;H3P3N2i9qc4yt7rK7ldqoeCVJGBybzPY5h1Dd7P7p8v&quot;&#125;&#125;&#123;&quot;txpk&quot;:&#123; &quot;imme&quot;:true, &quot;freq&quot;:861.3, &quot;rfch&quot;:0, &quot;powe&quot;:12, &quot;modu&quot;:&quot;FSK&quot;, &quot;datr&quot;:50000, &quot;fdev&quot;:3000, &quot;size&quot;:32, &quot;data&quot;:&quot;H3P3N2i9qc4yt7rK7ldqoeCVJGBybzPY5h1Dd7P7p8v&quot;&#125;&#125; The root object of TX_ACK packet must contain an object named “txpk_ack”: 123&#123; &quot;txpk_ack&quot;: &#123;...&#125;&#125; That object contain status information concerning the associated PULL_RESP packet. Name Type Function error string Indication about success or type of failure that occured for downlink request. The possible values of “error” field are: Value Definition NONE Packet has been programmed for downlink TOO_LATE Rejected because it was already too late to program this packet for downlink TOO_EARLY Rejected because downlink packet timestamp is too much in advance COLLISION_PACKET Rejected because there was already a packet programmed in requested timeframe COLLISION_BEACON Rejected because there was already a beacon planned in requested timeframe TX_FREQ Rejected because requested frequency is not supported by TX RF chain TX_POWER Rejected because requested power is not supported by gateway GPS_UNLOCKED Rejected because GPS is unlocked, so GPS timestamp cannot be used Examples (white-spaces, indentation and newlines added for readability): 123&#123;&quot;txpk_ack&quot;:&#123; &quot;error&quot;:&quot;COLLISION_PACKET&quot;&#125;&#125; Revisions v1.3 Added downlink feedback from gateway to server (PULL_RESP -&gt; TX_ACK) v1.2 Added value of FSK bitrate for upstream. Added parameters for FSK bitrate and frequency deviation for downstream. v1.1 Added syntax for status report JSON object on upstream. v1.0 Initial version.","tags":[{"name":"lorawan","slug":"lorawan","permalink":"http://wumuwumu.github.io/tags/lorawan/"}]},{"title":"let-us-encrypt证书","date":"2018-12-05T12:59:59.000Z","path":"2018/12/05/let-us-encrypt证书/","text":"基本知识为了实现通配符证书，Let’s Encrypt 对 ACME 协议的实现进行了升级，只有 v2 协议才能支持通配符证书。 客户在申请 Let’s Encrypt 证书的时候，需要校验域名的所有权，证明操作者有权利为该域名申请证书，目前支持三种验证方式： dns-01：给域名添加一个 DNS TXT 记录。 http-01：在域名对应的 Web 服务器下放置一个 HTTP well-known URL 资源文件。 tls-sni-01：在域名对应的 Web 服务器下放置一个 HTTPS well-known URL 资源文件。 而申请通配符证书，只能使用 dns-01 的方式 ACME v2 和 v1 协议是互相不兼容的，为了使用 v2 版本，客户端需要创建另外一个账户（代表客户端操作者），以 Certbot 客户端为例，大家可以查看： Enumerable Orders 和限制 安装12wget https://dl.eff.org/certbot-autochmod a+x ./certbot-auto 申请1./certbot-auto certonly -d *.newyingyong.cn --manual --preferred-challenges dns --server https://acme-v02.api.letsencrypt.org/directory certonly，表示安装模式，Certbot 有安装模式和验证模式两种类型的插件。 –manual 表示手动安装插件，Certbot 有很多插件，不同的插件都可以申请证书，用户可以根据需要自行选择 -d 为那些主机申请证书，如果是通配符，输入 *.newyingyong.cn（可以替换为你自己的域名） -preferred-challenges dns，使用 DNS 方式校验域名所有权 –server，Let’s Encrypt ACME v2 版本使用的服务器不同于 v1 版本，需要显示指定。 添加记录根据命令行提示，填写相关的内容，注意在添加记录的时候，要等到记录生效才确定。 123456789-------------------------------------------------------------------------------Please deploy a DNS TXT record under the name_acme-challenge.newyingyong.cn with the following value:2_8KBE_jXH8nYZ2unEViIbW52LhIqxkg6i9mcwsRvhQBefore continuing, verify the record is deployed.-------------------------------------------------------------------------------Press Enter to ContinueWaiting for verification...Cleaning up challenges 12## 检测记录生效$ dig -t txt _acme-challenge.newyingyong.cn @8.8.8.8 更新查看当前服务器所配置的证书 1certbot-auto certificates 使用申请的普通证书，使用certbot-auto renew 使用通配符证书。 添加DNS记录 1git clone https://github.com/ywdblog/certbot-letencrypt-wildcardcertificates-alydns-au.git 1./certbot-auto renew --cert-name simplehttps.com --manual-auth-hook /脚本目录/au.sh 自动更新 11 1 */1 * * root certbot-auto renew --manual --preferred-challenges dns --manual-auth-hook /脚本目录/sslupdate.sh 参考 https://www.jianshu.com/p/c5c9d071e395 https://www.jianshu.com/p/074e147b68b0 certbot工具https://segmentfault.com/a/1190000015354547","tags":[{"name":"web","slug":"web","permalink":"http://wumuwumu.github.io/tags/web/"}]},{"title":"css动画","date":"2018-12-04T13:36:49.000Z","path":"2018/12/04/css动画/","text":"transition transition-duration transition-property transition-delay transition-timing-function animation @keyframes animation animation-name animation-duration animation-timing-function animation-delay animation-iteration-count animation-fill-mode animation-direction animation-play-state(这个要写在最下面，不然不会生效) transform none translate(x,y)/translate3d(x,y,z) translateX(x)/translateY(y)/translateZ(z) materix/materix3d scale/scale3d scaleX/scaleY/scaleZ rotate/rotate3d rotateX/rotateY/rotateZ skew/skewX/skewY perspective","tags":[{"name":"css","slug":"css","permalink":"http://wumuwumu.github.io/tags/css/"}]},{"title":"清除inline-block之间的间隙","date":"2018-12-03T11:54:08.000Z","path":"2018/12/03/清除inline-block之间的间隙/","text":"原因两个inline-block之间存在间隙，这是因为html元素换行导致的（换行和元素之间的空格、tabs、多个空格，结果一样，最后都是一个空格） 移除空格如果我们使用html minimize工具，会清除html之间的空格。如果没有使用就需要我们手动去除。该方法简单但是不推荐使用，阅读不方便。 123456789101112131415&lt;!-- 方法一 --&gt;&lt;div&gt;one&lt;/div&gt;&lt;div&gt;two&lt;/div&gt;&lt;div&gt;three&lt;/div&gt;&lt;!-- 方法二 --&gt;&lt;div&gt;one&lt;/div&gt;&lt;div&gt;two&lt;/div&gt;&lt;div&gt;three&lt;/div&gt;&lt;!-- 方法三 --&gt;&lt;div&gt;one&lt;/div&gt;&lt;!----&gt;&lt;div&gt;two&lt;/div&gt;&lt;!----&gt;&lt;div&gt;three&lt;/div&gt; 负值margin不推荐使用，每个浏览器之间的间隙不同。 1234nav a &#123; display: inline-block; margin-right: -4px;&#125; 父元素font-size设置为0123456.space &#123; font-size: 0;&#125;.space a &#123; font-size: 12px;&#125; 这种方法是推荐使用的，但是在ie和Chrome浏览器(新的浏览器没有问题)上可能出现问题，因为在chrome上有最小字体限制。改进方法如下。 1234.space &#123; font-size: 0; -webkit-text-size-adjust:none;&#125; 使用letter-spacingletter-spacing用于修改字符间的间隙。 123456.space &#123; letter-spacing: -3px;&#125;.space a &#123; letter-spacing: 0;&#125; 使用word-spacingword-spacing修改单词之间的间隙 123456.space &#123; word-spacing: -6px;&#125;.space a &#123; word-spacing: 0;&#125; 使用浮动123a&#123; float:left;&#125; 参考 https://www.zhangxinxu.com/wordpress/2012/04/inline-block-space-remove-%E5%8E%BB%E9%99%A4%E9%97%B4%E8%B7%9D/ 代码 https://codepen.io/wumuwumu/pen/WYmKYX","tags":[{"name":"js","slug":"js","permalink":"http://wumuwumu.github.io/tags/js/"}]}]